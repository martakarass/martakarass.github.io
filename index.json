
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"As a statistician on Takeda‚Äôs Quantitative Sciences team in Boston, MA, I focus on statistical modeling and analysis of data from digital devices such as wearable accelerometers, ambulatory blood pressure monitors, respiratory monitors, and polysomnography. My recent projects involve digital assessments in hypersomnolence disorders and ALS.\nBefore joining Takeda, I was a postdoctoral researcher at the Onnela Lab at Harvard T.H. Chan School of Public Health. I received a PhD in Biostatistics from Johns Hopkins Bloomberg School of Public Health in 2021, and hold Bachelor‚Äôs and Master‚Äôs degrees in Mathematics from Wroclaw University of Science and Technology, Poland.\nIn my leisure moments, I find joy in connecting with people, running, and hiking. The peak of my running tenure was probably finishing a full marathon in 2017. Currently, I am working on summiting New Hampshire‚Äôs 48 peaks over 4,000 feet. Some summits photos can be found in my üåÑ gallery.\nRecently Our paper ‚ÄúWearable device and smartphone data can track ALS disease progression and may serve as novel clinical trial outcome measures‚Äù is published. This work investigates whether mobile applications and wearable devices can be used to quantify ALS disease progression through active (surveys) and passive (sensors) data collection. Our preprint ‚ÄúPerformance analyses of step-counting algorithms using wrist accelerometry‚Äù is available online. This work evaluates performance of several modern wrist-accelerometry-based algorithms for step count estimation using a common dataset with various continuous walking trials. Our paper ‚ÄúEstimating knee movement patterns of recreational runners across training sessions using multilevel functional regression models‚Äù is published. This work can serve as a reference for practitioners modeling repeated functional measures at different resolution levels in the context of biomechanics and sports science applications. SEE ALL PAST UPDATES \u0026gt; ","date":1554595200,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1554595200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"","publishdate":"0001-01-01T00:00:00Z","relpermalink":"","section":"authors","summary":"As a statistician on Takeda‚Äôs Quantitative Sciences team in Boston, MA, I focus on statistical modeling and analysis of data from digital devices such as wearable accelerometers, ambulatory blood pressure monitors, respiratory monitors, and polysomnography.","tags":null,"title":"Marta Karas","type":"authors"},{"authors":[],"categories":null,"content":" Click on the Slides button above to view the built-in slides feature. Slides can be added in a few ways:\nCreate slides using Hugo Blox Builder‚Äôs Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes. Further event details, including page elements such as image galleries, can be added to the body of this page.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a8edef490afe42206247b6ac05657af0","permalink":"https://example.com/talk/example-talk/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/example-talk/","section":"event","summary":"An example talk using Hugo Blox Builder's Markdown slides feature.","tags":[],"title":"Example Talk","type":"event"},{"authors":null,"categories":null,"content":"Welcome to my photo gallery! These are some of my favorite pictures taken in recent years. Click on any picture to enter the gallery view and see it in full size (this allows to see description of a picture‚Äôs location). Thank you for visiting my website.\n","date":1701561600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1701561600,"objectID":"ca12becfadd86ee6150e97a010fd114e","permalink":"https://example.com/resources/gallery-all/","publishdate":"2023-12-03T00:00:00Z","relpermalink":"/resources/gallery-all/","section":"resources","summary":"Welcome to my photo gallery! These are some of my favorite pictures taken in recent years. Click on any picture to enter the gallery view and see it in full size (this allows to see description of a picture‚Äôs location).","tags":null,"title":"Nature and the city","type":"resources"},{"authors":null,"categories":null,"content":"from IPython.core.display import Image Image(\u0026#39;https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png\u0026#39;) print(\u0026#34;Welcome to Academic!\u0026#34;) Welcome to Academic! Organize your notebooks Place the notebooks that you would like to publish in a notebooks folder at the root of your website.\nImport the notebooks into your site pipx install academic academic import \u0026#39;notebooks/**.ipynb\u0026#39; content/post/ --verbose The notebooks will be published to the folder you specify above. In this case, they will be published to your content/post/ folder.\n","date":1699056000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1699056000,"objectID":"c64af73532bdb3323cfd36f4f3ae73fe","permalink":"https://example.com/post/_template_blog-with-jupyter/","publishdate":"2023-11-04T00:00:00Z","relpermalink":"/post/_template_blog-with-jupyter/","section":"post","summary":"Easily blog from Jupyter notebooks!","tags":null,"title":"Blog with Jupyter Notebooks!","type":"post"},{"authors":null,"categories":[],"content":"In this post, we use the Repeated Measures Correlation (rmcorr) approach to quantify association between two continuous measures recorded multiple times per study subject.\nI recently learned about the rmcorr approach and explored it with a view of applying in one of the current projects. I think rmcorr is a very interesting method with neat interpretation. Here, I highlight key notes from how the rmcorr works, and I point to a few cases where I expect that rmcorr may be not an ideal choice to quantify the association.\nTable of Contents Repeated Measures Correlation Method Key notes Scenarios Scenario 1a Scenario 1b Scenario 2a Scenario 2b Scenario 3a Scenario 3b Scenario 4a Scenario 4b Observations Conclusion Repeated Measures Correlation The Bakdash et al. (2017) paper Repeated Measures Correlation discusses within-participants correlation to analyze the common intra-individual association for paired repeated measures. The paper provides background information, methods description, R package reference, and discussion on tradeoffs with rmcorr as compared to multilevel modeling.\nMethod In short, rmcorr:\nIs based on ANCOVA analysis: it considers a subject as a factor-level variable to remove between-person variation. ANCOVA model is set to estimate subjects‚Äô common regression slope $\\beta$. The rmcorr correlation coefficient is calculated using sums of squares for the Measure and for the error from the above ANCOVA model and its sign is determined by the sign (positive vs. negative) of estimated $\\beta$. It has a nice, intuitive interpretation. Authors argue is ‚Äúcomparable to linear multilevel model (LMM) with random intercept only and a fixed effect‚Äù. Key notes Key notes on rmcorr:\nrmcorr does not allow for slope variation by subject, rmcorr does not utilize any of LMM‚Äôs partial-pooling idea (i.e., where for subjects with a relatively smaller number of observations, their estimates are ‚Äúshrunk‚Äù closer to the overall population average); and hence, as the authors noted, rmcorr may be not an ideal choice to quantify the association:\nwhen the effect slope varies between subjects. I also thought having random slopes is even more problematic:\nwhen the number of observations varies between subjects (e.g., varies between 2-10), when there are influential observation(s) (e.g., subject(s) with wide $x$ values range). Scenarios To illustrate the above notes, 4x2 simple data scenarios are considered.\nIn each scenario, data on N = 20 subjects with 2 continuous variables ($x$, $y$), recorded at multiple occasions for each subject, are simulated. The scenarios differ in true data-generating model that is assumed: Scenario 1: random intercept only; Scenario 2: random intercept and random slope (uncorrelated); Scenario 3: random intercept and random slope (uncorrelated), added one subject with influential observations; Scenario 4: random intercept and random slope (uncorrelated), unbalanced number of observations per subject. For each scenario, two versions are considered, assuming: (a): small variance of residual error; (b) large variance of residual error. To show the results:\nFor each scenario, 1 data set is used and results from applying 2 approaches are shown: left plot: rmcorr with an estimate of ‚Äúrepeated measures correlation‚Äù, right plot: LMM fit with an estimate of fixed effect slope. For all scenarios, left plot points same as right plot points (the same data used for 2 approaches). For all scenarios, population-level effect $x$ on $y$ was assumed to be equal $2.0$. For all scenarios, LMM assumes random intercept and random slope for $x$, and a fixed effect for $x$. (Click to see R code shared for all simulations.) library(tidyverse) library(lme4) library(lmerTest) library(rmcorr) library(MASS) # define simulation parameters # number of subjects M \u0026lt;- 20 # number of repeated measures per subjects (for all except last two scenarios) k \u0026lt;- 5 #\u0026#39; Function to apply two approaches to `dat` data set: #\u0026#39; (1) rmcorr with an estimate of \u0026#34;repeated measures correlation\u0026#34;, #\u0026#39; (2) LMM fit with an estimate of fixed effect slope est_and_plot \u0026lt;- function(dat){ # approach 1 rmcorr_out \u0026lt;- rmcorr(subj_id, x, y, dat) plt_label \u0026lt;- paste0(\u0026#34;r = \u0026#34;, round(rmcorr_out$r, 3), \u0026#34; (95% CI: [\u0026#34;, paste0(round(rmcorr_out$CI, 3), collapse = \u0026#34;, \u0026#34;), \u0026#34;])\u0026#34;) par(mar = rep(2, 4)) plot(rmcorr_out, main = plt_label) # approach 2 lmm_out \u0026lt;- lmer(y ~ x + (1 + x | subj_id), data = dat) conf_est \u0026lt;- summary(lmm_out)$coef[2, 1] conf_out \u0026lt;- as.vector(confint(lmm_out, parm = \u0026#34;x\u0026#34;)) plt_label \u0026lt;- paste0(\u0026#34;beta = \u0026#34;, round(conf_est, 3), \u0026#34; (95% CI: [\u0026#34;, paste0(round(conf_out, 3), collapse = \u0026#34;, \u0026#34;), \u0026#34;])\u0026#34;) dat$mu \u0026lt;- getME(lmm_out, \u0026#34;mu\u0026#34;)[1 : nrow(dat)] plt \u0026lt;- ggplot(dat, aes(x = x, y = y, color = subj_id, group = subj_id)) + # geom_line(color = \u0026#34;black\u0026#34;) + geom_line(aes(x = x, y = mu), size = 0.3) + geom_point() + theme_classic(base_size = 12) + theme( legend.position = \u0026#34;none\u0026#34;, ) + labs(x = \u0026#34;x\u0026#34;, y = \u0026#34;y\u0026#34;, title = plt_label) plot(plt) } Scenario 1a Data-generating model:\nrandom ‚Ä¶","date":1651007968,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1651007968,"objectID":"93635c456dc53ca0ce98ed84dcc85e4c","permalink":"https://example.com/post/2022-04-27-rmcorr_vs_lmm/","publishdate":"2022-04-26T17:19:28-04:00","relpermalink":"/post/2022-04-27-rmcorr_vs_lmm/","section":"post","summary":"In this post, we use the Repeated Measures Correlation (rmcorr) approach to quantify association between two continuous measures recorded multiple times per study subject.\nI recently learned about the rmcorr approach and explored it with a view of applying in one of the current projects.","tags":[],"title":"Repeated Measures Correlation (rmcorr): when may be not an ideal choice to quantify the association","type":"post"},{"authors":null,"categories":[],"content":"In this post, we:\nuse a GPX file with with geographic information, exported from Strava from one running activity, parse and plot the data. Table of Contents GPX data from Strava Parsing GPX Computing distance, time elapsed and speed Plot elevation, speed Plot run path Acknowledgements GPX data from Strava GPX stands for ‚ÄúGPS Exchange Format‚Äù. It is an XML schema commonly used for storing GPS data.\nStrava (Strava, Inc; San Francisco, CA) is a popular activity tracker app I have been using for a few weeks. Strava allows to export GPS data collected during a recorded activity in a GPX format. To export the data, go to Strava activity page \u0026gt; ‚Äúthree dots‚Äù button \u0026gt; Export GPX.\nI downloaded GPX data from a run I did on Jan 1, 2022. The run distance is 10.88 km and spans 1:03:49 time. The data is available on my GitHub and can be downloaded using the code below.\n(Click to see the code to download the data.) url \u0026lt;- paste0( \u0026#34;https://raw.githubusercontent.com/martakarass/gps-stats/main/data/\u0026#34;, \u0026#34;/Morning_Run_2022-01-01.gpx\u0026#34;) fpath \u0026lt;- paste0( \u0026#34;/Users/martakaras/Downloads\u0026#34;, \u0026#34;/Morning_Run_2022-01-01.gpx\u0026#34;) # download result \u0026lt;- curl::curl_download(url, destfile = fpath, quiet = FALSE) Parsing GPX First, we parse the GPX file and put the extracted data trajectories into a data frame:\ntimestamp, latitude, longitude, elevation. (Click to see the code.) # rm(list = ls()) library(tidyverse) library(here) library(XML) library(lubridate) library(ggmap) library(geosphere) options(digits.secs = 3) options(scipen = 999) # parse GPX file path_tmp \u0026lt;- paste0(\u0026#34;/Users/martakaras/Downloads/Morning_Run_2022-01-01.gpx\u0026#34;) parsed \u0026lt;- htmlTreeParse(file = path_tmp, useInternalNodes = TRUE) # get values via via the respective xpath coords \u0026lt;- xpathSApply(parsed, path = \u0026#34;//trkpt\u0026#34;, xmlAttrs) elev \u0026lt;- xpathSApply(parsed, path = \u0026#34;//trkpt/ele\u0026#34;, xmlValue) ts_chr \u0026lt;- xpathSApply(parsed, path = \u0026#34;//trkpt/time\u0026#34;, xmlValue) # combine into df dat_df \u0026lt;- data.frame( ts_POSIXct = ymd_hms(ts_chr, tz = \u0026#34;EST\u0026#34;), lat = as.numeric(coords[\u0026#34;lat\u0026#34;,]), lon = as.numeric(coords[\u0026#34;lon\u0026#34;,]), elev = as.numeric(elev) ) head(dat_df) ts_POSIXct lat lon elev 1 2022-01-01 09:42:01 42.32791 -71.10868 23.0 2 2022-01-01 09:42:06 42.32791 -71.10868 23.0 3 2022-01-01 09:42:08 42.32795 -71.10866 23.2 4 2022-01-01 09:42:10 42.32817 -71.10872 24.7 5 2022-01-01 09:42:11 42.32816 -71.10875 24.7 6 2022-01-01 09:42:12 42.32814 -71.10875 23.8 Computing distance, time elapsed and speed Next, we compute:\ndistance (in meters) between subsequent GPS recordings time elapsed (in seconds) between subsequent GPS recordings, speed (metres per seconds, kilometres per hour) ‚Äì temporal, based on subsequent GPS recordings. (Click to see the code.) # compute distance (in meters) between subsequent GPS points dat_df \u0026lt;- dat_df %\u0026gt;% mutate(lat_lead = lead(lat)) %\u0026gt;% mutate(lon_lead = lead(lon)) %\u0026gt;% rowwise() %\u0026gt;% mutate(dist_to_lead_m = distm(c(lon, lat), c(lon_lead, lat_lead), fun = distHaversine)[1,1]) %\u0026gt;% ungroup() # compute time elapsed (in seconds) between subsequent GPS points dat_df \u0026lt;- dat_df %\u0026gt;% mutate(ts_POSIXct_lead = lead(ts_POSIXct)) %\u0026gt;% mutate(ts_diff_s = as.numeric(difftime(ts_POSIXct_lead, ts_POSIXct, units = \u0026#34;secs\u0026#34;))) # compute metres per seconds, kilometres per hour dat_df \u0026lt;- dat_df %\u0026gt;% mutate(speed_m_per_sec = dist_to_lead_m / ts_diff_s) %\u0026gt;% mutate(speed_km_per_h = speed_m_per_sec * 3.6) # remove some columns we won\u0026#39;t use anymore dat_df \u0026lt;- dat_df %\u0026gt;% select(-c(lat_lead, lon_lead, ts_POSIXct_lead, ts_diff_s)) head(dat_df) %\u0026gt;% as.data.frame() ts_POSIXct lat lon elev dist_to_lead_m ts_diff_s speed_m_per_sec speed_km_per_h 1 2022-01-01 09:42:01 42.32791 -71.10868 23.0 0.000000 5 0.000000 0.000000 2 2022-01-01 09:42:06 42.32791 -71.10868 23.0 4.406590 2 2.203295 7.931862 3 2022-01-01 09:42:08 42.32795 -71.10866 23.2 25.403927 2 12.701963 45.727068 4 2022-01-01 09:42:10 42.32817 -71.10872 24.7 2.510645 1 2.510645 9.038324 5 2022-01-01 09:42:11 42.32816 -71.10875 24.7 2.264098 1 2.264098 8.150751 6 2022-01-01 09:42:12 42.32814 -71.10875 23.8 1.899407 1 1.899407 6.837864 Plot elevation, speed Plot elevation\n(Click to see the code.) plt_elev \u0026lt;- ggplot(dat_df, aes(x = ts_POSIXct, y = elev)) + geom_line() + labs(x = \u0026#34;Time\u0026#34;, y = \u0026#34;Elevation [m]\u0026#34;) + theme_grey(base_size = 14) plt_elev Plot speed\n(Click to see the code.) plt_speed_km_per_h \u0026lt;- ggplot(dat_df, aes(x = ts_POSIXct, y = speed_km_per_h)) + geom_line() + labs(x = \u0026#34;Time\u0026#34;, y = \u0026#34;Speed [km/h]\u0026#34;) + theme_grey(base_size = 14) plt_speed_km_per_h The above plot is very wiggly due to small time increment over which the speed statistic was computed. It could be made smoother by first aggregating distance covered and time elapsed over a fixed time interval longer than GPS recordings interval (e.g. 10 seconds), or by using data smoothing (e.g. LOWESS).\nThe dips in the plot are, to my judgement, correct representations of the times I briefly stopped during the run for various reasons.\nPlot run path A simple, graphics-based version of the trajectory plot:\n(Click to see the ‚Ä¶","date":1641417568,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641417568,"objectID":"b05a3b4f7f5ea01f9c962401798241d0","permalink":"https://example.com/post/2022-01-05-gps_strava_read_and_viz/","publishdate":"2022-01-05T17:19:28-04:00","relpermalink":"/post/2022-01-05-gps_strava_read_and_viz/","section":"post","summary":"In this post, we:\nuse a GPX file with with geographic information, exported from Strava from one running activity, parse and plot the data. ","tags":[],"title":"GPS data in R: parse and plot GPX data exported from Strava","type":"post"},{"authors":[],"categories":[],"content":"Power and sample size calculation are major components of statistical analyses. The upstrap resampling method was proposed as a general solution to this problem.\nWe evaluate power estimation properties of the upstrap and provide a series of ‚Äúread, adapt and use‚Äù R code examples for power estimation in simple and complex settings (bioRxiv preprint).\nTable of Contents Scientific problem Challenges Proposed solution Code example: testing for significance of LM coefficient Contributions Scientific problem We consider the following problem: given an observed data sample $\\mathbf{x}$ of sample size $N$, given specific null and alternative hypothesis, and a test statistic, assuming significance level $\\alpha$, estimate sample size $M$ required to achieve power $1 -\\beta$ (i.e., to achieve probability $1 -\\beta$ of rejecting the null hypothesis when the null is true).\nHere, we consider the tasks to estimate the power to detect:\nan effect size observed in the data; an effect size chosen by a researcher. We aim to ddress complex settings, including testing significance of model coefficients in: LM, GLM, LMM, GLMM, GEE, and others.\nChallenges For multilevel data settings, the existing approaches tend to fall into two categories.\nFirst are theoretical results for estimating power in specific multilevel data setups; these often use assumptions about the intra-class correlation coefficient, and/or assume a particular study design (e.g., that the data are balanced). Another class is based on simulations. Such may require specifying a population model for the data, simulating data from the assumed model, and estimating the power via Monte Carlo simulations.\nThe former arguably lacks flexibility; in addition, finding and then determining applicability of a theoretical result may be difficult. The latter is flexible but involves potentially complex programming task.\nProposed solution Upstrap The upstrap resampling method (Crainiceanu, C.M., Crainiceanu, A. (2020)) was proposed as a general solution to this problem.\nUpstrap starts with a sample (observed data) and resamples with replacement either fewer or more samples than in the original data set. The below example shows difference between bootstrap and upstrap resample.\n# simulate data x \u0026lt;- rnorm(n = 10) # bootstrap resample x_b \u0026lt;- sample(x, size = 10, replace = TRUE) # upstrap resample (case: more samples than in original x) x_u \u0026lt;- sample(x, size = 20, replace = TRUE) Upstrap for estimating power Given observed data sample $\\mathbf{x}$, to estimate power for a target sample size, we propose:\nGenerate $B$ resamples of target sample size by sampling with replacement from $\\mathbf{x}$.\nPerform hypothesis test on each resample.\nEstimate power as the proportion of $B$ resamples where the null hypothesis was rejected.\nThe above procedure estimates power corresponding to the effect size observed in the sample $\\mathbf{x}$. For example, for one-sample t-test, the above procedure estimates:\npower.t.test(delta = mean(x), sd = sd(x), ...)$power Upstrap for estimating power: specific effect size In practice, one is often is interested in estimating power for a specific effect size. For example, for one-sample t-test, a specific effect size could be set to 0.3:\npower.t.test(delta = 0.3, sd = sd(x), ...)$power To address such cases, we propose to update response variable values in the observed sample $\\mathbf{x}$ to ensure the effect size in this updated data is our target effect size. Details are provided in the preprint.\nCode example: testing for significance of LM coefficient Below, we demonstrate the upstrap power estimation method for testing for significance of LM coefficient. In the preprint, we provide more R code examples, including testing for significance of coefficient in LM, GLM, LMM, GLMM.\nWe define simulation parameters and simulate a sample of size $N = 50$.\n(Click to see setup definition and R code.) Setup Consider a random sample with $N = 50$ independent observations (e.g., 50 subjects, 1 observation per subject). Assume a continuous response variable $Y$ and two covariates: dichotomous $X_1$, continuous $X_2$. We are interested in estimating power of test for significance of the coefficient $\\beta_{1}$ in linear model $Y_{i}=\\beta_{0}+\\beta_{1} X_{1 i}+\\beta_{2} X_{2 i}+\\varepsilon_{i}$, where $i=1, \\ldots, N$ and $\\varepsilon_{i} \\sim_{\\text {iid }} N\\left(0, \\sigma^{2}\\right)$.\n# simulation parameters N \u0026lt;- 50 coef_x0 \u0026lt;- 0 coef_x1 \u0026lt;- 0.2 coef_x2 \u0026lt;- 0.1 sigma2 \u0026lt;- 1 # simulate sample set.seed(1) subjid_i \u0026lt;- 1 : N # subject ID unique in data set x1_i \u0026lt;- rbinom(n = N, size = 1, prob = 0.5) x2_i \u0026lt;- rbinom(n = N, size = 1, prob = 0.5) eps_i \u0026lt;- rnorm(N, sd = sqrt(sigma2)) y_i \u0026lt;- coef_x0 + (coef_x1 * x1_i) + (coef_x2 * x2_i) + eps_i dat \u0026lt;- data.frame(y = y_i, x1 = x1_i, x2 = x2_i, subjid = subjid_i) str(dat) # \u0026#39;data.frame\u0026#39;:\t50 obs. of 4 variables: # $ y : num 0.398 -0.512 0.541 -0.929 1.433 ... # $ x1 : int 0 0 1 1 0 1 1 1 1 0 ... # $ x2 : int 0 1 0 0 0 0 0 1 1 0 ... # $ ‚Ä¶","date":1630511693,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630511693,"objectID":"d3827687fac2a57875e8145171889c68","permalink":"https://example.com/post/project_upstrap/","publishdate":"2021-09-01T11:54:53-04:00","relpermalink":"/post/project_upstrap/","section":"post","summary":"We evaluate the power and sample size estimation properties of the upstrap resampling method.","tags":null,"title":"Upstrap for estimating power and sample size in complex models","type":"post"},{"authors":null,"categories":[],"content":"In this post, we:\nuse dataset ‚ÄúLabeled raw accelerometry data captured during walking, stair climbing and driving‚Äù that is freely available on PhysioNet;\nderive four minute-level summary measures of physical activity ‚Äì AC, MIMS, ENMO, MAD, AI ‚Äì from raw accelerometry data using SummarizedActigraphy R package;\nsummarize minute-level summary measures across walking and driving activities.\nTable of Contents Dataset ‚ÄúLabeled raw accelerometry data‚Äù Computing minute-level summary measures of raw accelerometry data Summary measures SummarizedActigraphy R package Preparing the data input Computing the measures Minute-level physical activity measures during walking and driving activities Merging physical activity measures with physical activity labels Visualizing physical activity measures across activity types Acknowledgements Citation info Dataset ‚ÄúLabeled raw accelerometry data‚Äù The dataset contains raw accelerometry data collected during outdoor walking, stair climbing, and driving for n=32 healthy adults. It is freely is available to download on PhysioNet at: https://doi.org/10.13026/51h0-a262\nThe study was led by Dr.¬†Jaroslaw Harezlak, assisted by Drs. William Fadel and Jacek Urbanek. Accelerometry data were collected simultaneously at four body locations: left wrist, left hip, left ankle, and right ankle, at a sampling frequency of 100 Hz. The 3-axial ActiGraph GT3X+ devices were used to collect the data. The data include labels of activity type performed for each time point of data collection (1=walking; 2=descending stairs; 3=ascending stairs; 4=driving; 77=clapping; 99=non-study activity). We downloaded the data zip from PhysioNet and unpacked. This can be done manually from the website, or using download the data programmatically.\n(Click to see the code to download the data.) url \u0026lt;- paste0( \u0026#34;https://physionet.org/static/published-projects\u0026#34;, \u0026#34;/accelerometry-walk-climb-drive\u0026#34;, \u0026#34;/labeled-raw-accelerometry-data-captured-during-walking-stair-climbing-and-driving-1.0.0.zip\u0026#34;) destfile \u0026lt;- paste0( \u0026#34;/Users/martakaras/Downloads\u0026#34;, \u0026#34;/labeled-raw-accelerometry-data-captured-during-walking-stair-climbing-and-driving-1.0.0.zip\u0026#34;) # download zip result \u0026lt;- curl::curl_download(url, destfile = destfile, quiet = FALSE) # unzip zip unzip(zipfile = destfile, exdir = dirname(destfile)) First, let‚Äôs look at raw accelerometry data of a single subject.\n(Click to see the code.) library(tidyverse) library(lubridate) library(lme4) library(knitr) # remotes::install_github(\u0026#34;muschellij2/SummarizedActigraphy\u0026#34;) library(SummarizedActigraphy) library(MIMSunit) library(activityCounts) options(digits.secs = 3) options(scipen=999) # define path to raw data files directory raw_files_dir \u0026lt;- paste0( \u0026#34;/Users/martakaras/Downloads\u0026#34;, \u0026#34;/labeled-raw-accelerometry-data-captured-during-walking-stair-climbing-and-driving-1.0.0\u0026#34;, \u0026#34;/raw_accelerometry_data\u0026#34;) # single participant\u0026#39;s raw accelerometry data dat_fpaths \u0026lt;- list.files(raw_files_dir, full.names = TRUE) dat_i \u0026lt;- read_csv(dat_fpaths[1]) dat_i # A tibble: 303,300 x 14 activity time_s lw_x lw_y lw_z lh_x lh_y lh_z la_x la_y la_z \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; 1 99 0.01 0.039 1.02 -0.02 -0.18 1.23 0.023 0.156 0.855 -0.582 2 99 0.02 -0.629 -0.461 0.973 -0.246 0.137 0.969 -0.707 0.559 0.449 3 99 0.03 -0.926 -1.26 0.691 0.238 -0.328 1.22 -1.44 1.37 0.367 4 99 0.04 -0.871 -1.50 -0.246 0.711 -0.484 0.414 -1.66 1.64 -0.543 5 99 0.05 -0.727 -1.62 -0.559 1.03 -0.297 0.145 -1.76 1.68 -0.918 6 99 0.06 -0.543 -1.66 -0.629 1.12 -0.246 0.137 -1.80 1.65 -0.988 7 99 0.07 -0.348 -1.64 -0.609 1.24 -0.426 0.047 -1.76 1.57 -0.992 8 99 0.08 -0.16 -1.60 -0.566 1.18 -0.539 -0.008 -1.63 1.53 -1.02 9 99 0.09 -0.012 -1.53 -0.523 1.03 -0.633 -0.043 -1.13 1.87 -0.738 10 99 0.1 0.117 -1.43 -0.484 0.922 -0.766 -0.047 0.285 1.37 -0.156 # ‚Ä¶ with 303,290 more rows, and 3 more variables: ra_x \u0026lt;dbl\u0026gt;, ra_y \u0026lt;dbl\u0026gt;, # ra_z \u0026lt;dbl\u0026gt; Each file contains 14 variables:\nactivity ‚Äì type of activity (1=walking; 2=descending stairs; 3=ascending stairs; 4=driving; 77=clapping; 99=non-study activity); time_s ‚Äì time from device initiation (seconds [s]); lw_x, lw_y, lw_z ‚Äì acceleration [g] measured by a left wrist-worn sensor at axis x, y, z; lh_x, lh_y, lh_z ‚Äì acceleration [g] measured by a left hip-worn sensor at axis x, y, z; la_x, la_y, la_z ‚Äì acceleration [g] measured by a left ankle-worn sensor at axis x, y, z; ra_x, ra_y, ra_z ‚Äì acceleration [g] measured by a right ankle-worn sensor at axis x, y, z. An exemplary few seconds of raw data from of walking and driving activities, collected by sensors at four different locations:\n(Click to see the code.) loc_id_levels \u0026lt;- c(\u0026#34;lw\u0026#34;, \u0026#34;lh\u0026#34;, \u0026#34;la\u0026#34;, \u0026#34;ra\u0026#34;) loc_id_labels \u0026lt;- c(\u0026#34;left wrist\u0026#34;, \u0026#34;left hip\u0026#34;, \u0026#34;left ankle\u0026#34;, \u0026#34;right ankle\u0026#34;) activity_levels \u0026lt;- c(\u0026#34;walking\u0026#34;, \u0026#34;driving\u0026#34;) plt_df \u0026lt;- dat_i %\u0026gt;% filter(activity %in% c(1,4)) %\u0026gt;% group_by(activity) %\u0026gt;% mutate(time_s = time_s - min(time_s)) %\u0026gt;% ungroup() %\u0026gt;% filter(time_s \u0026gt;= (5 + 5), time_s \u0026lt; (5 + 10)) %\u0026gt;% mutate(activity = recode(activity, ‚Ä¶","date":1625951968,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625951968,"objectID":"ac294fa861c638aab47728620ded81fb","permalink":"https://example.com/post/2021-06-29-pa_measures_and_summarizedactigraphy/","publishdate":"2021-07-10T17:19:28-04:00","relpermalink":"/post/2021-06-29-pa_measures_and_summarizedactigraphy/","section":"post","summary":"In this post, we:\nuse dataset ‚ÄúLabeled raw accelerometry data captured during walking, stair climbing and driving‚Äù that is freely available on PhysioNet;\nderive four minute-level summary measures of physical activity ‚Äì AC, MIMS, ENMO, MAD, AI ‚Äì from raw accelerometry data using SummarizedActigraphy R package;\nsummarize minute-level summary measures across walking and driving activities.\n","tags":[],"title":"Computing minute-level summary measures of physical activity from raw accelerometry data in R: AC, MIMS, ENMO, MAD, and AI","type":"post"},{"authors":null,"categories":[],"content":"2023 [2023-03-06] Our paper ‚ÄúWearable device and smartphone data can track ALS disease progression and may serve as novel clinical trial outcome measures‚Äù is published. This work investigates whether mobile applications and wearable devices can be used to quantify ALS disease progression through active (surveys) and passive (sensors) data collection. 2022 [2022-11-21] Our preprint ‚ÄúWearable device and smartphone data can track ALS disease progression and may serve as novel clinical trial outcome measures‚Äù is out. This work investigates whether mobile applications and wearable devices can be used to quantify ALS disease progression through active (surveys) and passive (sensors) data collection. [2022-10-25] Our preprint ‚ÄúPerformance analyses of step-counting algorithms using wrist accelerometry‚Äù is available online. This work evaluates performance of several modern wrist-accelerometry-based algorithms for step count estimation using a common dataset with various continuous walking trials. [2022-07-27] Our paper ‚ÄúEstimating knee movement patterns of recreational runners across training sessions using multilevel functional regression models‚Äù is published. This work can serve as a reference for practitioners modeling repeated functional measures at different resolution levels in the context of biomechanics and sports science applications. [2022-07-22] Our paper ‚ÄúComparison of Accelerometry-Based Measures of Physical Activity: Retrospective Observational Data Analysis Study‚Äù is published. This work provides comparison and harmonization mapping between minute-level accelerometry-derived measures (ActiGraph AC, MIMS, ENMO, MAD, AI). [2022-07-13] Our paper ‚ÄúQuantification of acceleration as activity counts in ActiGraph wearables‚Äù is out. This work publishes activity counts algorithm from ActiGraph‚Äôs ActiLife and CentrePoint. [2022-04-27] I was elected as a member of the Alpha chapter of the Delta Omega Society. A nice certificate I received is available here. [2022-01-13] Our paper ‚ÄúFree-living gait cadence measured by wearable accelerometer: a promising alternative to traditional measures of mobility for assessing fall risk‚Äù is published. We showed that in community-dwelling older adults, free-living walking cadence was significantly related to fall rates (while clinic-based mobility measures were not).\n[2022-01-01] For my first post-grad role, I accepted a postdoctoral researcher position at the Onnela Lab at Harvard University. I will be working at methods for digital phenotyping and its applications.\n2021 I defended my PhD thesis ‚ÄúStatistical Methods for Wearable Device Data and Sample Size Calculation in Complex Models‚Äù! My defense slides, with video recording linked, are available. I published the Acknowledgements and Dedication parts of the thesis.\nI received Travel Reimbursement Award to 3rd Annual Health Data Science Symposium at Harvard to present our work on harmonization of open-source and proprietary accelerometry-based physical activity measures in Nov 2021. Our preprint ‚ÄúUpstrap for estimating power and sample size in complex models‚Äù is available on bioRxiv. We evaluate power estimation properties of the upstrap and provide a series of ‚Äúread, adapt and use‚Äù R code examples for simple and complex settings, including GLMM. See the relevant project page.\nI received Student Poster Sponsorship and will present our work on harmonization of open-source and proprietary accelerometry-based physical activity measures during the ActiGraph Digital Data Summit in Nov 2021. I am a co-instructor for Introduction to R for Public Health Researchers Summer Institute week-long course. All class materials are available online. Our paper ‚ÄúEstimation of free-living walking cadence from wrist-worn sensor accelerometry data and its association with SF-36 quality of life scores‚Äù is published. Check out the GitHub repo showing examples of using our method for automatic walking strides segmentation from wrist-worn sensor accelerometry data collected in the free-living environment. I received the Louis I. and Thomas D. Dublin Award for the Advancement of Epidemiology and Biostatistics for work on open-source physical activity measurements. I received the Helen Abbey Award for excellence in teaching. Our paper ‚ÄúConnectivity‚Äêinformed adaptive regularization for generalized outcomes‚Äù is published. We proposed riPEER (ridgified Partially Empirical Eigenvectors for Regression) extension to generalized linear regression, addressing both theoretical and computational issues. See the relevant project page. 2020 Our paper ‚ÄúPredicting Subjective Recovery from Lower Limb Surgery Using Consumer Wearables‚Äù is published. We showed that passively collected wearable PGHD can capture post-surgery physical activity changes relative to individual‚Äôs baseline, and baseline data can improve prediction of self-reported recovery time at 4 weeks post surgery. During the upcoming workshop ‚ÄúThe Future of Digital Health‚Äù, I will give a talk highlighting ‚Ä¶","date":1598313600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598313600,"objectID":"4964d15a2ea64cf2255afdddb4bb9834","permalink":"https://example.com/resources/recently-all/","publishdate":"2020-08-25T00:00:00Z","relpermalink":"/resources/recently-all/","section":"resources","summary":"2023 [2023-03-06] Our paper ‚ÄúWearable device and smartphone data can track ALS disease progression and may serve as novel clinical trial outcome measures‚Äù is published. This work investigates whether mobile applications and wearable devices can be used to quantify ALS disease progression through active (surveys) and passive (sensors) data collection.","tags":[],"title":"All past updates","type":"resources"},{"authors":null,"categories":[],"content":"2022 Repeated Measures Correlation (rmcorr): when may be not an ideal choice to quantify the association\nGPS data in R: parse and plot GPX data exported from Strava\n2021 Computing minute-level summary measures of physical activity from raw accelerometry data in R: AC, MIMS, ENMO, MAD, and AI\nUpstrap for estimating power and sample size in complex models\n2019 When to use t distribution versus normal distribution quantiles in constructing confidence interval for the mean\nFavourite topics seen at RStudio conference 2019\nrunstats R package: fast computation of running statistics for time series\nadeptdata R package: raw accelerometry data sets and their derivatives\nAdaptive empirical pattern transformation (ADEPT): a fast, scalable, and accurate method for pattern segmentation in time series\n2018 (In Polish) Aplikacja na studia doktoranckie (Biostatystyka) w USA: notatki nt. procesu rekrutacji 2017 Brain connectivity-informed regularization methods for regression ","date":1598313600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598313600,"objectID":"01362b7264e88b75504a10a21ac7a901","permalink":"https://example.com/resources/posts-all/","publishdate":"2020-08-25T00:00:00Z","relpermalink":"/resources/posts-all/","section":"resources","summary":"2022 Repeated Measures Correlation (rmcorr): when may be not an ideal choice to quantify the association\nGPS data in R: parse and plot GPX data exported from Strava\n2021 Computing minute-level summary measures of physical activity from raw accelerometry data in R: AC, MIMS, ENMO, MAD, and AI","tags":[],"title":"All posts","type":"resources"},{"authors":null,"categories":[],"content":"* Joint first co-authorship.\n2023 Tracking ALS disease progression using passively collected smartphone sensor data by Karas, M.*, Olsen, J.*, Straczkiewicz, M., Johnson, S. A., Burke, K. M., Iwasaki, S., Lahav, A., Scheier, Z. A., Clark, A. P., Iyer, A. S., Huang, E., Berry, J. D., \u0026amp; Onnela, J.-P. (2023). [Preprint]. Preprint | Code Wearable device and smartphone data quantify ALS progression and may provide novel outcome measures by Johnson, S. A.*, Karas, M.*, Burke, K. M., Straczkiewicz, M., Scheier, Z. A., Clark, A. P., Iwasaki, S., Lahav, A., Iyer, A. S., Onnela, J.-P., \u0026amp; Berry, J. D. (2023). Npj Digital Medicine, 6(1), 34. Journal | Code Estimating knee movement patterns of recreational runners across training sessions using multilevel functional regression models by Matabuena, M.*, Karas, M.*, Riazati, S., Caplan, N., \u0026amp; Hayes, P. R. (2023). The American Statistician, 77(2), 169‚Äì181. Preprint | Journal | Code Free-living gait cadence measured by wearable accelerometer: a promising alternative to traditional measures of mobility for assessing fall risk by Urbanek, J. K., Roth, D. L., Karas, M., Wanigatunga, A. A., Mitchell, C. M., Juraschek, S. P., Cai, Y., Appel, L. J., \u0026amp; Schrack, J. A. (2023). The Journals of Gerontology: Series A, 78(5), 802‚Äì810. PubMed | Journal\n2022 Quantification of acceleration as activity counts in ActiGraph wearable. by Neishabouri, A., Nguyen, J., Samuelsson, J., Guthrie, T., Biggs, M., Wyatt, J., Cross, D., Karas, M., Migueles, J. H., Khan, S., \u0026amp; Guo, C. C. (2022). Scientific Reports, 12(1), 11958. Journal | Software Comparison of accelerometry-based measures of physical activity: retrospective observational data analysis study by Karas, M.*, Muschelli, J.*, Leroux, A., Urbanek, J. K., Wanigatunga, A. A., Bai, J., Crainiceanu, C. M., \u0026amp; Schrack, J. A. (2022). JMIR mHealth and uHealth, 10(7), e38077. Journal | Code Performance analyses of step-counting algorithms using wrist accelerometry by Pilkar, R., Gerstel, D., Toole, E., Biggs, M., Guthrie, T., Karas, M., Achkar, C. M. E., Renevey, P., Soltani, A., Sloan, S., Nguyen, J., Patterson, M. R., Ferrario, D., Lemay, M., Neishabouri, A., \u0026amp; Guo, C. (2022). [Preprint]. Preprint Smartphone-based gait cadence to identify older adults with decreased functional capacity by Rubin, D. S., Ranjeva, S. L., Urbanek, J. K., Karas, M., Madariaga, M. L. L., \u0026amp; Huisingh-Scheetz, M. (2022). Digital Biomarkers, 6(2), 61‚Äì70. Journal\n2021 Upstrap for estimating power and sample size in complex models by Karas, M., \u0026amp; Crainiceanu, C. M. (2021). [Preprint]. Preprint | Code Adaptive empirical pattern transformation (ADEPT) with application to walking stride segmentation by Karas, M., Straczkiewicz, M., Fadel, W., Harezlak, J., Crainiceanu, C. M., \u0026amp; Urbanek, J. K. (2021). Biostatistics, 22(2), 331‚Äì347. Journal | Code | Software (R) | Software (Python) Estimation of free-living walking cadence from wrist-worn sensor accelerometry data and its association with SF-36 quality of life scores by Karas, M., Urbanek, J. K., Illiano, V. P., Bogaarts, G., Crainiceanu, C. M., \u0026amp; Dorn, J. F. (2021). Physiological Measurement, 42(6), 065006. Preprint | Journal | Code | Software Connectivity‚Äêinformed adaptive regularization for generalized outcomes by Brzyski, D., Karas, M., M Ances, B., Dzemidzic, M., Go√±i, J., W Randolph, T., \u0026amp; Harezlak, J. (2021). Canadian Journal of Statistics, 49(1), 203‚Äì227. Journal\n2020 Predicting subjective recovery from lower limb surgery using consumer wearables by Karas, M., Marinsek, N., Goldhahn, J., Foschini, L., Ramirez, E., \u0026amp; Clay, I. (2020). Digital Biomarkers, 4(Suppl. 1), 73‚Äì86. Journal 2019 Accelerometry data in health research: challenges and opportunities. Review and examples by Karas, M., Bai, J., StrƒÖczkiewicz, M., Harezlak, J., Glynn, N. W., Harris, T., Zipunnikov, V., Crainiceanu, C., \u0026amp; Urbanek, J. K. (2019). Statistics in Biosciences, 11(2), 210‚Äì237. PubMed | Journal Brain connectivity-informed regularization methods for regression by Karas, M., Brzyski, D., Dzemidzic, M., Go√±i, J., Kareken, D. A., Randolph, T. W., \u0026amp; Harezlak, J. (2019). Statistics in Biosciences, 11(1), 47‚Äì90. PubMed | Journal | Software\n","date":1598313600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598313600,"objectID":"4c57682240a0a8ec56a1ee487d53ed0d","permalink":"https://example.com/resources/publications-all/","publishdate":"2020-08-25T00:00:00Z","relpermalink":"/resources/publications-all/","section":"resources","summary":"* Joint first co-authorship.\n2023 Tracking ALS disease progression using passively collected smartphone sensor data by Karas, M.*, Olsen, J.*, Straczkiewicz, M., Johnson, S. A., Burke, K. M., Iwasaki, S., Lahav, A.","tags":[],"title":"All publications","type":"resources"},{"authors":null,"categories":[],"content":"2023 Wearable device and smartphone data for tracking ALS disease progression, talk, Digital Health Innovation Summit, July 7, 2023, San Francisco, CA, 2023. Slides 2022 Wearable devices can track ALS disease progression and may serve as novel clinical trial outcome measures, poster, ActiGraph Digital Data Summit 2022, Nov 13, 2022, Pensacola Beach, FL, USA. Comparison of accelerometry-based measures of physical activity, talk, JSM, Aug 8, 2023, Washington, DC, USA. Slides Smartphone devices data in health research: opportunities and challenges, talk, Wearable and Implantable Technology group meeting (Department of Biostatistics, Johns Hopkins University), Mar 4, 2022, virtual. Slides\n2021 Harmonization of open-source and proprietary accelerometry-based physical activity measures, talk, 3rd Annual Health Data Science Symposium: Smartphones, Wearables, and Health, Nov 5, 2021, Boston, MA, USA. Statistical methods and modeling of person-generated health data from wearable and mobile devices, talk, Onnela Lab meeting (Department of Biostatistics, Harvard University), Jul 28, 2021, virtual. Accelerometry-based physical activity measures across NHANES waves. Comparison of five minute-level measures: AC, MIMS, ENMO, MAD, AI, talk, Wearable and Implantable Technology group meeting (Department of Biostatistics, Johns Hopkins University), May 13, 2021, virtual. National Health and Nutrition Examination Survey (NHANES) and comparability of accelerometry-derived physical activity measures across NHANES waves, talk, ENGAGE lab meeting (Department of Epidemiology, Johns Hopkins University), May 6, 2021, virtual. Comparison of accelerometry-derived physical activity summary measures by age, sex, and BMI, talk, The Epidemiology and Biostatistics of Aging (EBA) Training Program: 6th Annual Joint Presentation of the SMART/ WIT, ENGAGE, and BHS Lab groups, Apr 27, 2021, virtual. Video\n2020 Estimation of free-living walking cadence from wrist-worn sensor accelerometry data and its association with SF-36 quality of life scores, talk, CMStatistics 2020, Dec 20, 2020, virtual. arctools R package: Processing and physical activity summaries of minute level activity data, talk, Wearable and Implantable Technology group meeting (Department of Biostatistics, Johns Hopkins University), Dec 17, 2020, virtual. Code | Slides Predicting subjective recovery from lower limb surgery using consumer wearables, talk, Workshop ‚ÄòThe Future of Digital Health‚Äô, Dec 8, 2020, virtual. Slides | Video Recent developments in R tidyverse, talk, Computing Club (Department of Biostatistics, Johns Hopkins University), Dec 1, 2020, virtual. Slides | Video Summer internship project: Predicting subjective recovery from lower limb surgery using consumer wearables, talk, Wearable and Implantable Technology group meeting (Department of Biostatistics, Johns Hopkins University), Sep 30, 2020, virtual. Novel approach for precise walking cadence estimation from high-density tri-axial accelerometry data collected at wrist in free-living, talk, 41st Annual Conference of the International Society for Clinical Biostatistics, Aug 27, 2020, virtual. arctools: R software for computing summaries of minute-level physical activity data, talk, ENGAGE lab meeting (Department of Epidemiology, Johns Hopkins University), Jun 4, 2020, virtual. Precise walking strides segmentation from raw accelerometry data: ADEPT method, talk, Webinar for OSS developers in physical behavior field, Mar 11, 2020, virtual. Code | Slides Towards precise walking strides segmentation from accelerometry data in free-living: ADEPT method and further developments, talk, BIRS: Use of Wearable and Implantable Devices in Health Research, Feb 25, 2020, Banff, Alberta, Canada. 2019 Functional registration of walking strides in high-density accelerometry data for estimation of gait asymmetry, talk, CFE-CMStatistics 2019 conference, Dec 16, 2019, London, UK. Methods for fast processing of time-series: runstats R package, talk, Webinar for OSS developers in physical behavior field, Nov 5, 2019, virtual. Code | Slides Considerations in statistical modeling of walking features derived from wrist-worn sensor in free-living, talk, Wearable and Implantable Technology group meeting (Department of Biostatistics, Johns Hopkins University), Oct 31, 2019, Baltimore, MD, USA. Overview of JHU Biostat Wearable and Implantable Technology (WIT) working group research, talk, Journal Club (Department of Biostatistics, Johns Hopkins University), Oct 15, 2019, Baltimore, MD, USA. Slides Walking measurements derived from free-living wrist-worn sensor as novel digital endpoints, talk, Novartis 2019 US Analytics Conference - Digital Endpoint Analytics Session, Oct 8, 2019, East Hanover, NJ. Automatic estimation of step asymmetry in a split-belt treadmill experiment using high-resolution accelerometry data, talk, ICAMPAM 2019 conference, Jun 26, 2019, Maastricht, the Netherlands. Physical activity monitoring with wearable accelerometers: ‚Ä¶","date":1598313600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598313600,"objectID":"076996b33761faafcf689785c4ba43f9","permalink":"https://example.com/resources/talks-all/","publishdate":"2020-08-25T00:00:00Z","relpermalink":"/resources/talks-all/","section":"resources","summary":"2023 Wearable device and smartphone data for tracking ALS disease progression, talk, Digital Health Innovation Summit, July 7, 2023, San Francisco, CA, 2023. Slides 2022 Wearable devices can track ALS disease progression and may serve as novel clinical trial outcome measures, poster, ActiGraph Digital Data Summit 2022, Nov 13, 2022, Pensacola Beach, FL, USA.","tags":[],"title":"All talks","type":"resources"},{"authors":[],"categories":[],"content":"We propose adaptive empirical pattern transformation (ADEPT), a fast, scalable, and accurate method for pattern segmentation in time-series.\nTable of Contents Scientific problem Challenges Proposed solution Published work Software Images used in the post ‚Äì credit/references Scientific problem The motivation for the work was to provide fast and accurate open-source method for pattern segmentation from raw accelerometry data.\nThe methods were needed for automated walking strides segmentation from accelerometry recordings collected during continuous walking that we had across a number of health studies.\nChallenges The plot below shows an example of raw accelerometry data ‚Äì three-dimensional time-series of acceleration [g] measurements. Data showed were collected 5 s of walking for two different individuals, with 4 wearable sensors worn simultaneously at wrist, hip, left, and right ankle.\nWhile the repetitive patterns of walking are relatively clear to a human observer, there are a few challenges in segmenting them accurately with an algorithm:\nThere are variations in shape, magnitude and duration of a pattern within individual‚Äôs data. These might be e.g. due to terrain elevation changes, or temporal changes of step length and cadence (think about slowing down when approaching the turn of the corridor, or basically walking slower during an evening stroll versus morning rush to work). There is variability of walking data between individuals (e.g. see the plot above). A sensor can move, or be worn on different hands by the same person on different days.\nProposed solution We propose adaptive empirical pattern transformation (ADEPT) to segment walking stride patterns in vector magnitude of raw accelerometry data.\nThe ADEPT algorithm uses a predefined template and detects its repetitions by maximizing the local distance (i.e. correlation) between (a) collection of scale-transformed templates and (b) the observed data signal. The scale-transformation adjusts the duration of the dictionary template, allowing for the detection of patterns that are shorter or longer than the original dictionary template. Multiple distinct baseline templates can be used simultaneously to account for various shape patterns occurring in the data.\nThe GIF below demonstrates the big picture of the algorithm.\nThe underlying template‚Äôs scaling and translating along the observed data signal is closely related to the Continuous Wavelet Transform (CWT), $$W_{\\Psi}(s, \\tau) = \\int_{-\\infty}^{\\infty} x(t) \\frac{1}{\\sqrt{s}}\\Psi \\left(\\frac{t - \\tau}{s} \\right)dt.$$ Conversely to CWT‚Äôs mother wavelet $\\Psi(\\cdot)$, ADEPT uses a data-based pattern function (not required to satisfy the wavelet admissibility condition) and comes with a number of other algorithm features tailored for its target application.\nPublished work We published the proposed ADEPT method in work Adaptive empirical pattern transformation (ADEPT) with application to walking stride segmentation Karas, M., Straczkiewicz, M., Fadel, W., Harezlak, J., Crainiceanu, C.M., Urbanek, J.K. (2018). Biostatistics, Volume 22, Issue 2, April 2021, Pages 331‚Äì347. Software We provided open-source implementation of the proposed ADEPT method in R package adept (CRAN index). The R package is accompanied by two vignettes:\nIntroduction to adept package,\nWalking strides segmentation with adept.\nüéâ The adept R package was selected in Top 40 new CRAN packages in May 2019 (list link).\nImages used in the post ‚Äì credit/references Featured image. Figure 2 in the manuscript: Karas, M., Straczkiewicz, M., Fadel, W., Harezlak, J., Crainiceanu, C.M., Urbanek, J.K. Adaptive empirical pattern transformation (ADEPT) with application to walking stride segmentation (2018). Biostatistics, Volume 22, Issue 2, April 2021, Pages 331‚Äì347. Link (last accessed on May 26, 2021).\nThree-dimensional time-series image. Figure 1 in the manuscript: Karas, M., Straczkiewicz, M., Fadel, W., Harezlak, J., Crainiceanu, C.M., Urbanek, J.K. Adaptive empirical pattern transformation (ADEPT) with application to walking stride segmentation (2018). Biostatistics, Volume 22, Issue 2, April 2021, Pages 331‚Äì347. Link (last accessed on May 26, 2021).\n","date":1575647693,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575647693,"objectID":"17f4fa5ce9f2b4d14b5833c3cc025a70","permalink":"https://example.com/post/project_adept/","publishdate":"2019-12-06T11:54:53-04:00","relpermalink":"/post/project_adept/","section":"post","summary":"We propose adaptive empirical pattern transformation (ADEPT), a fast, scalable, and accurate method for pattern segmentation in time-series.","tags":["statistical methods","wearable accelerometers","actigraphy","pattern segmentation","walking","cadence"],"title":"Adaptive empirical pattern transformation (ADEPT)","type":"post"},{"authors":null,"categories":[],"content":"To construct confidence interval for the mean, we often use quantiles of standardized sample mean distribution. Here, I include a list of cases where I‚Äôd use quantiles of t-distribution versus quantiles of normal distribution for that purpose.\nNote: the below text could be directly translated to answer when to use t-test versus z-test in testing hypothesis about the mean parameter.\nTable of Contents Example 1: constructing confidence interval for $\\mu$ with $z$-quantiles Example 2: constructing confidence interval for $\\mu$ with $t$-quantiles Case 1: observations from normal distribution, $\\sigma$ known, any $n$ Case 2: observations from normal distribution, $\\sigma$ unknown, small $n$ Case 3: observations from normal distribution, $\\sigma$ unknown, large $n$ Case 4: observations from any distribution, $\\sigma$ known, small $n$ Case 5: observations from any distribution, $\\sigma$ known, large $n$ Case 6: observations from any distribution, $\\sigma$ unknown, small $n$ Case 7: observations from any distribution, $\\sigma$ unknown, large $n$ Standardized sample mean Consider $X_1, \\ldots, X_n$ ‚Äì a sequence of i.i.d. random variables with mean $E(X_i) = \\mu$ and variance $\\text{var}(X_i) = \\sigma^2$. To construct confidence intervals for $\\mu$ parameter, we often use a standardized sample mean,\n$$ \\begin{aligned} \\frac{\\overline{X}_n - \\mu}{\\sigma/\\sqrt{n}}, \\end{aligned} $$\nor its version where $S_n$ ‚Äì a consistent estimator of true standard deviation $\\sigma$ ‚Äì is used, $\\frac{\\overline{X}_n - \\mu}{S_n/\\sqrt{n}}$; the latter is common in practice as we typically do not know $\\sigma$ and must estimate it from the data. Knowing distribution of a standardized sample mean allows us to construct confidence interval for a mean $\\mu$ parameter.\nExample 1: constructing confidence interval for $\\mu$ with $z$-quantiles Assume $X_1, \\ldots, X_n$ are i.i.d. $\\sim N(\\mu,\\sigma^2)$ and $\\sigma$ is known. Then we have an exact distributional result for a standardized sample mean,\n$$ \\begin{aligned} \\frac{\\overline{X}_n - \\mu}{\\sigma/\\sqrt{n}} \\sim N(0,1). \\end{aligned} $$\nLet us denote $z_{ 1-\\frac{\\alpha}{2}}$ to be $(1-\\frac{\\alpha}{2})$-th quantile of standard normal distribution $N(0,1)$. Since $N(0,1)$ is symmetric around $0$, we have $z_{\\frac{\\alpha}{2}} = -z_{1-\\frac{\\alpha}{2}}$ and we can write\n$$ \\begin{aligned} 1-\\alpha = P\\left(-z_{1-\\frac{\\alpha}{2}} \\leq \\frac{\\overline{X}_n-\\mu}{\\sigma/\\sqrt{n}} \\leq z_{1-\\frac{\\alpha}{2}} \\right) = P\\left(\\bar{X}_n-z_{1-\\frac{\\alpha}{2}} \\frac{\\sigma}{\\sqrt{n}} \\leq \\mu \\leq \\bar{X}_n+z_{1-\\frac{\\alpha}{2}} \\frac{\\sigma}{\\sqrt{n}} \\right), \\end{aligned} $$\nwhich yields that $\\left[ \\bar{X}_n-z_{1-\\frac{\\alpha}{2}} \\frac{\\sigma}{\\sqrt{n}}, ; \\bar{X}_n+z_{1-\\frac{\\alpha}{2}} \\frac{\\sigma}{\\sqrt{n}}\\right]$ is a $(1-\\alpha )$-confidence interval for a mean parameter $\\mu$.\nExample 2: constructing confidence interval for $\\mu$ with $t$-quantiles Assume $X_1, \\ldots, X_n$ are i.i.d. $\\sim N(\\mu,\\sigma^2)$ and $\\sigma$ is unknown. We use $S_n$ ‚Äì a consistent sample estimator of true standard deviation ‚Äì to approximate $\\sigma$, and have an exact distributional result for a standardized sample mean,\n$$ \\begin{aligned} \\frac{\\overline{X}_n - \\mu}{S_n/\\sqrt{n}} \\sim t_{n-1}. \\end{aligned} $$\nLet us denote $t_{n-1,1-\\frac{\\alpha}{2}}$ to be a $(n-1,1-\\frac{\\alpha}{2})$-th quantile of $t$-distributuon with $n-1$ degrees of freedom. Since $t$ is symmetric around $0$, we have\n$$ \\begin{aligned} 1-\\alpha =P\\left(-t_{n-1,1-\\frac{\\alpha}{2}} \\leq \\frac{\\bar{X}_{n}-\\mu}{S_{n} / \\sqrt{n}} \\leq t_{n-1,1-\\frac{\\alpha}{2}}\\right) = P\\left(\\bar{X}_n-t_{n-1, 1-\\frac{\\alpha}{2}}\\frac{S_n}{\\sqrt{n}} \\leq \\mu \\leq \\bar{X}_n+t_{n-1, 1-\\frac{\\alpha}{2}} \\frac{S_n}{\\sqrt{n}} \\right), \\end{aligned} $$\nwhich yields that $\\left[ \\bar{X}_n-t_{n-1, 1-\\frac{\\alpha}{2}} \\frac{S_n}{\\sqrt{n}}, ; \\bar{X}_n+t_{n-1, 1-\\frac{\\alpha}{2}} \\frac{S_n}{\\sqrt{n}}\\right]$ is a $(1-\\alpha )$-confidence interval for a mean parameter $\\mu$.\nCases In many cases, whether to use quantiles of $t$-student distribution versus standard normal distribution is based on:\ndistribution of $X_1, \\ldots, X_n$ variables, whether $\\sigma$ is known or not (and we need to estimate it i.e. with $S_n$), what is sample size $n$. Note: the below cases could be directly translated to answer when to use $t$-test versus $z$-test in testing hypothesis about the mean $\\mu$ parameter, i.e. test for $H_0: \\mu = \\mu_0$ versus $H_1: \\mu \u0026lt; \\mu_0$, or $H_1: \\mu \\neq \\mu_0$, or $H_1: \\mu \u0026gt; \\mu_0$.\nCase 1: observations from normal distribution, $\\sigma$ known, any $n$ Observations $X_1, \\ldots, X_n$ are from normal $N(\\mu, \\sigma^2)$ distribution. $\\sigma$ known. Any sample size $n$. $\\Rightarrow$ We have exact result that $\\frac{\\bar{X}_{n}-\\mu}{\\sigma / \\sqrt{n}} \\sim N(0,1)$ and hence we use quantiles of normal distribution in constructing the CI.\nCase 2: observations from normal distribution, $\\sigma$ unknown, small $n$ Observations $X_1, \\ldots, X_n$ are ‚Ä¶","date":1572297568,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572297568,"objectID":"d1bdde96b4cf824c5001d198319f10e4","permalink":"https://example.com/post/2019-10-28-ttest-versus-ztest/","publishdate":"2019-10-28T17:19:28-04:00","relpermalink":"/post/2019-10-28-ttest-versus-ztest/","section":"post","summary":"To construct confidence interval for the mean, we often use quantiles of standardized sample mean distribution. Here, I include a list of cases where I‚Äôd use quantiles of t-distribution versus quantiles of normal distribution for that purpose.\nNote: the below text could be directly translated to answer when to use t-test versus z-test in testing hypothesis about the mean parameter.\n","tags":[],"title":"When to use t distribution versus normal distribution quantiles in constructing confidence interval for the mean","type":"post"},{"authors":[],"categories":[],"content":"Package adeptdata was created to host raw accelerometry data sets and their derivatives. Some of them are used in the corresponding adept package.\nPackage CRAN index is located here. Package GitHub repo is located here.\nTable of Contents Outdoor continuous walking raw accelerometry data acc_walking_IU Outdoor run raw accelerometry data acc_running Walking stride accelerometry data templates stride_template Installation Install from CRAN.\ninstall.packages(\u0026#34;adeptdata\u0026#34;) Data objects Outdoor continuous walking raw accelerometry data acc_walking_IU acc_walking_IU is a sample of raw accelerometry data collected during outdoor continuous walking from 32 healthy participants between 23 and 52 years of age. Data were collected at frequency 100 Hz simultaneously with four wearable accelerometers located at left wrist, left hip and both ankles. See ?acc_walking_IU for details.\nlibrary(adeptdata) library(dplyr) library(ggplot2) library(reshape2) library(lubridate) acc_walking_IU %\u0026gt;% filter(time_s \u0026lt; 6, subj_id == acc_walking_IU$subj_id[1]) %\u0026gt;% mutate(loc_id = factor( loc_id, levels = c(\u0026#34;left_wrist\u0026#34;, \u0026#34;left_hip\u0026#34;, \u0026#34;left_ankle\u0026#34;, \u0026#34;right_ankle\u0026#34;), labels = c(\u0026#34;Left wrist\u0026#34;, \u0026#34;Left hip\u0026#34;, \u0026#34;Left ankle\u0026#34;, \u0026#34;Right ankle\u0026#34;))) %\u0026gt;% melt(id.vars = c(\u0026#34;subj_id\u0026#34;, \u0026#34;loc_id\u0026#34;, \u0026#34;time_s\u0026#34;)) %\u0026gt;% ggplot(aes(x = time_s, y = value, color = variable)) + geom_line() + facet_wrap(~ loc_id, ncol = 2) + theme_bw(base_size = 9) + labs(x = \u0026#34;Exercise time [s]\u0026#34;, y = \u0026#34;Amplitude [g]\u0026#34;, color = \u0026#34;Sensor\\naxis\u0026#34;, title = \u0026#34;Raw accelerometry data of walking (100 Hz)\u0026#34;) Outdoor run raw accelerometry data acc_running acc_running is a sample raw accelerometry data collected during 25 minutes of an outdoor run. Data were collected at frequency 100 Hz with two ActiGraph GT9X Link sensors located at left hip and left ankle. See ?acc_running for details.\nt1 \u0026lt;- ymd_hms(\u0026#34;2018-10-25 18:07:00\u0026#34;, tz = \u0026#34;UTC\u0026#34;) t2 \u0026lt;- ymd_hms(\u0026#34;2018-10-25 18:20:30\u0026#34;, tz = \u0026#34;UTC\u0026#34;) t3 \u0026lt;- ymd_hms(\u0026#34;2018-10-25 18:22:00\u0026#34;, tz = \u0026#34;UTC\u0026#34;) acc_running %\u0026gt;% filter((date_time \u0026gt;= t1 \u0026amp; date_time \u0026lt; t1 + as.period(4, \u0026#34;seconds\u0026#34;)) | (date_time \u0026gt;= t2 \u0026amp; date_time \u0026lt; t2 + as.period(4, \u0026#34;seconds\u0026#34;)) | (date_time \u0026gt;= t3 \u0026amp; date_time \u0026lt; t3 + as.period(4, \u0026#34;seconds\u0026#34;)) ) %\u0026gt;% mutate(loc_id = factor( loc_id, levels = c(\u0026#34;left_hip\u0026#34;, \u0026#34;left_ankle\u0026#34;), labels = c(\u0026#34;Left hip\u0026#34;, \u0026#34;Left ankle\u0026#34;))) %\u0026gt;% melt(id.vars = c(\u0026#34;date_time\u0026#34;, \u0026#34;loc_id\u0026#34;)) %\u0026gt;% mutate(date_time_floor = paste0( \u0026#34;Minute start: \u0026#34;, floor_date(date_time, unit = \u0026#34;minutes\u0026#34;))) %\u0026gt;% ggplot(aes(x = date_time, y = value, color = variable)) + geom_line(size = 0.5) + facet_grid(loc_id ~ date_time_floor, scales = \u0026#34;free_x\u0026#34;) + theme_bw(base_size = 9) + labs(x = \u0026#34;Time [s]\u0026#34;, y = \u0026#34;Acceleration [g]\u0026#34;, color = \u0026#34;Sensor\\naxis\u0026#34;, title = \u0026#34;Raw accelerometry data (100 Hz)\u0026#34;) Walking stride accelerometry data templates stride_template stride_template is a list containing walking stride pattern templates derived from accelerometry data collected at four body locations: left wrist, left hip, left ankle, and right ankle. See ?stride_template for details.\ndata.frame( x = rep(seq(0, 1, length.out = 200), 2), y = c(stride_template$left_ankle[[2]][1, ], stride_template$left_ankle[[2]][2, ]), group = c(rep(1, 200), rep(2, 200))) %\u0026gt;% ggplot(aes(x = x, y = y, group = group)) + geom_line() + facet_grid(group ~ .) + theme_bw(base_size = 9) + labs(x = \u0026#34;Time [s]\u0026#34;, y = \u0026#34;Vector magnitude [g]\u0026#34;, title = \u0026#34;Walking stride templates (left ankle)\u0026#34;) ","date":1555948493,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555948493,"objectID":"e31d24678d6bf35de8a39af5fe667d01","permalink":"https://example.com/post/project_adeptdata/","publishdate":"2019-04-22T11:54:53-04:00","relpermalink":"/post/project_adeptdata/","section":"post","summary":"Package adeptdata was created to host raw accelerometry data sets and their derivatives.","tags":["rstats","data set","data","wearable accelerometers","actigraphy"],"title":"'adeptdata' R package: Raw Accelerometry Data Sets and Their Derivatives","type":"post"},{"authors":["Marta Karas"],"categories":null,"content":" Create your slides in Markdown - click the Slides button to check out the example. Add the publication‚Äôs full text or supplementary notes here. You can use rich formatting such as including code, math, and images.\n","date":1554595200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1554595200,"objectID":"557dc08fd4b672a0c08e0a8cf0c9ff7d","permalink":"https://example.com/publication/preprint/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/preprint/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example preprint / working paper","type":"publication"},{"authors":[],"categories":[],"content":"Package runstats provides methods for fast computation of running sample statistics for time series. The methods utilize Convolution Theorem to compute convolutions via Fast Fourier Transform (FFT). Implemented running statistics include:\nmean, standard deviation, variance, covariance, correlation, euclidean distance. Table of Contents Compare RunningCov {runstats} with a conventional method Compare RunningCov {runstats} with sliding_cov {dvmisc} c++ implementation Session info Website Package website is located here.\nInstallation install.packages(\u0026#34;runstats\u0026#34;) Usage library(runstats) ## Example: running correlation x0 \u0026lt;- sin(seq(0, 2 * pi * 5, length.out = 1000)) x \u0026lt;- x0 + rnorm(1000, sd = 0.1) pattern \u0026lt;- x0[1:100] out1 \u0026lt;- RunningCor(x, pattern) out2 \u0026lt;- RunningCor(x, pattern, circular = TRUE) ## Example: running mean x \u0026lt;- cumsum(rnorm(1000)) out1 \u0026lt;- RunningMean(x, W = 100) out2 \u0026lt;- RunningMean(x, W = 100, circular = TRUE) Running statistics To better explain the details of running statistics, package‚Äôs function runstats.demo(func.name) allows to visualize how the output of each running statistics method is generated. To run the demo, use func.name being one of the methods‚Äô names:\n\u0026#34;RunningMean\u0026#34;, \u0026#34;RunningSd\u0026#34;, \u0026#34;RunningVar\u0026#34;, \u0026#34;RunningCov\u0026#34;, \u0026#34;RunningCor\u0026#34;, \u0026#34;RunningL2Norm\u0026#34;. ## Example: demo for running correlation method runstats.demo(\u0026#34;RunningCor\u0026#34;) ## Example: demo for running mean method runstats.demo(\u0026#34;RunningMean\u0026#34;) Performance We use rbenchmark to measure elapsed time of RunningCov execution, for different lengths of time-series x and fixed length of the shorter pattern y.\nlibrary(rbenchmark) library(ggplot2) set.seed (20190315) x.N.seq \u0026lt;- 10^(3:7) x.list \u0026lt;- lapply(x.N.seq, function(N) runif(N)) y \u0026lt;- runif(100) ## Benchmark execution time of RunningCov out.df \u0026lt;- data.frame() for (x.tmp in x.list){ out.df.tmp \u0026lt;- benchmark( \u0026#34;runstats\u0026#34; = runstats::RunningCov(x.tmp, y), replications = 10, columns = c(\u0026#34;test\u0026#34;, \u0026#34;replications\u0026#34;, \u0026#34;elapsed\u0026#34;, \u0026#34;relative\u0026#34;, \u0026#34;user.self\u0026#34;, \u0026#34;sys.self\u0026#34;)) out.df.tmp$x_length \u0026lt;- length(x.tmp) out.df.tmp$pattern_length \u0026lt;- length(y) out.df \u0026lt;- rbind(out.df, out.df.tmp) } knitr::kable(out.df) test replications elapsed relative user.self sys.self x_length pattern_length runstats 10 0.004 1 0.003 0.000 1000 100 runstats 10 0.023 1 0.019 0.004 10000 100 runstats 10 0.183 1 0.148 0.035 100000 100 runstats 10 1.700 1 1.592 0.107 1000000 100 runstats 10 19.852 1 17.185 2.576 10000000 100 Compare RunningCov {runstats} with a conventional method To compare runstats performance with ‚Äúconventional‚Äù loop-based way of computing running covariance in R, we use rbenchmark package to measure elapsed time of runstats::RunningCov and running covariance implemented with sapply loop, for different lengths of time-series x and fixed length of the shorter time-series y.\n## Conventional approach RunningCov.sapply \u0026lt;- function(x, y){ l_x \u0026lt;- length(x) l_y \u0026lt;- length(y) sapply(1:(l_x - l_y + 1), function(i){ cov(x[i:(i+l_y-1)], y) }) } out.df2 \u0026lt;- data.frame() for (x.tmp in x.list[c(1:4)]){ out.df.tmp \u0026lt;- benchmark( \u0026#34;conventional\u0026#34; = RunningCov.sapply(x.tmp, y), \u0026#34;runstats\u0026#34; = runstats::RunningCov(x.tmp, y), replications = 10, columns = c(\u0026#34;test\u0026#34;, \u0026#34;replications\u0026#34;, \u0026#34;elapsed\u0026#34;, \u0026#34;relative\u0026#34;, \u0026#34;user.self\u0026#34;, \u0026#34;sys.self\u0026#34;)) out.df.tmp$x_length \u0026lt;- length(x.tmp) out.df2 \u0026lt;- rbind(out.df2, out.df.tmp) } Benchmark results\nplt1 \u0026lt;- ggplot(out.df2, aes(x = x_length, y = elapsed, color = test)) + geom_line() + geom_point(size = 3) + scale_x_log10() + theme_minimal(base_size = 14) + labs(x = \u0026#34;Vector length of x\u0026#34;, y = \u0026#34;Elapsed [s]\u0026#34;, color = \u0026#34;Method\u0026#34;, title = \u0026#34;Running covariance (x,y) rbenchmark\u0026#34;, subtitle = \u0026#34;Vector length of y = 100\u0026#34;) + theme(legend.position = \u0026#34;bottom\u0026#34;) plt2 \u0026lt;- plt1 + scale_y_log10() + labs(y = \u0026#34;Log of elapsed [s]\u0026#34;, title = \u0026#34;\u0026#34;) cowplot::plot_grid(plt1, plt2, nrow = 1, labels = c(\u0026#39;A\u0026#39;, \u0026#39;B\u0026#39;)) Compare RunningCov {runstats} with sliding_cov {dvmisc} c++ implementation dvmisc package (GitHub, CRAN) is a package for Convenience Functions, Moving Window Statistics, and Graphics, and includes functions for calculating moving-window statistics efficiently via c++, written by Dane Van Domelen. Here, we compare RunningCov {runstats} performance with c++ implementation from sliding_cov {dvmisc}. Dane contributed the code in its large part.\n# devtools::install_github(\u0026#34;vandomed/dvmisc\u0026#34;) library(dvmisc) set.seed(20100315) x.N.seq \u0026lt;- 10^(3:6) x.list \u0026lt;- lapply(x.N.seq, function(N) runif(N)) get.out.df \u0026lt;- function(y){ out.df \u0026lt;- data.frame() for (x.tmp in x.list){ if (length(x.tmp) \u0026lt; length(y)){ out.df.tmp \u0026lt;- data.frame( test = NA, replications = NA, elapsed = NA, relative = NA, user.self = NA, sys.self = NA) } else { out.df.tmp \u0026lt;- benchmark( \u0026#34;runstats\u0026#34; = runstats::RunningCov(x.tmp, y), \u0026#34;dvmisc\u0026#34; = dvmisc::sliding_cov(y, x.tmp), replications = 10, columns = c(\u0026#34;test\u0026#34;, \u0026#34;replications\u0026#34;, \u0026#34;elapsed\u0026#34;, \u0026#34;relative\u0026#34;, \u0026#34;user.self\u0026#34;, \u0026#34;sys.self\u0026#34;)) } out.df.tmp$x_length \u0026lt;- length(x.tmp) out.df \u0026lt;- rbind(out.df, out.df.tmp) } return(out.df) } out.df_y10 \u0026lt;- get.out.df(runif(10^1)) ‚Ä¶","date":1552665293,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552665293,"objectID":"4da29d14aa0d2e5c61810c41edab9aee","permalink":"https://example.com/post/project_runstats/","publishdate":"2019-03-15T11:54:53-04:00","relpermalink":"/post/project_runstats/","section":"post","summary":"Package runstats provides methods for fast computation of running sample statistics for time series via Fast Fourier Transform.","tags":["rstats","signal processing","methods"],"title":"'runstats' R package: Fast Computation of Running Statistics for Time Series","type":"post"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Hugo Blox Builder Hugo Blox Builder | Documentation\nFeatures Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides Controls Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;) Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\nFragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three A fragment can accept two optional parameters:\nclass: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\nOnly the speaker can read these notes Press S key to view Themes black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/media/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}} Custom CSS Example Let‚Äôs make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; } Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://example.com/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Hugo Blox Builder's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":[],"content":"I spent the last days of 2018-19 winter break in Austin, TX where I traveled for RStudio 2019 conference. I attended e-poster session and two days of the conference: Jan 17-18. Below I list some of the topics from the talks that I particularly liked and I think I am likely to benefit from in the future.\nAlso:\nThe official repo with abstracts for every session, workshop (together with workshop files free to download for most if not all of them), and e-poster can be accessed here. Table of Contents Data visualization Tyler Morgan-Wall (website, twitter) showing stunning gifs and pics while presenting ‚Äú3D mapping, plotting, and printing with rayshader‚Äù.\nFeatures showed include shadowing, rotating, water transparency, visualization of a simulation of different water levels. The talk had possibly the most exciting ‚Äúfuture work‚Äù announced - because who doesn‚Äôt get excited about the idea of ‚Äúrotating 3D ggplots‚Äù? üíò The rayshader package is available on GitHub (link).\n(The gif on the left and the image on the right are both sourced from ‚Äòrayshader‚Äô package GitHub website (link), as accessed on Jan 26, 2019.)\nThomas Lin Pedersen (website, twitter) giving a ‚Äúgganimate live cookbook‚Äù speech (and joining Tyler‚Äôs talk on the podium of most fun presentations, I guess üòÇ)\nThe gganimate package was introduced per ‚Äúextension to ggplot2‚Äù providing ‚Äúimplementation of the grammar of animated graphics‚Äù. Presentation slides are available online here; IMHO worth checking out for inspiring ways of presenting data over time! Besides, https://gganimate.com comes with a bunch of examples, like the one below.\n(The gif sourced from ‚Äògganimate‚Äô package website (link), as accessed on Jan 26, 2019.)\nDocuments building Yihui Xie (website, twitter) leaving the audience very enthusiastic (or maybe more like: blown away) with all the recent development in document building.\nThe presentation starts with ‚ÄúIn HTML and the Web I trust‚Äù (üòç I share like 99% of my work summaries with advisors and colleagues in a form HTML). With pagedown we can now go ahead and get paged HTML documents, e.g. business card, resume, poster. Fairly üë∂ development (\u0026#34;status: experimental\u0026#34;). Presentation slides are available here.\n(Slides 10, 11, 15, 17 from RStudio 2019 conference talk ‚Äúpagedown: Creating Beautiful PDFs with R Markdown + CSS + Your Web Browser‚Äù by Yihui Xie and Romain Lesur on Jan 18, 2019 in Austin, TX.)\nRich Iannone (website, twitter) introducing the gt package.\nWith gt package (link), one can turn a data table into ‚Äúinformation-rich, publication-quality‚Äù üéØ table outputs. The table outputs can be in HTML, LaTeX, and RTF. The ‚Äúmodular‚Äù way of building these reminds me of ggplot2 plots construction. Presentation slides are available here.\n(Slide 5. from RStudio 2019 conference talk ‚ÄúIntroducing the ‚Äògt‚Äô package‚Äù by Rich Iannone on Jan 18, 2019 in Austin, TX.)\nCommunity and personal development David Robinson (website, twitter) delivering a keynote talk ‚ÄúThe unreasonable effectiveness of public work‚Äù (slides).\nIMHO a phenomenal speech: a kind and resonating talk after which one not only wants to stand up and change the world right now üî• but also maintains the same feeling a week after üí™. Review the slides to: (a) learn Author‚Äôs points on why it so worth it to spend time on public work, (b) for a pack of actual how-to examples and guidelines on building a public portfolio, (c) for a bunch of interesting points made about a value of work (Author‚Äôs work, but fairly generalizable IMHO); my favorite is copy-pasted below! I particularly appreciated that all stages of advancement in building online portfolio were addressed, üë∂-steps including!\n(Slide 63. from RStudio 2019 conference keynote talk ‚ÄúThe unreasonable effectiveness of public work‚Äù by David Robinson on Jan 18, 2019 in Austin, TX.)\nJesse Mostipak (website, twitter) talking about the experience of building R4DS online learning community.\nSome empowering messages came in the lines of this talk - just look at the slides pics below to get the flavor üôå. My take-home ones include a description of a data scientist: ‚Äúconstantly learning, constantly making mistakes, constantly learning from them‚Äù.\n(Pictures I took during RStudio 2019 conference talk ‚ÄúR4DS online learning community‚Äù by Jesse Mostipak on Jan 17, 2019 in Austin, TX.)\n","date":1548544643,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548544643,"objectID":"8e0366c29c4dd9ece328fd9bc7e47b37","permalink":"https://example.com/post/2019-01-26-favs-of-rstudio2019-conference/","publishdate":"2019-01-26T19:17:23-04:00","relpermalink":"/post/2019-01-26-favs-of-rstudio2019-conference/","section":"post","summary":"I spent the last days of 2018-19 winter break in Austin, TX where I traveled for RStudio 2019 conference. I attended e-poster session and two days of the conference: Jan 17-18.","tags":[],"title":"Some of the topics seen at RStudio conference 2019","type":"post"},{"authors":null,"categories":[],"content":"W 2015 roku zdobylam tytul Magister na kierunku Matematyka na Politechnice Wroclawskiej. W sierpniu 2016 rozpoczelam aplikacje na programy doktoranckie na kierunku Biostatystyka w Stanach Zjednoczonych rozpoczynajace sie jesienia 2017. Dostalam i zaakeptowalam oferte z Departamentu Biostatystyki na Johns Hopkins Bloomberg School of Public Health w Baltimore (stan Maryland), oferujacego jeden z 3 najlepszych (ex aequo) programow na kierunku Biostatystyka w USA wg rankingu usnews.com z 2018 roku. W notatce opisuje etapy rekrutacji i podaje swoje obserwacje i wskazowki.\nTable of Contents Uwagi wstepne Ponizszy opis jest napisany w kontekscie aplikacji na kierunek Biostatystyka. Spodziewam sie, ze aplikacje na inne (w szczegolnosci: zblizone tematycznie) programy moga przebiegac podobnie, jednoczesnie nigdy nie zrobilam porownania. Przykladowa roznica o ktorej wiem, ze istnieje: na programy PhD na kierunku Biostatystyka aplikuje sie ogolnie na program i po 0-2 latach okresla lub zaweza obszar badawczy i jednoczesnie decyduje na konkretnych opiekunow naukowych (w tym: promotora/promotorow); typowe dla niektorych innych kierunkow STEM jest z kolei aplikowanie do konkretnego laboratorium (czesto: pod opieke naukowa konkretnego profesora).\nPonizszy opis jest napisany w oparciu o tylko i wylacznie moje doswiadczenia (za wyjatkiem miejsc, gdzie zaznaczam explicite, ze wiem cos ze slyszenia od innych studentow).\nPonizszy opis jest napisany w kontekscie aplikacji na programy rozpoczynajace sie jesienia 2017, wypelnianymi zgodnie z wymaganiami i terminami, ktorych pilnowalam aplikujac w 2016. Nie sprawdzilam, czy i co zmienilo sie od tego czasu.\nWyodrebniam nastepujace glowne komponenty procesu aplikacji:\nWstepny wybor uczelni i zbudowanie planu czasowego procesu aplikacji Przygotowanie do egzaminow: GRE + TOEFL Przygotowanie Personal Statement Zorganizowanie listow rekomendacyjnych Networking Aplikowanie: wykonanie i nadzorowanie post-procesu 1. Wstepny wybor uczelni i zbudowanie planu czasowego timeline aplikacji Generalne ramy czasowe.\nUczelnie roznia sie datami ostatecznego terminu aplikacji, jednoczesnie daty te zjezdzaja sie na przestrzeni ok. 1,5 miesiaca: od 1. grudnia roku N do polowy stycznia roku (N+1) na programy rozpoczynajace sie jesienia roku (N+1). 60% terminow to wlasnie 1. grudnia. üì£ Stad prace nad aplikacjami zaczynamy najpozniej 12-14 miesiecy przed faktycznym rozpoczeciem studiow. Wstepny wybor uczelni.\nWybieralam uczelnie na podstawie: a) rankingow programu Biostatystyka (podzbior ‚ÄúBiostatystyka‚Äù z usnews.com; w 2016 korzystalam glownie z tego dokumentu); b) rozmow z profesorami i studentami w USA w czasie pracy jako Research Assistant na Indiana University w pierwszej polowie 2016; c) przegladania grup naukowych i zainteresowan profesorow na stronach uczelni; d) lokalizacji; tu: zarowno ogolny obszar USA jak i konkretne miasto (przyklad: nie chcialam studiowac w NYC).\nBylo mi ciezko ocenic a priori, w jaki sposob moja aplikacja bedzie ewaluowana przez komitety rekrutacyjne na uczelniach w USA.\nAplikacja studenta z Polski na Biostatystyke u USA to wciaz egzotyczna sprawa. Komisje rekrutacyjne sa najczesniej niezaznajomione z polskim systemem szkolnictwa wyzszego (gdzie sa np. z chinskimi i hinduskimi), stad ocena ‚Äú5.0‚Äù z Analizy matematycznej I na Politechnice Wroclawskiej nie mowi im bardzo duzo na temat tego, co w zwiazku z tym powinnam umiec. (Przyklad: w czasie etapu rekrutacji ‚Äúon site‚Äù na jednej z uczelni Ivy League, dziekan departamentu Biostatystyki, z ktorym mialam 20-minutowe spotkanie, poprosil mnie o chocby ogolny opis tego ‚Äújak wygladaly moje studia w Polsce‚Äù üëΩ). Aplikowalam do 16 uczelni (relatywnie wielu), wg nastepujacego schematu: 1/3 uczelni na liscie to ‚Äúdream schools‚Äù - uczelnie z czolowki rankingow programu Biostatystyka; 1/3 to uczelnie ‚Äúsafe‚Äù - spodziewam sie, ze mam spore szanse sie tam dostac; 1/3 to uczelnie ‚Äúpo srodku‚Äù.\nDostalam sie / dostalam zaproszenie do 2. etapu rekrutacji od nieco ponad 50% z uczelni, do ktorych aplikowalam (2. etap to zaproszenie na rozmowe ‚Äúon site‚Äù lub jego odpowiednik dla studentow przebywajacych poza USA: rozmowa telefoniczna z czlonkami komitetu rekrutacyjnego; w kilku przypadkach nie kontynuowalam procesu do 2. etapu wiedzac, ze dostalam sie juz do miejsca wyzej na liscie moich preferencji).\nCo interesujace, uczelnie, ktore byly mna zainteresowanie, to niemal dokladnie: polowa uczelni z grupy ‚Äúdream schools‚Äù + polowa z ‚Äúsafe‚Äù + polowa z ‚Äúpo srodku‚Äù üëèüëç\nPlan czasowy procesu aplikacji powinien zawierac:\nDaty ostatecznego terminu aplikacji (‚Äúdeadline‚Äù) do wybranych uczelni, tzn. daty, po ktorych zamykaja sie systemy online umozliwiajace wykonanie aplikacji.\nListe wymaganych egzaminow i najdalszy termin ich realizacji (np. w Polsce), ktory zapewnia bezpieczne okno czasowe na przeslanie wynikow do uczelni. Najczesciej beda to dwa egzaminy: TOEFL i GRE General.\nWyniki TOEFL i GRE General wysylane sa automatycznie do uczelni, ktorych liste ‚Ä¶","date":1535930243,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535930243,"objectID":"72f641a34cee5120d548eacbfe44450e","permalink":"https://example.com/post/2018-09-02-applying-for-phd-in-usa/","publishdate":"2018-09-02T19:17:23-04:00","relpermalink":"/post/2018-09-02-applying-for-phd-in-usa/","section":"post","summary":"W 2015 roku zdobylam tytul Magister na kierunku Matematyka na Politechnice Wroclawskiej. W sierpniu 2016 rozpoczelam aplikacje na programy doktoranckie na kierunku Biostatystyka w Stanach Zjednoczonych rozpoczynajace sie jesienia 2017. Dostalam i zaakeptowalam oferte z Departamentu Biostatystyki na Johns Hopkins Bloomberg School of Public Health w Baltimore (stan Maryland), oferujacego jeden z 3 najlepszych (ex aequo) programow na kierunku Biostatystyka w USA wg rankingu usnews.","tags":[],"title":"[in Polish] Aplikacja na studia doktoranckie (Biostatystyka) w USA: notatki nt. procesu rekrutacji","type":"post"},{"authors":[],"categories":[],"content":"We propose to estimate association between the brain structure features and a scalar outcome in a regression model while utilizing additional information about structural connectivity between the brain regions.\nSpecifically, we propose a novel regularization method ‚Äì riPEER (ridgified Partially Empirical Eigenvectors for Regression) ‚Äì that defines a regularization penalty term based on the structural connectivity-derived Laplacian matrix.\nTable of Contents Scientific problem Challenges Proposed solution Published work Software Images used in the post ‚Äì credit/references Scientific problem The motivation for the work was to quantify the association between alcohol abuse phenotypes (outcome) and cortical thickness of the brain (covariates) in a study sample of young social-to-heavy drinking males. The data included measurements of average cortical thickness estimated for 68 brain regions.\nThis image (see images credit below) visualizes process of obtaining cortical thickness measurements from structural MRI images.\nChallenges Commonly shared issues in such settings are:\nhigh dimensionality of the data - we typically parcel the brain into tens, or hundreds of units from which we take measurements, and each unit may then correspond to a covariate in the data set,\ncorrelation of the covariates - measurements from spatially neighbouring or otherwise connected brain regions are likely to be correlated,\nsmall sample size - brain imaging studies often recruit a few tens of participants only.\nProposed solution We propose penalized regression method riPEER to estimate a linear model: $$y = Zb + X\\beta + \\varepsilon$$ where:\n$y$ - response (here: alcohol abuse phenotypes), $Z$ - input data matrix (here: cortical thickness measurements), $X$ - input data matrix (here: demographics data), $\\beta$ - regression coefficients, not penalized in estimation process $b$ - regression coefficients, penalized in estimation process and for whom there is a prior graph of similarity / graph of connections. available. The riPEER estimation method uses a penalty being a linear combination of a graph-based and ridge penalty terms: $$ \\hat{\\beta}, \\hat{b} = \\underset{\\beta,b}{\\text{arg min}} \\left[ (y - X\\beta - Zb)^T(y - X\\beta - Zb) + \\lambda_Qb^TQb + \\lambda_Rb^Tb \\right ] $$\nwhere:\n$Q$ - a graph-originated penalty matrix; typically: a graph Laplacian matrix (here: a graph Laplacian derived from structural connectivity of brain regions), $\\lambda_Q$ - regularization parameter for a graph-based penalty term, $\\lambda_R$ - regularization parameter for ridge penalty term. riPEER penalty term In the riPEER penalty term $(\\lambda_Qb^TQb + \\lambda_Rb^Tb)$,\nA graph-originated penalty matrix $Q$ allows imposing similarity between coefficients of variables which are connected.\nA ridge penalty term, $\\lambda_Rb^Tb$, allows for L2 regularization component; in addition, even with very small $\\lambda_R$, eliminates computational issues arising from singularity of $Q$.\nRegularization parameters $\\lambda_R$, $\\lambda_Q$ are estimated automatically as ML estimators of equivalent Linear Mixed Models optimization problem.\nPublished work We published the proposed riPEER method in work Brain connectivity-informed regularization methods for regression (Karas, M., Brzyski, D., Dzemidzic, M., Goni, J., Kareken, D.A., Randolph, T., Harezlak J. (2017)).\nWe published the riPEER extension to generalized linear regression, addressing both theoretical and computational issues, in work Connectivity‚Äêinformed adaptive regularization for generalized outcomes (Brzyski, D., Karas, M., Ances, B.M., Dzemidzic, M., Goni, J., Randolph, T., Harezlak J. (2021)).\nSoftware We provided open-source implementation of the proposed riPEER estimation method in R package mdpeer (CRAN index). The package provides functions for graph-constrained regression methods in which regularization parameters are selected automatically via estimation of equivalent Linear Mixed Model formulation.\nThe R package is accompanied by Intro and usage examples vignette.\nImages used in the post ‚Äì credit/references Featured image - top left component. Cortical thickness. Resources of Neurorecovery Laboratory at MGH/MIT/HMS Athinoula A. Martinos Center for Biomedical Imaging. Accessed at: link (last accessed on Nov 20, 2020).\nFeatured image - top middle component. Diffusion MRI Tractography in the brain white matter. Xavier Gigandet et. al. - Gigandet X, Hagmann P, Kurant M, Cammoun L, Meuli R, et al. (2008) Estimating the Confidence Level of White Matter Connections Obtained with MRI Tractography. PLoS ONE 3(12): e4006. doi:10.1371/journal.pone.0004006. Accessed at: link (last accessed on Nov 20, 2020).\nFeatured image - top right component. Databases of Statistical Information. Resources of Berkeley Advanced Media Institute Graduate School of Journalism. Accessed at: link (last accessed on Nov 20, 2020).\n","date":1512575693,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512575693,"objectID":"3e440c3636350437baecd6d5bdb5eb0c","permalink":"https://example.com/post/project_mdpeer/","publishdate":"2017-12-06T11:54:53-04:00","relpermalink":"/post/project_mdpeer/","section":"post","summary":"We propose riPEER regularization method to estimate association between the brain structure features and a scalar outcome in a regression model while utilizing additional information about structural connectivity between the brain regions.","tags":["brain imaging","statistical methods","regularization","regression"],"title":"Brain connectivity-informed regularization methods for regression","type":"post"},{"authors":null,"categories":null,"content":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"e8f8d235e8e7f2efd912bfe865363fc3","permalink":"https://example.com/project/example/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/example/","section":"project","summary":"An example of using the in-built project page.","tags":["Deep Learning"],"title":"Example Project","type":"project"},{"authors":null,"categories":null,"content":"","date":1461715200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1461715200,"objectID":"d1311ddf745551c9e117aa4bb7e28516","permalink":"https://example.com/project/external-project/","publishdate":"2016-04-27T00:00:00Z","relpermalink":"/project/external-project/","section":"project","summary":"An example of linking directly to an external project website using `external_link`.","tags":["Demo"],"title":"External Project","type":"project"},{"authors":["Marta Karas","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software. Create your slides in Markdown - click the Slides button to check out the example. Add the publication‚Äôs full text or supplementary notes here. You can use rich formatting such as including code, math, and images.\n","date":1441065600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1441065600,"objectID":"966884cc0d8ac9e31fab966c4534e973","permalink":"https://example.com/publication/journal-article/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/journal-article/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":["Source Themes"],"title":"An example journal article","type":"publication"},{"authors":["Marta Karas","Robert Ford"],"categories":null,"content":" Click the Cite button above to demo the feature to enable visitors to import publication metadata into their reference management software. Create your slides in Markdown - click the Slides button to check out the example. Add the publication‚Äôs full text or supplementary notes here. You can use rich formatting such as including code, math, and images.\n","date":1372636800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1372636800,"objectID":"69425fb10d4db090cfbd46854715582c","permalink":"https://example.com/publication/conference-paper/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/publication/conference-paper/","section":"publication","summary":"Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":[],"title":"An example conference paper","type":"publication"}]