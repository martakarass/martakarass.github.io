[{"authors":null,"categories":null,"content":"I am a postdoctoral researcher at the Onnela Lab in the Department of Biostatistics at Harvard T.H. Chan School of Public Health. I am working on methods for digital phenotyping and its applications with Beiwe platform.\nMy statistical methods interests include: power estimation in complex settings, methods for processing, features extraction and analysis of accelerometry data, causal inference, machine learning, R software development.\nI received PhD in Biostatistics from Johns Hopkins Bloomberg School of Public Health in 2022. I received my Bachelor’s and Master’s degrees in Mathematics from Wroclaw University of Science and Technology in Poland in 2013 and 2015. Prior to joining Johns Hopkins, I worked as a research assistant at Indiana University and as an analyst for Opera Software Internet browser company.\nIn my free time, I enjoy connecting with people and running. I completed my first full marathon in 2017 in Denver, CO.\n  Download my CV.\nRecently   For my first post-grad role, I accepted a postdoctoral researcher position at the Onnela Lab at Harvard University. I will be working at methods for digital phenotyping and its applications.\n  I defended my PhD thesis \u0026ldquo;Statistical Methods for Wearable Device Data and Sample Size Calculation in Complex Models\u0026rdquo;! My defense slides, with video recording linked, are available. I published the Acknowledgements and Dedication parts of the thesis.\n  I received Travel Reimbursement Award to 3rd Annual Health Data Science Symposium at Harvard to present our work on harmonization of open-source and proprietary accelerometry-based physical activity measures in Nov 2021. \n  Our preprint \u0026ldquo;Upstrap for estimating power and sample size in complex models\u0026rdquo; is available on bioRxiv. We evaluate power estimation properties of the upstrap and provide a series of \u0026ldquo;read, adapt and use\u0026rdquo; R code examples for simple and complex settings, including GLMM. See the relevant project page.\n  I received Student Poster Sponsorship and will present our work on harmonization of open-source and proprietary accelerometry-based physical activity measures during the ActiGraph Digital Data Summit in Nov 2021. \n  See all past updates.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"/author/marta-karas/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/marta-karas/","section":"authors","summary":"I am a postdoctoral researcher at the Onnela Lab in the Department of Biostatistics at Harvard T.H. Chan School of Public Health. I am working on methods for digital phenotyping and its applications with Beiwe platform.","tags":null,"title":"Marta Karas","type":"authors"},{"authors":null,"categories":null,"content":"Flexibility This feature can be used for publishing content such as:\n Online courses Project or software documentation Tutorials  The courses folder may be renamed. For example, we can rename it to docs for software/project documentation or tutorials for creating an online course.\nDelete tutorials To remove these pages, delete the courses folder and see below to delete the associated menu link.\nUpdate site menu After renaming or deleting the courses folder, you may wish to update any [[main]] menu links to it by editing your menu configuration at config/_default/menus.toml.\nFor example, if you delete this folder, you can remove the following from your menu configuration:\n[[main]] name = \u0026quot;Courses\u0026quot; url = \u0026quot;courses/\u0026quot; weight = 50  Or, if you are creating a software documentation site, you can rename the courses folder to docs and update the associated Courses menu configuration to:\n[[main]] name = \u0026quot;Docs\u0026quot; url = \u0026quot;docs/\u0026quot; weight = 50  Update the docs menu If you use the docs layout, note that the name of the menu in the front matter should be in the form [menu.X] where X is the folder name. Hence, if you rename the courses/example/ folder, you should also rename the menu definitions in the front matter of files within courses/example/ from [menu.example] to [menu.\u0026lt;NewFolderName\u0026gt;].\n","date":1536451200,"expirydate":-62135596800,"kind":"section","lang":"en","lastmod":1536451200,"objectID":"59c3ce8e202293146a8a934d37a4070b","permalink":"/courses/example/","publishdate":"2018-09-09T00:00:00Z","relpermalink":"/courses/example/","section":"courses","summary":"Learn how to use Academic's docs layout for publishing online courses, software documentation, and tutorials.","tags":null,"title":"Overview","type":"docs"},{"authors":null,"categories":null,"content":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 2 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"74533bae41439377bd30f645c4677a27","permalink":"/courses/example/example1/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example1/","section":"courses","summary":"In this tutorial, I\u0026rsquo;ll share my top 10 tips for getting started with Academic:\nTip 1 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 1","type":"docs"},{"authors":null,"categories":null,"content":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\nTip 4 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum. Sed ac faucibus dolor, scelerisque sollicitudin nisi. Cras purus urna, suscipit quis sapien eu, pulvinar tempor diam. Quisque risus orci, mollis id ante sit amet, gravida egestas nisl. Sed ac tempus magna. Proin in dui enim. Donec condimentum, sem id dapibus fringilla, tellus enim condimentum arcu, nec volutpat est felis vel metus. Vestibulum sit amet erat at nulla eleifend gravida.\nNullam vel molestie justo. Curabitur vitae efficitur leo. In hac habitasse platea dictumst. Sed pulvinar mauris dui, eget varius purus congue ac. Nulla euismod, lorem vel elementum dapibus, nunc justo porta mi, sed tempus est est vel tellus. Nam et enim eleifend, laoreet sem sit amet, elementum sem. Morbi ut leo congue, maximus velit ut, finibus arcu. In et libero cursus, rutrum risus non, molestie leo. Nullam congue quam et volutpat malesuada. Sed risus tortor, pulvinar et dictum nec, sodales non mi. Phasellus lacinia commodo laoreet. Nam mollis, erat in feugiat consectetur, purus eros egestas tellus, in auctor urna odio at nibh. Mauris imperdiet nisi ac magna convallis, at rhoncus ligula cursus.\nCras aliquam rhoncus ipsum, in hendrerit nunc mattis vitae. Duis vitae efficitur metus, ac tempus leo. Cras nec fringilla lacus. Quisque sit amet risus at ipsum pharetra commodo. Sed aliquam mauris at consequat eleifend. Praesent porta, augue sed viverra bibendum, neque ante euismod ante, in vehicula justo lorem ac eros. Suspendisse augue libero, venenatis eget tincidunt ut, malesuada at lorem. Donec vitae bibendum arcu. Aenean maximus nulla non pretium iaculis. Quisque imperdiet, nulla in pulvinar aliquet, velit quam ultrices quam, sit amet fringilla leo sem vel nunc. Mauris in lacinia lacus.\nSuspendisse a tincidunt lacus. Curabitur at urna sagittis, dictum ante sit amet, euismod magna. Sed rutrum massa id tortor commodo, vitae elementum turpis tempus. Lorem ipsum dolor sit amet, consectetur adipiscing elit. Aenean purus turpis, venenatis a ullamcorper nec, tincidunt et massa. Integer posuere quam rutrum arcu vehicula imperdiet. Mauris ullamcorper quam vitae purus congue, quis euismod magna eleifend. Vestibulum semper vel augue eget tincidunt. Fusce eget justo sodales, dapibus odio eu, ultrices lorem. Duis condimentum lorem id eros commodo, in facilisis mauris scelerisque. Morbi sed auctor leo. Nullam volutpat a lacus quis pharetra. Nulla congue rutrum magna a ornare.\nAliquam in turpis accumsan, malesuada nibh ut, hendrerit justo. Cum sociis natoque penatibus et magnis dis parturient montes, nascetur ridiculus mus. Quisque sed erat nec justo posuere suscipit. Donec ut efficitur arcu, in malesuada neque. Nunc dignissim nisl massa, id vulputate nunc pretium nec. Quisque eget urna in risus suscipit ultricies. Pellentesque odio odio, tincidunt in eleifend sed, posuere a diam. Nam gravida nisl convallis semper elementum. Morbi vitae felis faucibus, vulputate orci placerat, aliquet nisi. Aliquam erat volutpat. Maecenas sagittis pulvinar purus, sed porta quam laoreet at.\n","date":1557010800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557010800,"objectID":"1c2b5a11257c768c90d5050637d77d6a","permalink":"/courses/example/example2/","publishdate":"2019-05-05T00:00:00+01:00","relpermalink":"/courses/example/example2/","section":"courses","summary":"Here are some more tips for getting started with Academic:\nTip 3 Lorem ipsum dolor sit amet, consectetur adipiscing elit. Duis posuere tellus ac convallis placerat. Proin tincidunt magna sed ex sollicitudin condimentum.","tags":null,"title":"Example Page 2","type":"docs"},{"authors":null,"categories":[],"content":"In this post, we:\n use a GPX file with with geographic information, exported from Strava from one running activity, parse and plot the data.  Table of Contents  GPX data from Strava Parsing GPX Computing distance, time elapsed and speed Plot elevation, speed Plot run path Acknowledgements   GPX data from Strava GPX stands for “GPS Exchange Format”. It is an XML schema commonly used for storing GPS data.\nStrava (Strava, Inc; San Francisco, CA) is a popular activity tracker app I have been using for a few weeks. Strava allows to export GPS data collected during a recorded activity in a GPX format. To export the data, go to Strava activity page \u0026gt; “three dots” button \u0026gt; Export GPX.\nI downloaded GPX data from a run I did on Jan 1, 2022. The run distance is 10.88 km and spans 1:03:49 time. The data is available on my GitHub and can be downloaded using the code below.\n  (Click to see the code to download the data.)  url \u0026lt;- paste0( \u0026quot;https://raw.githubusercontent.com/martakarass/gps-stats/main/data/\u0026quot;, \u0026quot;/Morning_Run_2022-01-01.gpx\u0026quot;) fpath \u0026lt;- paste0( \u0026quot;/Users/martakaras/Downloads\u0026quot;, \u0026quot;/Morning_Run_2022-01-01.gpx\u0026quot;) # download result \u0026lt;- curl::curl_download(url, destfile = fpath, quiet = FALSE)    Parsing GPX First, we parse the GPX file and put the extracted data trajectories into a data frame:\n timestamp, latitude, longitude, elevation.    (Click to see the code.)  # rm(list = ls()) library(tidyverse) library(here) library(XML) library(lubridate) library(ggmap) library(geosphere) options(digits.secs = 3) options(scipen = 999) # parse GPX file path_tmp \u0026lt;- paste0(\u0026quot;/Users/martakaras/Downloads/Morning_Run_2022-01-01.gpx\u0026quot;) parsed \u0026lt;- htmlTreeParse(file = path_tmp, useInternalNodes = TRUE) # get values via via the respective xpath coords \u0026lt;- xpathSApply(parsed, path = \u0026quot;//trkpt\u0026quot;, xmlAttrs) elev \u0026lt;- xpathSApply(parsed, path = \u0026quot;//trkpt/ele\u0026quot;, xmlValue) ts_chr \u0026lt;- xpathSApply(parsed, path = \u0026quot;//trkpt/time\u0026quot;, xmlValue) # combine into df dat_df \u0026lt;- data.frame( ts_POSIXct = ymd_hms(ts_chr, tz = \u0026quot;EST\u0026quot;), lat = as.numeric(coords[\u0026quot;lat\u0026quot;,]), lon = as.numeric(coords[\u0026quot;lon\u0026quot;,]), elev = as.numeric(elev) ) head(dat_df)    ts_POSIXct lat lon elev 1 2022-01-01 09:42:01 42.32791 -71.10868 23.0 2 2022-01-01 09:42:06 42.32791 -71.10868 23.0 3 2022-01-01 09:42:08 42.32795 -71.10866 23.2 4 2022-01-01 09:42:10 42.32817 -71.10872 24.7 5 2022-01-01 09:42:11 42.32816 -71.10875 24.7 6 2022-01-01 09:42:12 42.32814 -71.10875 23.8  Computing distance, time elapsed and speed Next, we compute:\n distance (in meters) between subsequent GPS recordings time elapsed (in seconds) between subsequent GPS recordings, speed (metres per seconds, kilometres per hour) – temporal, based on subsequent GPS recordings.    (Click to see the code.)  # compute distance (in meters) between subsequent GPS points dat_df \u0026lt;- dat_df %\u0026gt;% mutate(lat_lead = lead(lat)) %\u0026gt;% mutate(lon_lead = lead(lon)) %\u0026gt;% rowwise() %\u0026gt;% mutate(dist_to_lead_m = distm(c(lon, lat), c(lon_lead, lat_lead), fun = distHaversine)[1,1]) %\u0026gt;% ungroup() # compute time elapsed (in seconds) between subsequent GPS points dat_df \u0026lt;- dat_df %\u0026gt;% mutate(ts_POSIXct_lead = lead(ts_POSIXct)) %\u0026gt;% mutate(ts_diff_s = as.numeric(difftime(ts_POSIXct_lead, ts_POSIXct, units = \u0026quot;secs\u0026quot;))) # compute metres per seconds, kilometres per hour dat_df \u0026lt;- dat_df %\u0026gt;% mutate(speed_m_per_sec = dist_to_lead_m / ts_diff_s) %\u0026gt;% mutate(speed_km_per_h = speed_m_per_sec * 3.6) # remove some columns we won't use anymore dat_df \u0026lt;- dat_df %\u0026gt;% select(-c(lat_lead, lon_lead, ts_POSIXct_lead, ts_diff_s)) head(dat_df) %\u0026gt;% as.data.frame()    ts_POSIXct lat lon elev dist_to_lead_m ts_diff_s speed_m_per_sec speed_km_per_h 1 2022-01-01 09:42:01 42.32791 -71.10868 23.0 0.000000 5 0.000000 0.000000 2 2022-01-01 09:42:06 42.32791 -71.10868 23.0 4.406590 2 2.203295 7.931862 3 2022-01-01 09:42:08 42.32795 -71.10866 23.2 25.403927 2 12.701963 45.727068 4 2022-01-01 09:42:10 42.32817 -71.10872 24.7 2.510645 1 2.510645 9.038324 5 2022-01-01 09:42:11 42.32816 -71.10875 24.7 2.264098 1 2.264098 8.150751 6 2022-01-01 09:42:12 42.32814 -71.10875 23.8 1.899407 1 1.899407 6.837864  Plot elevation, speed Plot elevation\n  (Click to see the code.)  plt_elev \u0026lt;- ggplot(dat_df, aes(x = ts_POSIXct, y = elev)) + geom_line() + labs(x = \u0026quot;Time\u0026quot;, y = \u0026quot;Elevation [m]\u0026quot;) + theme_grey(base_size = 14) plt_elev   Plot speed\n  (Click to see the code.)  plt_speed_km_per_h \u0026lt;- ggplot(dat_df, aes(x = ts_POSIXct, y = speed_km_per_h)) + geom_line() + labs(x = \u0026quot;Time\u0026quot;, y = \u0026quot;Speed [km/h]\u0026quot;) + theme_grey(base_size = 14) plt_speed_km_per_h   The above plot is very wiggly due to small time increment over which the speed statistic was computed. It could be made smoother by first aggregating distance covered and time elapsed over a fixed time interval longer than GPS recordings interval (e.g. 10 seconds), or by using data smoothing (e.g. LOWESS).\nThe dips in the plot are, to my judgement, correct representations of the times I briefly stopped during the run for various reasons.\nPlot run path A simple, graphics-based version of the trajectory plot:\n  (Click to see the code.)  plot(x = dat_df$lon, y = dat_df$lat, type = \u0026quot;l\u0026quot;, col = \u0026quot;blue\u0026quot;, lwd = 3, xlab = \u0026quot;Longitude\u0026quot;, ylab = \u0026quot;Latitude\u0026quot;)   A more fancy plot can be generated with ggmap package. I used labels to mark each kilometer passed. My Google API key is registered hence I could also access the Google map for use with ggmap (code for API key registration not showed.)\n  (Click to see the code.)  # get the map background bbox \u0026lt;- make_bbox(range(dat_df$lon), range(dat_df$lat)) dat_df_map \u0026lt;- get_googlemap(center = c(mean(range(dat_df$lon)), mean(range(dat_df$lat))), zoom = 15) # no Google token alternative: # dat_df_map \u0026lt;- get_map(bbox, maptype = \u0026quot;toner-lite\u0026quot;, source = \u0026quot;stamen\u0026quot;) # data frame to add distance marks dat_df_dist_marks \u0026lt;- dat_df %\u0026gt;% mutate(dist_m_cumsum = cumsum(dist_to_lead_m)) %\u0026gt;% mutate(dist_m_cumsum_km_floor = floor(dist_m_cumsum / 1000)) %\u0026gt;% group_by(dist_m_cumsum_km_floor) %\u0026gt;% filter(row_number() == 1, dist_m_cumsum_km_floor \u0026gt; 0) # generate plot plt_path_fancy \u0026lt;- ggmap(dat_df_map) + geom_point(data = dat_df, aes(lon, lat, col = elev), size = 1, alpha = 0.5) + geom_path(data = dat_df, aes(lon, lat), size = 0.3) + geom_label(data = dat_df_dist_marks, aes(lon, lat, label = dist_m_cumsum_km_floor), size = 3) + labs(x = \u0026quot;Longitude\u0026quot;, y = \u0026quot;Latitude\u0026quot;, color = \u0026quot;Elev. [m]\u0026quot;, title = \u0026quot;Track of one Marta's run on 2022-01-01\u0026quot;) plt_path_fancy   Acknowledgements The above content is inspired by AND borrows some code from Plotting GPS tracks with R by Ivan Lizarazo, who in turn based their content on Stay on track: Plotting GPS tracks with R by Sascha Wolfer. My main contributions are:\n identifying (TTBOMK) an error in their shared code for computing of subsequent locations distance, and using an alternative way, parsing GPX timestamp with lubridate function, providing my run data as open source GPS data set. ","date":1641417568,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1641417568,"objectID":"b05a3b4f7f5ea01f9c962401798241d0","permalink":"/post/2022-01-05-gps_strava_read_and_viz/","publishdate":"2022-01-05T17:19:28-04:00","relpermalink":"/post/2022-01-05-gps_strava_read_and_viz/","section":"post","summary":"In this post, we:\n use a GPX file with with geographic information, exported from Strava from one running activity, parse and plot the data. ","tags":[],"title":"GPS data in R: parse and plot GPX data exported from Strava","type":"post"},{"authors":[],"categories":null,"content":"","date":1636106700,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1636106700,"objectID":"565ff3b417203a1da3362400e6fa3614","permalink":"/talk/2021-11-05-adds/","publishdate":"2021-09-04T09:00:00Z","relpermalink":"/talk/2021-11-05-adds/","section":"talk","summary":"","tags":[],"title":"Harmonization of open-source and proprietary accelerometry-based physical activity measures","type":"talk"},{"authors":["Karas","M.","Crainiceanu","C.M."],"categories":[],"content":"","date":1630534118,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630534118,"objectID":"d713eb839f002d67fd3fad13795f9d2e","permalink":"/publication/upstrap_for_estimating_power/","publishdate":"2021-09-01T18:08:38-04:00","relpermalink":"/publication/upstrap_for_estimating_power/","section":"publication","summary":"Power and sample size calculation are major components of statistical analyses. The upstrap resampling method introduced by Crainiceanu and Crainiceanu (2018) was proposed as a general solution to this problem but has not been assessed in numerical experiments. We evaluate the power and sample size estimation properties of the upstrap for data sets that are larger or smaller than the original data set. We also expand the scope of upstrap and propose a solution to estimate the power to detect: (1) an effect size observed in the data; and (2) an effect size chosen by a researcher. Simulations include the following scenarios: (a) one- and two-sample t-tests; (b) linear regression with both Gaussian and binary outcomes; (c) multilevel mixed effects models with both Gaussian and binary outcomes. We illustrate the approach using a reanalysis of a cluster randomized trial of malaria transmission. The accompanying software and data are publicly available.","tags":["bootstrap","upstrap","power estimation","sample size calculation","sampling"],"title":"Upstrap for estimating power and sample size in complex models","type":"publication"},{"authors":[],"categories":[],"content":"Power and sample size calculation are major components of statistical analyses. The upstrap resampling method was proposed as a general solution to this problem.\nWe evaluate power estimation properties of the upstrap and provide a series of \u0026ldquo;read, adapt and use\u0026rdquo; R code examples for power estimation in simple and complex settings (bioRxiv preprint).\n**See images citation and/or credit information** [below](#custom). -- Table of Contents    Scientific problem Challenges Proposed solution Code example: testing for significance of LM coefficient Contributions     Scientific problem We consider the following problem: given an observed data sample $\\mathbf{x}$ of sample size $N$, given specific null and alternative hypothesis, and a test statistic, assuming significance level $\\alpha$, estimate sample size $M$ required to achieve power $1 -\\beta$ (i.e., to achieve probability $1 -\\beta$ of rejecting the null hypothesis when the null is true).\nHere, we consider the tasks to estimate the power to detect:\n an effect size observed in the data; an effect size chosen by a researcher.  We aim to ddress complex settings, including testing significance of model coefficients in: LM, GLM, LMM, GLMM, GEE, and others.\nChallenges For multilevel data settings, the existing approaches tend to fall into two categories.\n  First are theoretical results for estimating power in specific multilevel data setups; these often use assumptions about the intra-class correlation coefficient, and/or assume a particular study design (e.g., that the data are balanced). \n  Another class is based on simulations. Such may require specifying a population model for the data, simulating data from the assumed model, and estimating the power via Monte Carlo simulations.\n  The former arguably lacks flexibility; in addition, finding and then determining applicability of a theoretical result may be difficult. The latter is flexible but involves potentially complex programming task.\nProposed solution Upstrap The upstrap resampling method (Crainiceanu, C.M., Crainiceanu, A. (2020)) was proposed as a general solution to this problem.\nUpstrap starts with a sample (observed data) and resamples with replacement either fewer or more samples than in the original data set. The below example shows difference between bootstrap and upstrap resample.\n# simulate data x \u0026lt;- rnorm(n = 10) # bootstrap resample x_b \u0026lt;- sample(x, size = 10, replace = TRUE) # upstrap resample (case: more samples than in original x) x_u \u0026lt;- sample(x, size = 20, replace = TRUE)  Upstrap for estimating power Given observed data sample $\\mathbf{x}$, to estimate power for a target sample size, we propose:\n  Generate $B$ resamples of target sample size by sampling with replacement from $\\mathbf{x}$.\n  Perform hypothesis test on each resample.\n  Estimate power as the proportion of $B$ resamples where the null hypothesis was rejected.\n  The above procedure estimates power corresponding to the effect size observed in the sample $\\mathbf{x}$. For example, for one-sample t-test, the above procedure estimates:\npower.t.test(delta = mean(x), sd = sd(x), ...)$power  Upstrap for estimating power: specific effect size In practice, one is often is interested in estimating power for a specific effect size. For example, for one-sample t-test, a specific effect size could be set to 0.3:\npower.t.test(delta = 0.3, sd = sd(x), ...)$power  To address such cases, we propose to update response variable values in the observed sample $\\mathbf{x}$ to ensure the effect size in this updated data is our target effect size. Details are provided in the preprint.\nCode example: testing for significance of LM coefficient Below, we demonstrate the upstrap power estimation method for testing for significance of LM coefficient. In the preprint, we provide more R code examples, including testing for significance of coefficient in LM, GLM, LMM, GLMM.\nWe define simulation parameters and simulate a sample of size $N = 50$.\n  (Click to see setup definition and R code.)  Setup Consider a random sample with $N = 50$ independent observations (e.g., 50 subjects, 1 observation per subject). Assume a continuous response variable $Y$ and two covariates: dichotomous $X_1$, continuous $X_2$. We are interested in estimating power of test for significance of the coefficient $\\beta_{1}$ in linear model $Y_{i}=\\beta_{0}+\\beta_{1} X_{1 i}+\\beta_{2} X_{2 i}+\\varepsilon_{i}$, where $i=1, \\ldots, N$ and $\\varepsilon_{i} \\sim_{\\text {iid }} N\\left(0, \\sigma^{2}\\right)$.\n# simulation parameters N \u0026lt;- 50 coef_x0 \u0026lt;- 0 coef_x1 \u0026lt;- 0.2 coef_x2 \u0026lt;- 0.1 sigma2 \u0026lt;- 1 # simulate sample set.seed(1) subjid_i \u0026lt;- 1 : N # subject ID unique in data set x1_i \u0026lt;- rbinom(n = N, size = 1, prob = 0.5) x2_i \u0026lt;- rbinom(n = N, size = 1, prob = 0.5) eps_i \u0026lt;- rnorm(N, sd = sqrt(sigma2)) y_i \u0026lt;- coef_x0 + (coef_x1 * x1_i) + (coef_x2 * x2_i) + eps_i dat \u0026lt;- data.frame(y = y_i, x1 = x1_i, x2 = x2_i, subjid = subjid_i)   str(dat) # 'data.frame':\t50 obs. of 4 variables: # $ y : num 0.398 -0.512 0.541 -0.929 1.433 ... # $ x1 : int 0 0 1 1 0 1 1 1 1 0 ... # $ x2 : int 0 1 0 0 0 0 0 1 1 0 ... # $ subjid: int 1 2 3 4 5 6 7 8 9 10 ...  We get the observed effect size.\nfit \u0026lt;- lm(y ~ x1 + x2, data = dat) coef(fit)[\u0026quot;x1\u0026quot;] # 0.5585879  We estimate the test power via the upstrap.\nCase: target sample size M = N = 50, target effect size as observed in the sample # number of upstrap resamples B_boot \u0026lt;- 1000  out \u0026lt;- rep(NA, B_boot) for (rr in 1 : B_boot){ dat_rr_idx \u0026lt;- sample(1 : nrow(dat), replace = TRUE) dat_rr \u0026lt;- dat[dat_rr_idx, ] fit_rr \u0026lt;- lm(y ~ x1 + x2, data = dat_rr) pval_rr \u0026lt;- summary(fit_rr)$coef[\u0026quot;x1\u0026quot;, 4] out[rr] \u0026lt;- (pval_rr \u0026lt; 0.05) } mean(out) # [1] 0.493  Case: target sample size M = 100, target effect size as observed in the sample out \u0026lt;- rep(NA, B_boot) for (rr in 1 : B_boot){ dat_rr_idx \u0026lt;- sample(1 : nrow(dat), size = 100, replace = TRUE) dat_rr \u0026lt;- dat[dat_rr_idx, ] fit_rr \u0026lt;- lm(y ~ x1 + x2, data = dat_rr) pval_rr \u0026lt;- summary(fit_rr)$coef[\u0026quot;x1\u0026quot;, 4] out[rr] \u0026lt;- (pval_rr \u0026lt; 0.05) } mean(out) # [1] 0.773  Case: target sample size M = 100, target effect size set to 0.8 # update the outcome in the sample to represent the target effect size dat_upd \u0026lt;- dat dat_upd$y \u0026lt;- dat_upd$y + (0.8 - coef(fit)[\u0026quot;x1\u0026quot;]) * dat_upd$x1 out \u0026lt;- rep(NA, B_boot) for (rr in 1 : B_boot){ dat_rr_idx \u0026lt;- sample(1 : nrow(dat_upd), size = 100, replace = TRUE) dat_rr \u0026lt;- dat_upd[dat_rr_idx, ] fit_rr \u0026lt;- lm(y ~ x1 + x2, data = dat_rr) pval_rr \u0026lt;- summary(fit_rr)$coef[\u0026quot;x1\u0026quot;, 4] out[rr] \u0026lt;- (pval_rr \u0026lt; 0.05) } mean(out) # [1] 0.92  Contributions  Our preprint: Karas, M., Crainiceanu, C.M. (2021) Upstrap for estimating power and sample size in complex models is available on bioRxiv.  ","date":1630511693,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1630511693,"objectID":"6af0bbd5b34bdc119e4b39d100fc35e3","permalink":"/project/project_upstrap/","publishdate":"2021-09-01T11:54:53-04:00","relpermalink":"/project/project_upstrap/","section":"project","summary":"We evaluate the power and sample size estimation properties of the upstrap resampling method.","tags":["bootstrap","upstrap","power estimation","sample size calculation","sampling"],"title":"Upstrap for estimating power and sample size in complex models","type":"project"},{"authors":[],"categories":null,"content":"","date":1627462800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1627462800,"objectID":"cfcfbbe91b1cd18c6ab347cd73197df2","permalink":"/talk/2021-07-28-onnela_lab_meeting/","publishdate":"2021-07-28T09:00:00Z","relpermalink":"/talk/2021-07-28-onnela_lab_meeting/","section":"talk","summary":"","tags":[],"title":"Statistical methods and modeling of person-generated health data from wearable and mobile devices","type":"talk"},{"authors":null,"categories":[],"content":"In this post, we:\n  use dataset “Labeled raw accelerometry data captured during walking, stair climbing and driving” that is freely available on PhysioNet;\n  derive four minute-level summary measures of physical activity – AC, MIMS, ENMO, MAD, AI – from raw accelerometry data using SummarizedActigraphy R package;\n  summarize minute-level summary measures across walking and driving activities.\n  Table of Contents  Dataset “Labeled raw accelerometry data” Computing minute-level summary measures of raw accelerometry data  Summary measures SummarizedActigraphy R package Preparing the data input Computing the measures   Minute-level physical activity measures during walking and driving activities  Merging physical activity measures with physical activity labels Visualizing physical activity measures across activity types   Acknowledgements Citation info   Dataset “Labeled raw accelerometry data” The dataset contains raw accelerometry data collected during outdoor walking, stair climbing, and driving for n=32 healthy adults. It is freely is available to download on PhysioNet at: https://doi.org/10.13026/51h0-a262\n The study was led by Dr. Jaroslaw Harezlak, assisted by Drs. William Fadel and Jacek Urbanek. Accelerometry data were collected simultaneously at four body locations: left wrist, left hip, left ankle, and right ankle, at a sampling frequency of 100 Hz. The 3-axial ActiGraph GT3X+ devices were used to collect the data. The data include labels of activity type performed for each time point of data collection (1=walking; 2=descending stairs; 3=ascending stairs; 4=driving; 77=clapping; 99=non-study activity).  We downloaded the data zip from PhysioNet and unpacked. This can be done manually from the website, or using download the data programmatically.\n  (Click to see the code to download the data.)  url \u0026lt;- paste0( \u0026quot;https://physionet.org/static/published-projects\u0026quot;, \u0026quot;/accelerometry-walk-climb-drive\u0026quot;, \u0026quot;/labeled-raw-accelerometry-data-captured-during-walking-stair-climbing-and-driving-1.0.0.zip\u0026quot;) destfile \u0026lt;- paste0( \u0026quot;/Users/martakaras/Downloads\u0026quot;, \u0026quot;/labeled-raw-accelerometry-data-captured-during-walking-stair-climbing-and-driving-1.0.0.zip\u0026quot;) # download zip result \u0026lt;- curl::curl_download(url, destfile = destfile, quiet = FALSE) # unzip zip unzip(zipfile = destfile, exdir = dirname(destfile))    First, let’s look at raw accelerometry data of a single subject.\n  (Click to see the code.)  library(tidyverse) library(lubridate) library(lme4) library(knitr) # remotes::install_github(\u0026quot;muschellij2/SummarizedActigraphy\u0026quot;) library(SummarizedActigraphy) library(MIMSunit) library(activityCounts) options(digits.secs = 3) options(scipen=999) # define path to raw data files directory raw_files_dir \u0026lt;- paste0( \u0026quot;/Users/martakaras/Downloads\u0026quot;, \u0026quot;/labeled-raw-accelerometry-data-captured-during-walking-stair-climbing-and-driving-1.0.0\u0026quot;, \u0026quot;/raw_accelerometry_data\u0026quot;) # single participant's raw accelerometry data dat_fpaths \u0026lt;- list.files(raw_files_dir, full.names = TRUE) dat_i \u0026lt;- read_csv(dat_fpaths[1])   dat_i # A tibble: 303,300 x 14 activity time_s lw_x lw_y lw_z lh_x lh_y lh_z la_x la_y la_z \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; 1 99 0.01 0.039 1.02 -0.02 -0.18 1.23 0.023 0.156 0.855 -0.582 2 99 0.02 -0.629 -0.461 0.973 -0.246 0.137 0.969 -0.707 0.559 0.449 3 99 0.03 -0.926 -1.26 0.691 0.238 -0.328 1.22 -1.44 1.37 0.367 4 99 0.04 -0.871 -1.50 -0.246 0.711 -0.484 0.414 -1.66 1.64 -0.543 5 99 0.05 -0.727 -1.62 -0.559 1.03 -0.297 0.145 -1.76 1.68 -0.918 6 99 0.06 -0.543 -1.66 -0.629 1.12 -0.246 0.137 -1.80 1.65 -0.988 7 99 0.07 -0.348 -1.64 -0.609 1.24 -0.426 0.047 -1.76 1.57 -0.992 8 99 0.08 -0.16 -1.60 -0.566 1.18 -0.539 -0.008 -1.63 1.53 -1.02 9 99 0.09 -0.012 -1.53 -0.523 1.03 -0.633 -0.043 -1.13 1.87 -0.738 10 99 0.1 0.117 -1.43 -0.484 0.922 -0.766 -0.047 0.285 1.37 -0.156 # … with 303,290 more rows, and 3 more variables: ra_x \u0026lt;dbl\u0026gt;, ra_y \u0026lt;dbl\u0026gt;, # ra_z \u0026lt;dbl\u0026gt;  Each file contains 14 variables:\n activity – type of activity (1=walking; 2=descending stairs; 3=ascending stairs; 4=driving; 77=clapping; 99=non-study activity); time_s – time from device initiation (seconds [s]); lw_x, lw_y, lw_z – acceleration [g] measured by a left wrist-worn sensor at axis x, y, z; lh_x, lh_y, lh_z – acceleration [g] measured by a left hip-worn sensor at axis x, y, z; la_x, la_y, la_z – acceleration [g] measured by a left ankle-worn sensor at axis x, y, z; ra_x, ra_y, ra_z – acceleration [g] measured by a right ankle-worn sensor at axis x, y, z.  An exemplary few seconds of raw data from of walking and driving activities, collected by sensors at four different locations:\n  (Click to see the code.)  loc_id_levels \u0026lt;- c(\u0026quot;lw\u0026quot;, \u0026quot;lh\u0026quot;, \u0026quot;la\u0026quot;, \u0026quot;ra\u0026quot;) loc_id_labels \u0026lt;- c(\u0026quot;left wrist\u0026quot;, \u0026quot;left hip\u0026quot;, \u0026quot;left ankle\u0026quot;, \u0026quot;right ankle\u0026quot;) activity_levels \u0026lt;- c(\u0026quot;walking\u0026quot;, \u0026quot;driving\u0026quot;) plt_df \u0026lt;- dat_i %\u0026gt;% filter(activity %in% c(1,4)) %\u0026gt;% group_by(activity) %\u0026gt;% mutate(time_s = time_s - min(time_s)) %\u0026gt;% ungroup() %\u0026gt;% filter(time_s \u0026gt;= (5 + 5), time_s \u0026lt; (5 + 10)) %\u0026gt;% mutate(activity = recode(activity, '1' = 'walking', '4' = 'driving')) %\u0026gt;% pivot_longer(cols = -c(activity, time_s)) %\u0026gt;% separate(name, c(\u0026quot;loc_id\u0026quot;, \u0026quot;axis_id\u0026quot;), sep = \u0026quot;_\u0026quot;) %\u0026gt;% mutate(activity = factor(activity, levels = activity_levels)) %\u0026gt;% mutate(loc_id = factor(loc_id, levels = loc_id_levels, labels = loc_id_labels)) plt_raw_data \u0026lt;- ggplot(plt_df, aes(x = time_s, y = value, color = axis_id)) + geom_line() + facet_grid(loc_id ~ activity) + scale_y_continuous(limits = c(-1, 1) * max(abs(plt_df$value))) + labs(y = \u0026quot;Acceleration [g]\u0026quot;, x = \u0026quot;Time since activity started [s]\u0026quot;, color = \u0026quot;Sensor axis: \u0026quot;) + theme_gray(base_size = 14) + theme(legend.position=\u0026quot;bottom\u0026quot;) + scale_color_manual(values = c(\u0026quot;red\u0026quot;, \u0026quot;darkgreen\u0026quot;, \u0026quot;blue\u0026quot;))   plt_raw_data  Computing minute-level summary measures of raw accelerometry data Summary measures A number of open-source measures have been proposed to aggregate subsecond-level accelerometry data. These include:\n Monitor Independent Movement Summary (MIMS) (John et al., 2019), Euclidean Norm Minus One (ENMO) (van Hees et al., 2013), Mean Amplitude Deviation (MAD) (Vähä-Ypyä et al., 2015), Activity Index (AI) (Bai et al., 2016).  In addition, attempts have been made to reverse-engineer proprietary activity counts (AC) generated by ActiLife software from data collected by ActiGraph accelerometers. These include:\n activityCounts R package – allows generating “ActiLife counts from raw acceleration data for different accelerometer brands and it is developed based on the study done by Brond et al., 2017”.  SummarizedActigraphy R package We derive the measures using SummarizedActigraphy R package. The package was authored by John Muschelli and is available on GitHub (link).\nIt uses some of the existing R software under the hood to compute some part of the measures:\n MIMSunit R package to compute MIMS, activityCounts R package to compute AC.  It computes some other measures straightfoward from their definition, e.g.: MAD, AI.\nNote: the recommendations in the ENMO paper state data calibration preprocessing step. I observed an issue while attempting to run the calibration for files of less than 2h of duration which is the case for the exemplary data we use here. Therefore, here, we do not use data calibration. However, we successfully used SummarizedActigraphy R package to calibrate data and compute ENMO for hundreds of longer duration data files in the past \u0026ndash; see this R script.\nPreparing the data input We use raw accelerometry data from a wrist-worn sensor.\n  (Click to see notes and code.)  Note:\n SummarizedActigraphy R package needs data with observation timestamp in the form of POSIXct class. This timestamp column must be named HEADER_TIME_STAMP; the specific column naming is needed due to some issue reported for MIMSunit R package. We hence create a “fake” POSIXct column named HEADER_TIME_STAMP. The option options(digits.secs = 3) was used on the top of this R script to allow displaying decimal time part.  We first demonstrate the code for one participant.\ndat_input_i \u0026lt;- dat_i %\u0026gt;% mutate(HEADER_TIME_STAMP = seq(from = ymd_hms(\u0026quot;2021-01-01 00:00:00\u0026quot;), by = 0.01, length.out = nrow(dat_i))) %\u0026gt;% select(HEADER_TIME_STAMP, X = lw_x, Y = lw_y, Z = lw_z)   dat_input_i # A tibble: 303,300 x 4 HEADER_TIME_STAMP X Y Z \u0026lt;dttm\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; 1 2021-01-01 00:00:00.000 0.039 1.02 -0.02 2 2021-01-01 00:00:00.009 -0.629 -0.461 0.973 3 2021-01-01 00:00:00.019 -0.926 -1.26 0.691 4 2021-01-01 00:00:00.029 -0.871 -1.50 -0.246 5 2021-01-01 00:00:00.039 -0.727 -1.62 -0.559 6 2021-01-01 00:00:00.049 -0.543 -1.66 -0.629 7 2021-01-01 00:00:00.059 -0.348 -1.64 -0.609 8 2021-01-01 00:00:00.069 -0.16 -1.60 -0.566 9 2021-01-01 00:00:00.079 -0.012 -1.53 -0.523 10 2021-01-01 00:00:00.089 0.117 -1.43 -0.484 # … with 303,290 more rows  We use package’s helper function to check if sample rate is being determined correctly (should be 100 Hz).\nget_sample_rate(dat_input_i) [1] 100   Computing the measures We calculate minute-level summary measures: AC, MIMS, ENMO, MAD, AI. The output shows values of the measures per each minute.\nout_i = SummarizedActigraphy::calculate_measures( df = dat_input_i, dynamic_range = c(-8, 8), # dynamic range fix_zeros = FALSE, # fixes zeros from idle sleep mode -- not needed in our case calculate_mims = TRUE, # uses algorithm from MIMSunit package calculate_ac = TRUE, # uses algorithm from activityCounts package flag_data = FALSE, # runs raw data quality control flags algorithm -- not used in our case verbose = FALSE) ====================================================================================== out_i \u0026lt;- out_i %\u0026gt;% select(HEADER_TIME_STAMP = time, AC, MIMS = MIMS_UNIT, ENMO = ENMO_t, MAD, AI) out_i # A tibble: 51 × 6 HEADER_TIME_STAMP AC MIMS ENMO MAD AI \u0026lt;dttm\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; 1 2021-01-01 00:00:00.000 3623. 19.3 0.0882 0.140 7.62 2 2021-01-01 00:01:00.000 2460. 14.8 0.0544 0.0971 5.84 3 2021-01-01 00:02:00.000 648. 6.52 0.00607 0.0192 2.44 4 2021-01-01 00:03:00.000 36.2 1.86 0.000769 0.00862 1.03 5 2021-01-01 00:04:00.000 8084. 34.8 0.323 0.316 15.1 6 2021-01-01 00:05:00.000 13845. 60.1 0.442 0.358 25.3 7 2021-01-01 00:06:00.000 10549. 44.1 0.475 0.248 18.7 8 2021-01-01 00:07:00.000 11219. 45.8 0.508 0.244 19.6 9 2021-01-01 00:08:00.000 11624. 51.6 0.393 0.338 22.1 10 2021-01-01 00:09:00.000 11049. 45.8 0.404 0.288 20.0 # … with 41 more rows  Next, we calculate the measures for all n=32 subjects.\n  (Click to see the code.)  out_all \u0026lt;- data.frame() for (dat_fpath_i in dat_fpaths){ # dat_fpath_i \u0026lt;- dat_fpaths[1] message(basename(dat_fpath_i)) # read data basename_i \u0026lt;- gsub(\u0026quot;.csv\u0026quot;, \u0026quot;\u0026quot;, basename(dat_fpath_i)) dat_i \u0026lt;- read.csv(dat_fpath_i) # prepare data input ts_i \u0026lt;- seq(from = ymd_hms(\u0026quot;2021-01-01 00:00:00\u0026quot;), by = 0.01, length.out = nrow(dat_i)) dat_input_i \u0026lt;- dat_i %\u0026gt;% mutate(HEADER_TIME_STAMP = ts_i) %\u0026gt;% select(HEADER_TIME_STAMP, X = lw_x, Y = lw_y, Z = lw_z) # compute the measures out_i = SummarizedActigraphy::calculate_measures( df = dat_input_i, dynamic_range = c(-8, 8), # dynamic range fix_zeros = FALSE, # fixes zeros from idle sleep mode -- not needed in our case calculate_mims = TRUE, # uses algorithm from MIMSunit package calculate_ac = TRUE, # uses algorithm from activityCounts package flag_data = FALSE, # runs raw data quality control flags algorithm -- not used in our case verbose = FALSE) out_i \u0026lt;- out_i %\u0026gt;% select(HEADER_TIME_STAMP = time, AC, MIMS = MIMS_UNIT, MAD, AI) out_i \u0026lt;- mutate(out_i, subj_id = basename_i, .before = everything()) # append subject-specific measures to all subjects file out_all \u0026lt;- rbind(out_all, out_i) }   Minute-level physical activity measures during walking and driving activities Merging physical activity measures with physical activity labels Next, we merge the minute-level physical-activity measures with the activity labels. We only keep the minutes for which all the measurements were labelled with no more than one type of activity.\n  (Click to see the code.)  labels_df_all \u0026lt;- data.frame() for (dat_fpath_i in dat_fpaths){ # dat_fpath_i \u0026lt;- dat_fpaths[1] message(basename(dat_fpath_i)) # read data basename_i \u0026lt;- gsub(\u0026quot;.csv\u0026quot;, \u0026quot;\u0026quot;, basename(dat_fpath_i)) dat_i \u0026lt;- read.csv(dat_fpath_i) # aggregate activity labels ts_i \u0026lt;- seq(from = ymd_hms(\u0026quot;2021-01-01 00:00:00\u0026quot;), by = 0.01, length.out = nrow(dat_i)) labels_df_i \u0026lt;- dat_i %\u0026gt;% mutate(HEADER_TIME_STAMP = ts_i) %\u0026gt;% mutate(HEADER_TIME_STAMP = lubridate::floor_date(HEADER_TIME_STAMP, \u0026quot;1 min\u0026quot;)) %\u0026gt;% group_by(HEADER_TIME_STAMP) %\u0026gt;% filter(n_distinct(activity) == 1) %\u0026gt;% filter(row_number() == 1) %\u0026gt;% # 1=walking; 2=descending stairs; 3=ascending stairs; 4=driving filter(activity %in% c(1,2,3,4)) %\u0026gt;% ungroup() %\u0026gt;% select(HEADER_TIME_STAMP, activity) labels_df_i \u0026lt;- mutate(labels_df_i, subj_id = basename_i, .before = everything()) # append subject-specific measures to all subjects file labels_df_all \u0026lt;- rbind(labels_df_all, labels_df_i) } # recode activity label labels_df_all \u0026lt;- labels_df_all %\u0026gt;% mutate(activity = recode(activity, '1' = 'walking', '2' = 'descending_stairs', '3' = 'ascending_stairs', '4' = 'driving')) # merge: # (1) physical activity minute-level measures, # (2) physical activity minute-level activity labels out_all_merged \u0026lt;- out_all %\u0026gt;% inner_join(labels_df_all, by = c(\u0026quot;subj_id\u0026quot;, \u0026quot;HEADER_TIME_STAMP\u0026quot;))   out_all_merged # A tibble: 686 × 8 subj_id HEADER_TIME_STAMP AC MIMS ENMO MAD AI activity \u0026lt;chr\u0026gt; \u0026lt;dttm\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;chr\u0026gt; 1 id00b70b13 2021-01-01 00:06:00.000 10549. 44.1 0.475 0.248 18.7 walking 2 id00b70b13 2021-01-01 00:07:00.000 11219. 45.8 0.508 0.244 19.6 walking 3 id00b70b13 2021-01-01 00:11:00.000 11026. 42.3 0.498 0.215 18.9 walking 4 id00b70b13 2021-01-01 00:12:00.000 10561. 42.1 0.486 0.231 19.1 walking 5 id00b70b13 2021-01-01 00:13:00.000 13297. 53.9 0.591 0.325 23.1 walking 6 id00b70b13 2021-01-01 00:21:00.000 2603. 14.5 0.0501 0.0413 4.80 driving 7 id00b70b13 2021-01-01 00:23:00.000 499. 6.65 0.0534 0.0537 3.59 driving 8 id00b70b13 2021-01-01 00:24:00.000 398. 6.06 0.0537 0.0496 3.49 driving 9 id00b70b13 2021-01-01 00:25:00.000 130. 5.92 0.0542 0.0560 3.67 driving 10 id00b70b13 2021-01-01 00:26:00.000 414. 7.00 0.0469 0.0561 3.55 driving # … with 676 more rows table(out_all_merged$activity) driving walking 569 117  Visualizing physical activity measures across activity types We visualize values of minute-level physical activity measures. Note these only used data collected with a sensor located at left wrist.\n  (Click to see the code.)  name_levels \u0026lt;- c(\u0026quot;AC\u0026quot;, \u0026quot;MIMS\u0026quot;, \u0026quot;ENMO\u0026quot;, \u0026quot;MAD\u0026quot;, \u0026quot;AI\u0026quot;) activity_levels \u0026lt;- c(\u0026quot;walking\u0026quot;, \u0026quot;driving\u0026quot;) # define data subsets with one activity type only df_walk \u0026lt;- out_all_merged %\u0026gt;% filter(activity == \u0026quot;walking\u0026quot;) df_driv \u0026lt;- out_all_merged %\u0026gt;% filter(activity == \u0026quot;driving\u0026quot;) # estimate sample population mean for each measure via LMM measure_mean_vals \u0026lt;- c( # walking fixef(lmer(AC ~ (1 | subj_id), data = df_walk))[1], fixef(lmer(MIMS ~ (1 | subj_id), data = df_walk))[1], fixef(lmer(ENMO ~ (1 | subj_id), data = df_walk))[1], fixef(lmer(MAD ~ (1 | subj_id), data = df_walk))[1], fixef(lmer(AI ~ (1 | subj_id), data = df_walk))[1], # driving fixef(lmer(AC ~ (1 | subj_id), data = df_driv))[1], fixef(lmer(MIMS ~ (1 | subj_id), data = df_driv))[1], fixef(lmer(ENMO ~ (1 | subj_id), data = df_driv))[1], fixef(lmer(MAD ~ (1 | subj_id), data = df_driv))[1], fixef(lmer(AI ~ (1 | subj_id), data = df_driv))[1] ) measure_mean_df \u0026lt;- data.frame( value = measure_mean_vals, name = rep(name_levels, times = 2), activity = rep(activity_levels, each = 5) ) measure_mean_df_w \u0026lt;- measure_mean_df %\u0026gt;% mutate(value = round(value, 2)) %\u0026gt;% pivot_wider(names_from = activity) # define subject-specific means out_all_l \u0026lt;- out_all_merged %\u0026gt;% pivot_longer(cols = all_of(name_levels)) out_all_l_agg \u0026lt;- out_all_l %\u0026gt;% group_by(subj_id, name, activity) %\u0026gt;% summarise(value = mean(value)) subj_id_levels \u0026lt;- out_all_l_agg %\u0026gt;% filter(name == \u0026quot;AC\u0026quot;, activity == \u0026quot;walking\u0026quot;) %\u0026gt;% arrange(desc(value)) %\u0026gt;% pull(subj_id) # mutate data frames used on the plot to have factors at certain levels order measure_mean_df \u0026lt;- measure_mean_df %\u0026gt;% mutate(activity = factor(activity, levels = activity_levels)) %\u0026gt;% mutate(name = factor(name, levels = name_levels)) out_all_l \u0026lt;- out_all_l %\u0026gt;% mutate(subj_id = factor(subj_id, levels = subj_id_levels), activity = factor(activity, levels = activity_levels)) %\u0026gt;% mutate(name = factor(name, levels = name_levels)) out_all_l_agg \u0026lt;- out_all_l_agg %\u0026gt;% mutate(subj_id = factor(subj_id, levels = subj_id_levels), activity = factor(activity, levels = activity_levels)) %\u0026gt;% mutate(name = factor(name, levels = name_levels))   First, we estimated sample population mean for each minute-level measure via linear mixed model (LMM).\nkable(measure_mean_df_w)    name walking driving    AC 7616.48 801.15  MIMS 34.86 7.10  ENMO 0.24 0.04  MAD 0.26 0.05  AI 15.01 3.31    Next, we plot minute-level physical activity measures:\n individual measure values (red) for each subject, mean measure value (black) for each subject, sample population mean for each measure (dashed horizontal line) estimated via LMM.  On each plot panel, subject IDs (x-axis) are sorted by average AC during walking (left-top plot panel).\n  (Click to see the code.)  plt_measures \u0026lt;- ggplot(out_all_l, aes(x = subj_id, y = value)) + geom_hline(data = measure_mean_df, aes(yintercept = value), linetype = 2, size = 0.2) + geom_point(size = 0.5, alpha = 0.3, color = \u0026quot;red\u0026quot;) + geom_point(data = out_all_l_agg, aes(x = subj_id, y = value), size = 0.5, color = \u0026quot;black\u0026quot;) + facet_grid(name ~ activity, scales = \u0026quot;free\u0026quot;) + labs(x = \u0026quot;Subject ID\u0026quot;, y = \u0026quot;\u0026quot;) + theme_gray(base_size = 14) + theme(panel.grid.major = element_line(size = 0.3), panel.grid.minor = element_blank(), axis.text.x = element_text(size = 6, angle = 90, vjust = 0.5, hjust = 1))   plt_measures  The above plot highlights what are minute-level summary measures of physical activity from raw accelerometry data: AC, MIMS, ENMO, MAD, and AI, across two different activities (walking, driving) in the study sample of n=32 healthy individuals.\nAcknowledgements I’d like to thank John Muschelli for feedback on this tutorial.\nCitation info Please cite SummarizedActigraphy R package, if applicable.\nPlease cite raw accelerometry dataset DOI, if applicable (go to see this dataset citation options).\n","date":1625951968,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1625951968,"objectID":"ac294fa861c638aab47728620ded81fb","permalink":"/post/2021-06-29-pa_measures_and_summarizedactigraphy/","publishdate":"2021-07-10T17:19:28-04:00","relpermalink":"/post/2021-06-29-pa_measures_and_summarizedactigraphy/","section":"post","summary":"In this post, we:\n  use dataset “Labeled raw accelerometry data captured during walking, stair climbing and driving” that is freely available on PhysioNet;\n  derive four minute-level summary measures of physical activity – AC, MIMS, ENMO, MAD, AI – from raw accelerometry data using SummarizedActigraphy R package;\n  summarize minute-level summary measures across walking and driving activities.\n ","tags":[],"title":"Computing minute-level summary measures of physical activity from raw accelerometry data in R: AC, MIMS, ENMO, MAD, and AI","type":"post"},{"authors":["Karas","M.","Urbanek","J.K.","Illiano","V.P.","Bogaarts","G.","Crainiceanu","C.M.","Dorn","J.F."],"categories":[],"content":"","date":1622239718,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1622239718,"objectID":"f8da11db7f74d60ffcfb5680211fe5f5","permalink":"/publication/estimating_free_living_walking_cadence/","publishdate":"2021-05-28T18:08:38-04:00","relpermalink":"/publication/estimating_free_living_walking_cadence/","section":"publication","summary":"**Objective**: We evaluate the stride segmentation performance of the Adaptive Empirical Pattern Transformation (ADEPT) for subsecond-level accelerometry data collected in the free-living environment using a wrist-worn sensor. **Approach**: We substantially expand the scope of the existing ADEPT pattern-matching algorithm. Methods are applied to subsecond-level accelerometry data collected continuously for 4 weeks in 45 participants, including 30 arthritis and 15 control patients. We estimate the daily walking cadence for each participant and quantify its association with SF-36 quality of life (QoL) measures. **Main results**: We provide free, open-source software to segment individual walking strides in subsecond-level accelerometry data. Walking cadence is significantly associated with the Role physical score reported via SF-36 after adjusting for age, gender, weight and height. **Significance**: Methods provide automatic, precise walking stride segmentation, which allows estimation of walking cadence from free-living wrist-worn accelerometry data. Results provide new evidence of associations between free-living walking parameters and health outcomes.","tags":["wearable accelerometers","physical activity","actigraphy","consumer-grade devices","rehabilitation monitoring"],"title":"Estimation of free-living walking cadence from wrist-worn sensor accelerometry data and its association with SF-36 quality of life scores","type":"publication"},{"authors":[],"categories":null,"content":"","date":1620916200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620916200,"objectID":"eddcc5800bfd8ebec23e242e6f472b5f","permalink":"/talk/2021-05-13-wit_lab_meeting/","publishdate":"2021-05-12T14:30:00Z","relpermalink":"/talk/2021-05-13-wit_lab_meeting/","section":"talk","summary":"","tags":[],"title":"Accelerometry-based physical activity measures across NHANES waves. Comparison of five minute-level measures: AC, MIMS, ENMO, MAD, AI","type":"talk"},{"authors":[],"categories":null,"content":"","date":1620304200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1620304200,"objectID":"b10bae655e7a402c459a156e3fa94fc4","permalink":"/talk/2021-05-06-engage_lab_meeting/","publishdate":"2021-05-06T12:30:00Z","relpermalink":"/talk/2021-05-06-engage_lab_meeting/","section":"talk","summary":"","tags":[],"title":"National Health and Nutrition Examination Survey (NHANES) and comparability of accelerometry-derived physical activity measures across NHANES waves","type":"talk"},{"authors":[],"categories":null,"content":"","date":1619532000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1619532000,"objectID":"f54b4498cc856b9ed1b1c1da45470523","permalink":"/talk/2021-04-27-eba_training_program/","publishdate":"2021-04-27T14:00:00Z","relpermalink":"/talk/2021-04-27-eba_training_program/","section":"talk","summary":"","tags":[],"title":"Comparison of accelerometry-derived physical activity summary measures by age, sex, and BMI","type":"talk"},{"authors":["Brzyski","D.","Karas","M.","Ances","B.","Dzemidzic","M.","Goni","J.","Randolph","T.W.","Harezlak","J."],"categories":[],"content":"","date":1613426390,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1613426390,"objectID":"36971b4e4e7ba5f36a7df40b77d0fd41","permalink":"/publication/brain-connectivity-informed-regularization-generalized/","publishdate":"2021-02-15T17:59:50-04:00","relpermalink":"/publication/brain-connectivity-informed-regularization-generalized/","section":"publication","summary":"We propose a novel regularization method, generalized ridgified Partially Empirical Eigenvectors for Regression (griPEER), to estimate associations between the brain structure features and a scalar outcome within the generalized linear regression framework. griPEER improves the regression coefficient estimation by providing a principled approach to use external information from the structural brain connectivity. Specifically, we incorporate a penalty term, derived from the structural connectivity Laplacian matrix, in the penalized generalized linear regression. In this work, we address both theoretical and computational issues and demonstrate the robustness of our method despite incomplete information about the structural brain connectivity. In addition, we also provide a significance testing procedure for performing inference on the estimated coefficients. Finally, griPEER is evaluated both in extensive simulation studies and using clinical data to classify HIV+ and HIV− individuals.","tags":["brain imaging","statistical methods","regularization","regression","generalized regression"],"title":"Connectivity-Informed Adaptive Regularization for Generalized Outcomes","type":"publication"},{"authors":[],"categories":null,"content":"","date":1608463800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608463800,"objectID":"9b7a91c6d457324ee12451613e9256fc","permalink":"/talk/2020-12-20-cmsstatistics/","publishdate":"2020-12-19T11:30:00Z","relpermalink":"/talk/2020-12-20-cmsstatistics/","section":"talk","summary":"Walking and gait parameters have become increasingly important in epidemiological and clinical studies. Indeed, 3 out of 7 submissions to the FDA for eCOA qualifications of digital endpoints are quantifying gait parameters. Recent evidence suggests that observations collected in a free-living environment are complementary to traditional, lab- and clinic-based walking measurements. Sub-second-level actigraphy data can provide a detailed description of human movement. Despite the growing need, the number of publicly available methods to derive gait from high-density accelerometry data collected by wrist-worn devices is limited. We propose an extension of ADEPT, a pattern-matching method, to segment individual walking strides in sub-second-level accelerometry data collected in a free-living environment using a wrist-worn sensor. We evaluate the method on 4-week observation data from 30 people with and 15 people without arthritis. We show that daily walking cadence is significantly associated with general mental health, social functioning, and role physical scores reported via SF-36. We provide open-source software and out-of-study sample data examples online.","tags":[],"title":"Estimation of free-living walking cadence from wrist-worn sensor accelerometry data and its association with SF-36 quality of life scores","type":"talk"},{"authors":[],"categories":null,"content":"","date":1608215400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1608215400,"objectID":"c413c6edd4792f0efb46e34bcf32fabb","permalink":"/talk/2020-12-17-wit_arctools/","publishdate":"2020-12-16T14:30:00Z","relpermalink":"/talk/2020-12-17-wit_arctools/","section":"talk","summary":"","tags":[],"title":"arctools R package: Processing and physical activity summaries of minute level activity data","type":"talk"},{"authors":["Karas","M.","Marinsek","N.","Goldhahn","J.","Foschini","L.","Ramirez","E.","Clay","I."],"categories":[],"content":"","date":1607551718,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607551718,"objectID":"8ff8db192554f809ce45da84ad6ccc19","permalink":"/publication/predicting-subjective-recovery/","publishdate":"2020-12-09T18:08:38-04:00","relpermalink":"/publication/predicting-subjective-recovery/","section":"publication","summary":"**Introduction**: A major challenge in the monitoring of rehabilitation is the lack of long-term individual baseline data which would enable accurate and objective assessment of functional recovery. Consumer-grade wearable devices enable the tracking of individual everyday functioning prior to illness or other medical events which necessitate the monitoring of recovery trajectories. **Methods**: For 1,324 individuals who underwent surgery on a lower limb, we collected their Fitbit device data of steps, heart rate, and sleep from 26 weeks before to 26 weeks after the self-reported surgery date. We identified subgroups of individuals who self-reported surgeries for bone fracture repair (n = 355), tendon or ligament repair/reconstruction (n = 773), and knee or hip joint replacement (n = 196). We used linear mixed models to estimate the average effect of time relative to surgery on daily activity measurements while adjusting for gender, age, and the participant-specific activity baseline. We used a sub-cohort of 127 individuals with dense wearable data who underwent tendon/ligament surgery and employed XGBoost to predict the self-reported recovery time. **Results**: The 1,324 study individuals were all US residents, predominantly female (84%), white or Caucasian (85%), and young to middle-aged (mean age 36.2 years). We showed that 12 weeks pre- and 26 weeks post-surgery trajectories of daily behavioral measurements (steps sum, heart rate, sleep efficiency score) can capture activity changes relative to an individual’s baseline. We demonstrated that the trajectories differ across surgery types, recapitulate the documented effect of age on functional recovery, and highlight differences in relative activity change across self-reported recovery time groups. Finally, using a sub-cohort of 127 individuals, we showed that long-term recovery can be accurately predicted, on an individual level, only 1 month after surgery (AUROC 0.734, AUPRC 0.8). Furthermore, we showed that predictions are most accurate when long-term, individual baseline data are available. **Discussion**: Leveraging long-term, passively collected wearable data promises to enable relative assessment of individual recovery and is a first step towards data-driven intervention for individuals.","tags":["wearable accelerometers","physical activity","actigraphy","consumer-grade devices","rehabilitation monitoring"],"title":"Predicting Subjective Recovery from Lower Limb Surgery Using Consumer Wearables","type":"publication"},{"authors":[],"categories":null,"content":"","date":1607425200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1607425200,"objectID":"42a66be3d170bd6a138e1b335403b9d7","permalink":"/talk/2020-12-08-krager_workshop/","publishdate":"2020-12-08T12:15:00Z","relpermalink":"/talk/2020-12-08-krager_workshop/","section":"talk","summary":"","tags":[],"title":"Predicting subjective recovery from lower limb surgery using consumer wearables","type":"talk"},{"authors":[],"categories":null,"content":"","date":1606824900,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1606824900,"objectID":"a8ce183955db2dec655b2141bb1fdf1a","permalink":"/talk/2020-12-01-computing_club/","publishdate":"2020-11-16T12:15:00Z","relpermalink":"/talk/2020-12-01-computing_club/","section":"talk","summary":"","tags":[],"title":"Recent developments in R tidyverse","type":"talk"},{"authors":null,"categories":null,"content":"","date":1601490600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1601490600,"objectID":"188acd4f3a09ce14aba13f0c1215a4ee","permalink":"/talk/2020-09-31-wit_summer_project_evidation/","publishdate":"2020-09-30T14:30:00-04:00","relpermalink":"/talk/2020-09-31-wit_summer_project_evidation/","section":"talk","summary":"A major challenge in the monitoring of rehabilitation is the lack of long-term individual baseline data which would enable accurate and objective assessment of functional recovery. In my Summer 2020 internship project with Evidation Health, I explored usage of data from long-term baseline and post-op period data collected  from consumer-grade wearable devices to quantify changes in PA after lower limb surgery event.","tags":[],"title":"Summer internship project: Predicting subjective recovery from lower limb surgery using consumer wearables","type":"talk"},{"authors":null,"categories":null,"content":"","date":1598557850,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598557850,"objectID":"cfa7685b15806c6fbe8e60eb41f42e70","permalink":"/talk/2020-08-17-iscb_krakow/","publishdate":"2020-08-27T15:50:50-04:00","relpermalink":"/talk/2020-08-17-iscb_krakow/","section":"talk","summary":"","tags":[],"title":"Novel approach for precise walking cadence estimation from high-density tri-axial accelerometry data collected at wrist in free-living","type":"talk"},{"authors":null,"categories":[],"content":"2021   For my first post-grad role, I accepted a postdoctoral researcher position at the Onnela Lab at Harvard University. I will be working at methods for digital phenotyping and its applications.\n  I defended my PhD thesis \u0026ldquo;Statistical Methods for Wearable Device Data and Sample Size Calculation in Complex Models\u0026rdquo;! My defense slides, with video recording linked, are available. I published the Acknowledgements and Dedication parts of the thesis.\n  I received Travel Reimbursement Award to 3rd Annual Health Data Science Symposium at Harvard to present our work on harmonization of open-source and proprietary accelerometry-based physical activity measures in Nov 2021. \n  Our preprint \u0026ldquo;Upstrap for estimating power and sample size in complex models\u0026rdquo; is available on bioRxiv. We evaluate power estimation properties of the upstrap and provide a series of \u0026ldquo;read, adapt and use\u0026rdquo; R code examples for simple and complex settings, including GLMM. See the relevant project page.\n  I received Student Poster Sponsorship and will present our work on harmonization of open-source and proprietary accelerometry-based physical activity measures during the ActiGraph Digital Data Summit in Nov 2021. \n  I am a co-instructor for Introduction to R for Public Health Researchers Summer Institute week-long course. All class materials are available online. \n  Our paper \u0026ldquo;Estimation of free-living walking cadence from wrist-worn sensor accelerometry data and its association with SF-36 quality of life scores\u0026rdquo; is published. Check out the GitHub repo showing examples of using our method for automatic walking strides segmentation from wrist-worn sensor accelerometry data collected in the free-living environment. \n  I received the Louis I. and Thomas D. Dublin Award for the Advancement of Epidemiology and Biostatistics for work on open-source physical activity measurements. \n  I received the Helen Abbey Award for excellence in teaching. \n  Our paper \u0026ldquo;Connectivity‐informed adaptive regularization for generalized outcomes\u0026rdquo; is published. We proposed riPEER (ridgified Partially Empirical Eigenvectors for Regression) extension to generalized linear regression, addressing both theoretical and computational issues. See the relevant project page. \n  2020   Our paper \u0026ldquo;Predicting Subjective Recovery from Lower Limb Surgery Using Consumer Wearables\u0026rdquo; is published. We showed that passively collected wearable PGHD can capture post-surgery physical activity changes relative to individual\u0026rsquo;s baseline, and baseline data can improve prediction of self-reported recovery time at 4 weeks post surgery. \n  During the upcoming workshop \u0026ldquo;The Future of Digital Health\u0026rdquo;, I will give a talk highlighting \u0026ldquo;Predicting subjective recovery from lower limb surgery using consumer wearables\u0026rdquo; paper I co-authored. This workshop will highlight key papers from the newly released Special Issue of Digital Biomarkers, sponsored by Evidation Health and DiMe. Register for the workshop here. \n  During summer of 2020, I am working as a Data Science Intern in Digital Measures team @ Evidation Health. My main project aims at estimating medical procedure recovery trajectories and predicting recovery time from wearable patient-generated health data. \n  Why R? Foundation awards a supporting grant for Women in Data Science to aid exceptional female data scientists in Poland. I serve as a member of Scientific committee for this initiative. \n  2019   I passed my preliminary schoolwide oral exam. My Committee consisted of three faculty from the Dept. of Biostatistics, one faculty from Dept. of Epidemiology, and one faculty from the Dept. of Medicine. \n  I received Leadership, Empowerment and Learning Culture Award during Novartis US Analytics Conference 2019. My conference talk presented the work done as a Sensor Data Analytics Intern with Novartis in Basel, Switzerland during summer 2019. \n  Our paper \u0026ldquo;Adaptive empirical pattern transformation (ADEPT) with application to walking stride segmentation\u0026rdquo; just got published in Biostatistics with the journal\u0026rsquo;s highest reproducibility status! See ADEPT article online. \n  Our R package adept that implements ADEPT pattern-segmentation method is out on CRAN (CRAN index. The vignette shows example of strides segmentation from raw accelerometry data of 25min outdoor run w/ walking and resting bouts. Listed May 2019 top 40 new CRAN packages. \n  I am working as Sensor Data Analytics Intern with Novartis pharmaceutical company in Basel, Switzerland this summer. I am also presenting at ICAMPAM 2019 conference on Jun 26 in Maastricht, the Netherlands. \n  Our R package runstats that provides methods for fast computation of running sample statistics for time series is out on CRAN (CRAN index). Use runstats to compute (1) mean, (2) standard deviation, and (3) variance over a fixed-length window of time-series, (4) correlation, (5) covariance, and (6) Euclidean distance (L2 norm) between short-time pattern and time-series. Implemented methods utilize Convolution Theorem to compute convolutions via Fast Fourier Transform (FFT). \n  Our recent article \u0026ldquo;Accelerometry Data in Health Research: Challenges and Opportunities\u0026rdquo; is now published and available online. \n  2018   Interested in statistical methodology for singal processing in physical activity research? Come to JSM session on Wednesday (8/1/2018), 2:00 PM - 3:50 PM. \n  WhyR? 2018 conference starts on July 2nd in Wroclaw, Poland. Both academia and industry professionals meet and discuss experiences in R software development and analysis applications. \n  I am working as a Research Assistant with Drs. Jacek Urbanek and Ciprian Crainiceanu this summer at a novel approach for identifying individual working strides from subsecond accelerometry data of walking.\n  ","date":1598328000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598328000,"objectID":"4964d15a2ea64cf2255afdddb4bb9834","permalink":"/resources/recently-all/","publishdate":"2020-08-25T00:00:00-04:00","relpermalink":"/resources/recently-all/","section":"resources","summary":"2021   For my first post-grad role, I accepted a postdoctoral researcher position at the Onnela Lab at Harvard University. I will be working at methods for digital phenotyping and its applications.","tags":[],"title":"All past updates","type":"resources"},{"authors":null,"categories":[],"content":"Academic year 2021-22    Term Course Number Course title Teaching role     2021-22 Summer Institute 140.604 Introduction to R for Public Health Researchers Co-instructor   2021-22 Term 1 600.710 Statistical Concepts in Public Health 2 TA   2021-22 Term 2 140.612 Statistical Reasoning in Public Health II TA    Academic year 2020-21    Term Course Number Course title Teaching role     2020-21 Term 1 140.651 Methods in Biostatistics I Lead TA   2020-21 Term 2 140.651 Methods in Biostatistics II Lead TA    Academic year 2019-20    Term Course Number Course title Teaching role     2019-20 Term 1 140.850 Special topics course: Biostatistical Methods for Wearable Computing Co-instructor   2019-20 Term 1 140.651 Methods in Biostatistics I Lead TA   2019-20 Term 2 140.652 Methods in Biostatistics II Lead TA    Academic year 2018-19    Term Course Number Course title Teaching role     2018-19 Term 1 140.651 Methods in Biostatistics I TA   2018-19 Term 2 140.652 Methods in Biostatistics II TA   2018-19 Term 3 140.623 Statistical Methods in Public Health III TA   2018-19 Term 4 140.624 Statistical Methods in Public Health IV TA    ","date":1598328000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1598328000,"objectID":"dbe7eefd33ed0e1880a5b44bf664e532","permalink":"/resources/teaching-all/","publishdate":"2020-08-25T00:00:00-04:00","relpermalink":"/resources/teaching-all/","section":"resources","summary":"Academic year 2021-22    Term Course Number Course title Teaching role     2021-22 Summer Institute 140.604 Introduction to R for Public Health Researchers Co-instructor   2021-22 Term 1 600.","tags":[],"title":"Teaching experience","type":"resources"},{"authors":null,"categories":null,"content":"","date":1591282800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1591282800,"objectID":"86f70f742e516927d62d896938efdbb7","permalink":"/talk/2020-06-04-engage_arcstats/","publishdate":"2020-06-04T11:00:00-04:00","relpermalink":"/talk/2020-06-04-engage_arcstats/","section":"talk","summary":"","tags":[],"title":"arctools: R software for computing summaries of minute-level physical activity data","type":"talk"},{"authors":null,"categories":null,"content":"","date":1583949650,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1583949650,"objectID":"3ac85b7b71bc67214f69718321c0789e","permalink":"/talk/2020-03-11-kth-oss-webinar-adept/","publishdate":"2020-03-11T14:00:50-04:00","relpermalink":"/talk/2020-03-11-kth-oss-webinar-adept/","section":"talk","summary":"","tags":[],"title":"Precise walking strides segmentation from raw accelerometry data: ADEPT method","type":"talk"},{"authors":null,"categories":null,"content":". .\n","date":1582614900,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1582614900,"objectID":"5f613839a84ba053233819dc96d5f2a6","permalink":"/talk/2020-02-25-banff/","publishdate":"2020-02-25T02:15:00-05:00","relpermalink":"/talk/2020-02-25-banff/","section":"talk","summary":". .","tags":[],"title":"Towards precise walking strides segmentation from accelerometry data in free-living: ADEPT method and further developments","type":"talk"},{"authors":null,"categories":null,"content":"","date":1576503600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1576503600,"objectID":"88654bb80156a4f14851581c08b55704","permalink":"/talk/2019-12-03-cmstatistics-london/","publishdate":"2019-12-16T08:40:00-05:00","relpermalink":"/talk/2019-12-03-cmstatistics-london/","section":"talk","summary":"Ability to characterize human gait pattern has significant potential in health research and can help guide clinical decision making. For example, in stroke survivors, quantification of asymmetry in walking strides during and after an in-lab intervention is of particular interest. To address this problem, we use high-density, high-throughput wearable accelerometry data and propose a stride pattern registration framework. We use a two-parameter family of time-warping functions to estimate clinically relevant stride characteristics, including duration and asymmetry. To demonstrate the approach, we collect accelerometry data on a healthy adult walking on a split-belt treadmill under different conditions mimicking step-to-step asymmetry. To analyze the data, we first use ADaptive Empirical Pattern Transformation (ADEPT) - a fast and scalable method for strides segmentation. We then employ a parametrized stride pattern framework to further characterize segmented strides. We conclude that the parametrized adaptive pattern matching appears to be a promising approach for estimation of step asymmetry.","tags":[],"title":"Functional registration of walking strides in high-density accelerometry data for estimation of gait asymmetry","type":"talk"},{"authors":[],"categories":[],"content":"We propose adaptive empirical pattern transformation (ADEPT), a fast, scalable, and accurate method for pattern segmentation in time-series.\nTable of Contents    Scientific problem Challenges Proposed solution Published work Software Images used in the post \u0026ndash; credit/references     Scientific problem The motivation for the work was to provide fast and accurate open-source method for pattern segmentation from raw accelerometry data.\nThe methods were needed for automated walking strides segmentation from accelerometry recordings collected during continuous walking that we had across a number of health studies.\nChallenges The plot below shows an example of raw accelerometry data \u0026ndash; three-dimensional time-series of acceleration [g] measurements. Data showed were collected 5 s of walking for two different individuals, with 4 wearable sensors worn simultaneously at wrist, hip, left, and right ankle.\nWhile the repetitive patterns of walking are relatively clear to a human observer, there are a few challenges in segmenting them accurately with an algorithm:\n  There are variations in shape, magnitude and duration of a pattern within individual\u0026rsquo;s data. These might be e.g. due to terrain elevation changes, or temporal changes of step length and cadence (think about slowing down when approaching the turn of the corridor, or basically walking slower during an evening stroll versus morning rush to work). \n  There is variability of walking data between individuals (e.g. see the plot above). \n  A sensor can move, or be worn on different hands by the same person on different days.\n  Proposed solution We propose adaptive empirical pattern transformation (ADEPT) to segment walking stride patterns in vector magnitude of raw accelerometry data.\n  The ADEPT algorithm uses a predefined template and detects its repetitions by maximizing the local distance (i.e. correlation) between (a) collection of scale-transformed templates and (b) the observed data signal. \n  The scale-transformation adjusts the duration of the dictionary template, allowing for the detection of patterns that are shorter or longer than the original dictionary template. \n  Multiple distinct baseline templates can be used simultaneously to account for various shape patterns occurring in the data.\n  The GIF below demonstrates the big picture of the algorithm.\nThe underlying template\u0026rsquo;s scaling and translating along the observed data signal is closely related to the Continuous Wavelet Transform (CWT), $$W_{\\Psi}(s, \\tau) = \\int_{-\\infty}^{\\infty} x(t) \\frac{1}{\\sqrt{s}}\\Psi \\left(\\frac{t - \\tau}{s} \\right)dt.$$ Conversely to CWT\u0026rsquo;s mother wavelet $\\Psi(\\cdot)$, ADEPT uses a data-based pattern function (not required to satisfy the wavelet admissibility condition) and comes with a number of other algorithm features tailored for its target application.\nPublished work  We published the proposed ADEPT method in work Adaptive empirical pattern transformation (ADEPT) with application to walking stride segmentation Karas, M., Straczkiewicz, M., Fadel, W., Harezlak, J., Crainiceanu, C.M., Urbanek, J.K. (2018). Biostatistics, Volume 22, Issue 2, April 2021, Pages 331–347.  Software We provided open-source implementation of the proposed ADEPT method in R package adept (CRAN index). The R package is accompanied by two vignettes:\n  Introduction to adept package,\n  Walking strides segmentation with adept.\n  🎉 The adept R package was selected in Top 40 new CRAN packages in May 2019 (list link).\nImages used in the post \u0026ndash; credit/references   Featured image. Figure 2 in the manuscript: Karas, M., Straczkiewicz, M., Fadel, W., Harezlak, J., Crainiceanu, C.M., Urbanek, J.K. Adaptive empirical pattern transformation (ADEPT) with application to walking stride segmentation (2018). Biostatistics, Volume 22, Issue 2, April 2021, Pages 331–347. Link (last accessed on May 26, 2021).\n  Three-dimensional time-series image. Figure 1 in the manuscript: Karas, M., Straczkiewicz, M., Fadel, W., Harezlak, J., Crainiceanu, C.M., Urbanek, J.K. Adaptive empirical pattern transformation (ADEPT) with application to walking stride segmentation (2018). Biostatistics, Volume 22, Issue 2, April 2021, Pages 331–347. Link (last accessed on May 26, 2021).\n  ","date":1575647693,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1575647693,"objectID":"b9885aa50e3ce5dcd4044138b8c3281a","permalink":"/project/project_adept/","publishdate":"2019-12-06T11:54:53-04:00","relpermalink":"/project/project_adept/","section":"project","summary":"We propose adaptive empirical pattern transformation (ADEPT), a fast, scalable, and accurate method for pattern segmentation in time-series.","tags":["statistical methods","wearable accelerometers","actigraphy","pattern segmentation","walking","cadence"],"title":"Adaptive empirical pattern transformation (ADEPT)","type":"project"},{"authors":null,"categories":null,"content":"","date":1572984050,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572984050,"objectID":"7153d23a923d5ce46c72ad44d7effea6","permalink":"/talk/2019-11-05-3rd-oss-webinar/","publishdate":"2019-11-05T15:00:50-05:00","relpermalink":"/talk/2019-11-05-3rd-oss-webinar/","section":"talk","summary":"","tags":[],"title":"Methods for fast processing of time-series: runstats R package","type":"talk"},{"authors":null,"categories":null,"content":". .\n","date":1572546600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572546600,"objectID":"a8f13b4f67f632df2396e9821c7585cb","permalink":"/talk/2019-10-31-wit-novartis/","publishdate":"2019-10-31T14:30:00-04:00","relpermalink":"/talk/2019-10-31-wit-novartis/","section":"talk","summary":". .","tags":[],"title":"Considerations in statistical modeling of walking features derived from wrist-worn sensor in free-living","type":"talk"},{"authors":null,"categories":[],"content":"To construct confidence interval for the mean, we often use quantiles of standardized sample mean distribution. Here, I include a list of cases where I\u0026rsquo;d use quantiles of t-distribution versus quantiles of normal distribution for that purpose.\nNote: the below text could be directly translated to answer when to use t-test versus z-test in testing hypothesis about the mean parameter.\nTable of Contents  Example 1: constructing confidence interval for $\\mu$ with $z$-quantiles Example 2: constructing confidence interval for $\\mu$ with $t$-quantiles   Case 1: observations from normal distribution, $\\sigma$ known, any $n$ Case 2: observations from normal distribution, $\\sigma$ unknown, small $n$ Case 3: observations from normal distribution, $\\sigma$ unknown, large $n$ Case 4: observations from any distribution, $\\sigma$ known, small $n$ Case 5: observations from any distribution, $\\sigma$ known, large $n$ Case 6: observations from any distribution, $\\sigma$ unknown, small $n$ Case 7: observations from any distribution, $\\sigma$ unknown, large $n$   Standardized sample mean Consider $X_1, \\ldots, X_n$ \u0026ndash; a sequence of i.i.d. random variables with mean $E(X_i) = \\mu$ and variance $\\text{var}(X_i) = \\sigma^2$. To construct confidence intervals for $\\mu$ parameter, we often use a standardized sample mean,\n$$ \\begin{aligned} \\frac{\\overline{X}_n - \\mu}{\\sigma/\\sqrt{n}}, \\end{aligned} $$\nor its version where $S_n$ \u0026ndash; a consistent estimator of true standard deviation $\\sigma$ \u0026ndash; is used, $\\frac{\\overline{X}_n - \\mu}{S_n/\\sqrt{n}}$; the latter is common in practice as we typically do not know $\\sigma$ and must estimate it from the data. Knowing distribution of a standardized sample mean allows us to construct confidence interval for a mean $\\mu$ parameter.\nExample 1: constructing confidence interval for $\\mu$ with $z$-quantiles Assume $X_1, \\ldots, X_n$ are i.i.d. $\\sim N(\\mu,\\sigma^2)$ and $\\sigma$ is known. Then we have an exact distributional result for a standardized sample mean,\n$$ \\begin{aligned} \\frac{\\overline{X}_n - \\mu}{\\sigma/\\sqrt{n}} \\sim N(0,1). \\end{aligned} $$\nLet us denote $z_{ 1-\\frac{\\alpha}{2}}$ to be $(1-\\frac{\\alpha}{2})$-th quantile of standard normal distribution $N(0,1)$. Since $N(0,1)$ is symmetric around $0$, we have $z_{\\frac{\\alpha}{2}} = -z_{1-\\frac{\\alpha}{2}}$ and we can write\n$$ \\begin{aligned} 1-\\alpha = P\\left(-z_{1-\\frac{\\alpha}{2}} \\leq \\frac{\\overline{X}_n-\\mu}{\\sigma/\\sqrt{n}} \\leq z_{1-\\frac{\\alpha}{2}} \\right) = P\\left(\\bar{X}_n-z_{1-\\frac{\\alpha}{2}} \\frac{\\sigma}{\\sqrt{n}} \\leq \\mu \\leq \\bar{X}_n+z_{1-\\frac{\\alpha}{2}} \\frac{\\sigma}{\\sqrt{n}} \\right), \\end{aligned} $$\nwhich yields that $\\left[ \\bar{X}_n-z_{1-\\frac{\\alpha}{2}} \\frac{\\sigma}{\\sqrt{n}}, ; \\bar{X}_n+z_{1-\\frac{\\alpha}{2}} \\frac{\\sigma}{\\sqrt{n}}\\right]$ is a $(1-\\alpha )$-confidence interval for a mean parameter $\\mu$.\nExample 2: constructing confidence interval for $\\mu$ with $t$-quantiles Assume $X_1, \\ldots, X_n$ are i.i.d. $\\sim N(\\mu,\\sigma^2)$ and $\\sigma$ is unknown. We use $S_n$ \u0026ndash; a consistent sample estimator of true standard deviation \u0026ndash; to approximate $\\sigma$, and have an exact distributional result for a standardized sample mean,\n$$ \\begin{aligned} \\frac{\\overline{X}_n - \\mu}{S_n/\\sqrt{n}} \\sim t_{n-1}. \\end{aligned} $$\nLet us denote $t_{n-1,1-\\frac{\\alpha}{2}}$ to be a $(n-1,1-\\frac{\\alpha}{2})$-th quantile of $t$-distributuon with $n-1$ degrees of freedom. Since $t$ is symmetric around $0$, we have\n$$ \\begin{aligned} 1-\\alpha =P\\left(-t_{n-1,1-\\frac{\\alpha}{2}} \\leq \\frac{\\bar{X}_{n}-\\mu}{S_{n} / \\sqrt{n}} \\leq t_{n-1,1-\\frac{\\alpha}{2}}\\right) = P\\left(\\bar{X}_n-t_{n-1, 1-\\frac{\\alpha}{2}}\\frac{S_n}{\\sqrt{n}} \\leq \\mu \\leq \\bar{X}_n+t_{n-1, 1-\\frac{\\alpha}{2}} \\frac{S_n}{\\sqrt{n}} \\right), \\end{aligned} $$\nwhich yields that $\\left[ \\bar{X}_n-t_{n-1, 1-\\frac{\\alpha}{2}} \\frac{S_n}{\\sqrt{n}}, ; \\bar{X}_n+t_{n-1, 1-\\frac{\\alpha}{2}} \\frac{S_n}{\\sqrt{n}}\\right]$ is a $(1-\\alpha )$-confidence interval for a mean parameter $\\mu$.\nCases In many cases, whether to use quantiles of $t$-student distribution versus standard normal distribution is based on:\n distribution of $X_1, \\ldots, X_n$ variables, whether $\\sigma$ is known or not (and we need to estimate it i.e. with $S_n$), what is sample size $n$.  Note: the below cases could be directly translated to answer when to use $t$-test versus $z$-test in testing hypothesis about the mean $\\mu$ parameter, i.e. test for $H_0: \\mu = \\mu_0$ versus $H_1: \\mu \u0026lt; \\mu_0$, or $H_1: \\mu \\neq \\mu_0$, or $H_1: \\mu \u0026gt; \\mu_0$.\nCase 1: observations from normal distribution, $\\sigma$ known, any $n$  Observations $X_1, \\ldots, X_n$ are from normal $N(\\mu, \\sigma^2)$ distribution. $\\sigma$ known. Any sample size $n$.  $\\Rightarrow$ We have exact result that $\\frac{\\bar{X}_{n}-\\mu}{\\sigma / \\sqrt{n}} \\sim N(0,1)$ and hence we use quantiles of normal distribution in constructing the CI.\nCase 2: observations from normal distribution, $\\sigma$ unknown, small $n$  Observations $X_1, \\ldots, X_n$ are from normal $N(\\mu, \\sigma^2)$ distribution. $\\sigma$ unknown. Small sample size ($n \\leq 50$).  $\\Rightarrow$ We use $S_{n}$ to approximate $\\sigma$. We have exact result that $\\frac{\\bar{X}_{n}-\\mu}{S_{n} / \\sqrt{n}} \\sim t_{n-1}$ and hence we use quantiles of $t$ distribution with $n-1$ degrees of freedom in constructing the CI.\nCase 3: observations from normal distribution, $\\sigma$ unknown, large $n$  Observations $X_1, \\ldots, X_n$ are from normal $N(\\mu, \\sigma^2)$ distribution. $\\sigma$ unknown. Moderate to large sample size ($n \u0026gt; 50$).  $\\Rightarrow$ We use $S_{n}$ to approximate $\\sigma$. Because of $n$ large enough, Slutsky\u0026rsquo;s theorem asymptotic ``kicks in'' and allows to replace $\\sigma$ with $S_n$ \u0026ndash; a consistent estimator of true population standard deviation, and to write that $\\frac{\\bar{X}_{n}-\\mu}{S_n / \\sqrt{n}} \\approx \\sim N(0,1)$. Because of $n$ large enough, we assume $N(0,1)$ is approximated ($\\approx$) well enough to use quantiles of normal distribution in constructing the CI.\n$\\Rightarrow$ Another way to think about this case is that, as in Case 2, we have an exact result that $\\frac{\\bar{X}_{n}-\\mu}{S_{n} / \\sqrt{n}} \\sim t_{n-1}$, and with large $n$, quantiles of $t$-distribution with $n-1$ degrees of freedom are almost equvalent to quantiles of normal distribution.\nCase 4: observations from any distribution, $\\sigma$ known, small $n$  Observations $X_1, \\ldots, X_n$ are from (other than normal) distribution of mean $E(X_i) = \\mu$ and variance $\\text{var}(X_i) = \\sigma^2$ (for normally distributed $X_i$\u0026rsquo;s, see cases 1-3). $\\sigma$ known. Small sample size ($n \\leq 50$).  $\\Rightarrow$ Use CLT to get that standardized sample mean is approximately normal, $\\frac{\\bar{X}_{n}-\\mu}{\\sigma / \\sqrt{n}} \\approx \\sim N(0,1)$. Since there is CLT approximation and we have a small sample size, in practice, we typically use quantiles of $t$ distribution with $n-1$ degrees of freedom to get more conservative (wider) CI.\n Note: when $X_1, \\ldots, X_n$ distribution of is very skewed (i.e. Poisson) it may be not plausible that CLT already ``kicks in'' and other techniques may be needed.  Case 5: observations from any distribution, $\\sigma$ known, large $n$  Observations $X_1, \\ldots, X_n$ are from (other than normal) distribution of mean $E(X_i) = \\mu$ and variance $\\text{var}(X_i) = \\sigma^2$ (for normally distributed $X_i$\u0026rsquo;s, see cases 1-3). $\\sigma$ known. Moderate to large sample size ($n \u0026gt; 50$).  $\\Rightarrow$ Use CLT to get that standardized sample mean is approximately normal. We have $\\frac{\\bar{X}_{n}-\\mu}{\\sigma/ \\sqrt{n}} \\approx \\sim N(0,1)$. Since we have a moderate to small sample size, we assume that CLT ``kicks in'' and the approximation ($\\approx$) is good enough to use quantiles of normal distribution.\nCase 6: observations from any distribution, $\\sigma$ unknown, small $n$  Observations $X_1, \\ldots, X_n$ are from (other than normal) distribution of mean $E(X_i) = \\mu$ and variance $\\text{var}(X_i) = \\sigma^2$ (for normally distributed $X_i$\u0026rsquo;s, see cases 1-3). $\\sigma$ unknown. Small sample size ($n \\leq 50$).  $\\Rightarrow$ Use CLT to get that standardized sample mean is approximately normal and we also use Slutsky\u0026rsquo;s theorem to replace $\\sigma$ with $S_n$ \u0026ndash; a consistent estimator of true population standard deviation. We have $\\frac{\\bar{X}_{n}-\\mu}{S_n/ \\sqrt{n}} \\approx \\sim N(0,1)$. Since there is CLT and Slutsky\u0026rsquo;s theorem approximation and we have a small sample size, in practice, we typically use quantiles of $t$ distribution with $n-1$ degrees of freedom to get more conservative (wider) CI.\n Note: two approximations are happening here! Note: when $X_1, \\ldots, X_n$ distribution of is very skewed (i.e. Poisson) it may be not plausible that CLT already ``kicks in'' and other techniques may be needed.  Case 7: observations from any distribution, $\\sigma$ unknown, large $n$  Observations $X_1, \\ldots, X_n$ are from (other than normal) distribution of mean $E(X_i) = \\mu$ and variance $\\text{var}(X_i) = \\sigma^2$ (for normally distributed $X_i$\u0026rsquo;s, see cases 1-3). $\\sigma$ unknown. Moderate to large sample size ($n \u0026gt; 50$).  $\\Rightarrow$ Use CLT to get that standardized sample mean is approximately normal and we also use Slutsky\u0026rsquo;s theorem to replace $\\sigma$ with $S_n$ \u0026ndash; a consistent estimator of true population standard deviation. We have $\\frac{\\bar{X}_{n}-\\mu}{S_n/ \\sqrt{n}} \\approx \\sim N(0,1)$. Since we have a moderate to small sample size, we assume that both CLT and Slutsky asymptotics ``kicks in'' and the approximation ($\\approx$) is good enough to use quantiles of normal distribution.\nDisclaimer The views, thoughts, and opinions expressed in the text belong solely to the author, and not necessarily to the author’s employer, organization, committee or other group or individual.\nReferences [1]: Methods in Biostatistics with R. Ciprian Crainiceanu, Brian Caffo, John Muschelli (2019). Available online at https://leanpub.com/biostatmethods.\n","date":1572297568,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1572297568,"objectID":"d1bdde96b4cf824c5001d198319f10e4","permalink":"/post/2019-10-28-ttest-versus-ztest/","publishdate":"2019-10-28T17:19:28-04:00","relpermalink":"/post/2019-10-28-ttest-versus-ztest/","section":"post","summary":"To construct confidence interval for the mean, we often use quantiles of standardized sample mean distribution. Here, I include a list of cases where I\u0026rsquo;d use quantiles of t-distribution versus quantiles of normal distribution for that purpose.\nNote: the below text could be directly translated to answer when to use t-test versus z-test in testing hypothesis about the mean parameter.\n","tags":[],"title":"When to use t distribution versus normal distribution quantiles in constructing confidence interval for the mean","type":"post"},{"authors":null,"categories":null,"content":"Slides link.\n","date":1571156100,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1571156100,"objectID":"d85886c05315dfe317f7be3978a2ffbf","permalink":"/talk/2019-10-15-journal-club-wit/","publishdate":"2019-10-15T12:15:00-04:00","relpermalink":"/talk/2019-10-15-journal-club-wit/","section":"talk","summary":"Slides link.","tags":[],"title":"Overview of JHU Biostat Wearable and Implantable Technology (WIT) working group research","type":"talk"},{"authors":null,"categories":null,"content":"","date":1570542000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570542000,"objectID":"104e27d7884e261e298cccc597e71a30","permalink":"/talk/2019-10-06-novartis-internal/","publishdate":"2019-10-08T09:40:00-04:00","relpermalink":"/talk/2019-10-06-novartis-internal/","section":"talk","summary":"","tags":[],"title":"Walking measurements derived from free-living wrist-worn sensor as novel digital endpoints","type":"talk"},{"authors":null,"categories":[],"content":"Materials:\n  Oct 1, 2019: Introduction to raw accelerometry data. Introduction to project raw accelerometry data set\n  Oct 17, 2019: Time-series processing and pattern segmentation methods for high-dimensional time-series\n  Oct 24, 2019: my class project slides and code\n  ","date":1569885443,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569885443,"objectID":"6491c2747f541caff1dca2bcc199b9b6","permalink":"/resources/specialtopics/","publishdate":"2019-09-30T19:17:23-04:00","relpermalink":"/resources/specialtopics/","section":"resources","summary":"Materials:\n  Oct 1, 2019: Introduction to raw accelerometry data. Introduction to project raw accelerometry data set\n  Oct 17, 2019: Time-series processing and pattern segmentation methods for high-dimensional time-series","tags":[],"title":"Special Topics course 2018-19: Biostatistical Methods for Wearable Computing","type":"resources"},{"authors":["Karas","M.","Straczkiewicz","M.","Fadel","W.","Harezlak","J.","Crainiceanu","C.M.","Urbanek","J.K."],"categories":[],"content":"","date":1569276518,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569276518,"objectID":"ba77d5db09d0ef5af4b9ace242e40ff5","permalink":"/publication/adept/","publishdate":"2019-09-23T18:08:38-04:00","relpermalink":"/publication/adept/","section":"publication","summary":"Quantifying gait parameters and ambulatory monitoring of changes in these parameters have become increasingly important in epidemiological and clinical studies. Using high-density accelerometry measurements, we propose adaptive empirical pattern transformation (ADEPT), a fast, scalable, and accurate method for segmentation of individual walking strides. ADEPT computes the covariance between a scaled and translated pattern function and the data, an idea similar to the continuous wavelet transform. The difference is that ADEPT uses a data-based pattern function, allows multiple pattern functions, can use other distances instead of the covariance, and the pattern function is not required to satisfy the wavelet admissibility condition. Compared to many existing approaches, ADEPT is designed to work with data collected at various body locations and is invariant to the direction of accelerometer axes relative to body orientation. The method is applied to and validated on accelerometry data collected during a 450-m outdoor walk of 32 study participants wearing accelerometers on the wrist, hip, and both ankles. Additionally, all scripts and data needed to reproduce presented results are included in supplementary material available at Biostatistics online.","tags":["wearable accelerometers","physical activity","actigraphy","methods","pattern segmentation","gait","walking"],"title":"Adaptive empirical pattern transformation (ADEPT) with application to walking stride segmentation","type":"publication"},{"authors":null,"categories":null,"content":"","date":1561575900,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1561575900,"objectID":"63a064d9d86e0f182560a2652f61ce98","permalink":"/talk/2019-06-26-icampam/","publishdate":"2019-06-26T15:05:00-04:00","relpermalink":"/talk/2019-06-26-icampam/","section":"talk","summary":"Precise estimation of gait characteristics has significant potential in health research and can help guide clinical decision making. Reproducible estimation of walking strides can provide useful, precise, and time-varying estimators of the number of steps, cadence, stride and step asymmetry. We propose a novel stride segmentation approach and quantify its effectiveness in the estimation of gait and gait asymmetry. The method consists of a first phase, where a flexible pattern matching approach, called ADaptive Empirical Pattern Transformation (ADEPT), is used to conduct stride segmentation. A parametric nonlinear transformation of time is then used to characterize the level of asymmetry in strides relative to a reference template. The advantage of the approach is that its second phase allows quantification of walking asymmetry in one parameter that is directly related to the speeding and slowing down of one of the treadmill belts, thus providing a mechanistic interpretation of the process. Methods are applied to data collected in an experiment where a healthy adult walked on a split-belt treadmill under different conditions, including gait speed of 1.0, 1.25, 1.56 and 2.0 m/s as well as right foot ahead of left and left foot ahead of right (total N=671 walking strides.) Accelerometry data were collected at 100 Hz simultaneously by three wearable sensors located on the left wrist, left hip and left ankle. The Optotrak motion capture system provided the gold-standard reference for gait characteristics. Results indicate that: (1) the time-warped version of ADEPT estimates of stride cadence agree closely with the gold-standard (mean absolute percentage error was 1.67%, 1.7%, and 4.34% for left ankle, left hip, and left wrist, respectively); (2) the time-warping parameter is highly correlated with the known, imposed stride-asymmetry. Results suggest that the one-parameter characterization of time-warping is a useful and accurate representation of stride asymmetry.","tags":[],"title":"Automatic estimation of step asymmetry in a split-belt treadmill experiment using high-resolution accelerometry data","type":"talk"},{"authors":null,"categories":null,"content":"Slides from the talk part about RStudio 2019 conference can be found here.\n","date":1558648800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1558648800,"objectID":"dd8c067e58bf68bfff207a370090a1e6","permalink":"/talk/2019-05-23-stwur-meeting-talk/","publishdate":"2019-05-23T18:00:00-04:00","relpermalink":"/talk/2019-05-23-stwur-meeting-talk/","section":"talk","summary":"The talk is oriented at general audience and does not assume familiarity with any statistical methods (a total of one equation may feature in the slides; basically, I plan to craft it in a way my parents may listen and get it!). I plan to split my presentation into two parts. In the first part, I want to share about my current PhD program research, that is, developing methods to understand and quantify physical activity based on data collected with wearable sensors. Information from such devices is important as it can provide researchers with objective information about physical activity in large epidemiological studies or clinical trials (compared to, for example, bias-prone information from questionnaires). Specifically, we can leverage subsecond-level accelerometry data to characterize human gait. In one of my current projects, we focus on estimating gait pattern in post-stroke patients; in another project, I am working at methods for segmenting walking bouts from days, or weeks, of sensor monitoring data. In the second part of my presentation, I want to share about my personal favorites seen at RStudio conference I attended in Austin, TX in January 2019. ","tags":[],"title":"Physical activity monitoring with wearable accelerometers: strides segmentation, gait pattern estimation and walking detection from free-living data in R. Also: highlights from RStudio 2019 conference.","type":"talk"},{"authors":null,"categories":null,"content":"Some picture from the talk was posted here.\n","date":1557838800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1557838800,"objectID":"0d8a48ac02118a79868f802be4a4fbe7","permalink":"/talk/2019-05-14-motion-lab-talk/","publishdate":"2019-05-14T09:00:00-04:00","relpermalink":"/talk/2019-05-14-motion-lab-talk/","section":"talk","summary":"Some picture from the talk was posted here.","tags":[],"title":"Stride segmentation and stride pattern estimation based on accelerometry data","type":"talk"},{"authors":[],"categories":[],"content":"Package adeptdata was created to host raw accelerometry data sets and their derivatives. Some of them are used in the corresponding adept package.\nPackage CRAN index is located here. Package GitHub repo is located here.\nTable of Contents  Outdoor continuous walking raw accelerometry data acc_walking_IU Outdoor run raw accelerometry data acc_running Walking stride accelerometry data templates stride_template   Installation Install from CRAN.\ninstall.packages(\u0026quot;adeptdata\u0026quot;)  Data objects Outdoor continuous walking raw accelerometry data acc_walking_IU acc_walking_IU is a sample of raw accelerometry data collected during outdoor continuous walking from 32 healthy participants between 23 and 52 years of age. Data were collected at frequency 100 Hz simultaneously with four wearable accelerometers located at left wrist, left hip and both ankles. See ?acc_walking_IU for details.\nlibrary(adeptdata) library(dplyr) library(ggplot2) library(reshape2) library(lubridate) acc_walking_IU %\u0026gt;% filter(time_s \u0026lt; 6, subj_id == acc_walking_IU$subj_id[1]) %\u0026gt;% mutate(loc_id = factor( loc_id, levels = c(\u0026quot;left_wrist\u0026quot;, \u0026quot;left_hip\u0026quot;, \u0026quot;left_ankle\u0026quot;, \u0026quot;right_ankle\u0026quot;), labels = c(\u0026quot;Left wrist\u0026quot;, \u0026quot;Left hip\u0026quot;, \u0026quot;Left ankle\u0026quot;, \u0026quot;Right ankle\u0026quot;))) %\u0026gt;% melt(id.vars = c(\u0026quot;subj_id\u0026quot;, \u0026quot;loc_id\u0026quot;, \u0026quot;time_s\u0026quot;)) %\u0026gt;% ggplot(aes(x = time_s, y = value, color = variable)) + geom_line() + facet_wrap(~ loc_id, ncol = 2) + theme_bw(base_size = 9) + labs(x = \u0026quot;Exercise time [s]\u0026quot;, y = \u0026quot;Amplitude [g]\u0026quot;, color = \u0026quot;Sensor\\naxis\u0026quot;, title = \u0026quot;Raw accelerometry data of walking (100 Hz)\u0026quot;)  Outdoor run raw accelerometry data acc_running acc_running is a sample raw accelerometry data collected during 25 minutes of an outdoor run. Data were collected at frequency 100 Hz with two ActiGraph GT9X Link sensors located at left hip and left ankle. See ?acc_running for details.\nt1 \u0026lt;- ymd_hms(\u0026quot;2018-10-25 18:07:00\u0026quot;, tz = \u0026quot;UTC\u0026quot;) t2 \u0026lt;- ymd_hms(\u0026quot;2018-10-25 18:20:30\u0026quot;, tz = \u0026quot;UTC\u0026quot;) t3 \u0026lt;- ymd_hms(\u0026quot;2018-10-25 18:22:00\u0026quot;, tz = \u0026quot;UTC\u0026quot;) acc_running %\u0026gt;% filter((date_time \u0026gt;= t1 \u0026amp; date_time \u0026lt; t1 + as.period(4, \u0026quot;seconds\u0026quot;)) | (date_time \u0026gt;= t2 \u0026amp; date_time \u0026lt; t2 + as.period(4, \u0026quot;seconds\u0026quot;)) | (date_time \u0026gt;= t3 \u0026amp; date_time \u0026lt; t3 + as.period(4, \u0026quot;seconds\u0026quot;)) ) %\u0026gt;% mutate(loc_id = factor( loc_id, levels = c(\u0026quot;left_hip\u0026quot;, \u0026quot;left_ankle\u0026quot;), labels = c(\u0026quot;Left hip\u0026quot;, \u0026quot;Left ankle\u0026quot;))) %\u0026gt;% melt(id.vars = c(\u0026quot;date_time\u0026quot;, \u0026quot;loc_id\u0026quot;)) %\u0026gt;% mutate(date_time_floor = paste0( \u0026quot;Minute start: \u0026quot;, floor_date(date_time, unit = \u0026quot;minutes\u0026quot;))) %\u0026gt;% ggplot(aes(x = date_time, y = value, color = variable)) + geom_line(size = 0.5) + facet_grid(loc_id ~ date_time_floor, scales = \u0026quot;free_x\u0026quot;) + theme_bw(base_size = 9) + labs(x = \u0026quot;Time [s]\u0026quot;, y = \u0026quot;Acceleration [g]\u0026quot;, color = \u0026quot;Sensor\\naxis\u0026quot;, title = \u0026quot;Raw accelerometry data (100 Hz)\u0026quot;)  Walking stride accelerometry data templates stride_template stride_template is a list containing walking stride pattern templates derived from accelerometry data collected at four body locations: left wrist, left hip, left ankle, and right ankle. See ?stride_template for details.\ndata.frame( x = rep(seq(0, 1, length.out = 200), 2), y = c(stride_template$left_ankle[[2]][1, ], stride_template$left_ankle[[2]][2, ]), group = c(rep(1, 200), rep(2, 200))) %\u0026gt;% ggplot(aes(x = x, y = y, group = group)) + geom_line() + facet_grid(group ~ .) + theme_bw(base_size = 9) + labs(x = \u0026quot;Time [s]\u0026quot;, y = \u0026quot;Vector magnitude [g]\u0026quot;, title = \u0026quot;Walking stride templates (left ankle)\u0026quot;)  ","date":1555948493,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555948493,"objectID":"b1d8c0a633675ed94fd36f3871538599","permalink":"/project/project_adeptdata/","publishdate":"2019-04-22T11:54:53-04:00","relpermalink":"/project/project_adeptdata/","section":"project","summary":"Package adeptdata was created to host raw accelerometry data sets and their derivatives.","tags":["rstats","data set","data","wearable accelerometers","actigraphy"],"title":"'adeptdata' R package: Raw Accelerometry Data Sets and Their Derivatives","type":"project"},{"authors":null,"categories":null,"content":"","date":1555518650,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555518650,"objectID":"edf979b1d9c0986f5c6a6c1d8e90b72e","permalink":"/talk/2019-04-16-adaptive-trials-semminar/","publishdate":"2019-04-17T12:30:50-04:00","relpermalink":"/talk/2019-04-16-adaptive-trials-semminar/","section":"talk","summary":"","tags":[],"title":"Simulation experiment project for Seminar on Adaptive Clinical Trials","type":"talk"},{"authors":[],"categories":[],"content":"Package runstats provides methods for fast computation of running sample statistics for time series. The methods utilize Convolution Theorem to compute convolutions via Fast Fourier Transform (FFT). Implemented running statistics include:\n mean, standard deviation, variance, covariance, correlation, euclidean distance.  Table of Contents  Compare RunningCov {runstats} with a conventional method Compare RunningCov {runstats} with sliding_cov {dvmisc} c++ implementation Session info   Website Package website is located here.\nInstallation install.packages(\u0026quot;runstats\u0026quot;)  Usage library(runstats) ## Example: running correlation x0 \u0026lt;- sin(seq(0, 2 * pi * 5, length.out = 1000)) x \u0026lt;- x0 + rnorm(1000, sd = 0.1) pattern \u0026lt;- x0[1:100] out1 \u0026lt;- RunningCor(x, pattern) out2 \u0026lt;- RunningCor(x, pattern, circular = TRUE) ## Example: running mean x \u0026lt;- cumsum(rnorm(1000)) out1 \u0026lt;- RunningMean(x, W = 100) out2 \u0026lt;- RunningMean(x, W = 100, circular = TRUE)  Running statistics To better explain the details of running statistics, package\u0026rsquo;s function runstats.demo(func.name) allows to visualize how the output of each running statistics method is generated. To run the demo, use func.name being one of the methods' names:\n \u0026quot;RunningMean\u0026quot;, \u0026quot;RunningSd\u0026quot;, \u0026quot;RunningVar\u0026quot;, \u0026quot;RunningCov\u0026quot;, \u0026quot;RunningCor\u0026quot;, \u0026quot;RunningL2Norm\u0026quot;.  ## Example: demo for running correlation method runstats.demo(\u0026quot;RunningCor\u0026quot;)  ## Example: demo for running mean method runstats.demo(\u0026quot;RunningMean\u0026quot;)  Performance We use rbenchmark to measure elapsed time of RunningCov execution, for different lengths of time-series x and fixed length of the shorter pattern y.\nlibrary(rbenchmark) library(ggplot2) set.seed (20190315) x.N.seq \u0026lt;- 10^(3:7) x.list \u0026lt;- lapply(x.N.seq, function(N) runif(N)) y \u0026lt;- runif(100) ## Benchmark execution time of RunningCov out.df \u0026lt;- data.frame() for (x.tmp in x.list){ out.df.tmp \u0026lt;- benchmark( \u0026quot;runstats\u0026quot; = runstats::RunningCov(x.tmp, y), replications = 10, columns = c(\u0026quot;test\u0026quot;, \u0026quot;replications\u0026quot;, \u0026quot;elapsed\u0026quot;, \u0026quot;relative\u0026quot;, \u0026quot;user.self\u0026quot;, \u0026quot;sys.self\u0026quot;)) out.df.tmp$x_length \u0026lt;- length(x.tmp) out.df.tmp$pattern_length \u0026lt;- length(y) out.df \u0026lt;- rbind(out.df, out.df.tmp) }  knitr::kable(out.df)    test replications elapsed relative user.self sys.self x_length pattern_length    runstats 10 0.004 1 0.003 0.000 1000 100  runstats 10 0.023 1 0.019 0.004 10000 100  runstats 10 0.183 1 0.148 0.035 100000 100  runstats 10 1.700 1 1.592 0.107 1000000 100  runstats 10 19.852 1 17.185 2.576 10000000 100    Compare RunningCov {runstats} with a conventional method To compare runstats performance with \u0026ldquo;conventional\u0026rdquo; loop-based way of computing running covariance in R, we use rbenchmark package to measure elapsed time of runstats::RunningCov and running covariance implemented with sapply loop, for different lengths of time-series x and fixed length of the shorter time-series y.\n## Conventional approach RunningCov.sapply \u0026lt;- function(x, y){ l_x \u0026lt;- length(x) l_y \u0026lt;- length(y) sapply(1:(l_x - l_y + 1), function(i){ cov(x[i:(i+l_y-1)], y) }) } out.df2 \u0026lt;- data.frame() for (x.tmp in x.list[c(1:4)]){ out.df.tmp \u0026lt;- benchmark( \u0026quot;conventional\u0026quot; = RunningCov.sapply(x.tmp, y), \u0026quot;runstats\u0026quot; = runstats::RunningCov(x.tmp, y), replications = 10, columns = c(\u0026quot;test\u0026quot;, \u0026quot;replications\u0026quot;, \u0026quot;elapsed\u0026quot;, \u0026quot;relative\u0026quot;, \u0026quot;user.self\u0026quot;, \u0026quot;sys.self\u0026quot;)) out.df.tmp$x_length \u0026lt;- length(x.tmp) out.df2 \u0026lt;- rbind(out.df2, out.df.tmp) }  Benchmark results\nplt1 \u0026lt;- ggplot(out.df2, aes(x = x_length, y = elapsed, color = test)) + geom_line() + geom_point(size = 3) + scale_x_log10() + theme_minimal(base_size = 14) + labs(x = \u0026quot;Vector length of x\u0026quot;, y = \u0026quot;Elapsed [s]\u0026quot;, color = \u0026quot;Method\u0026quot;, title = \u0026quot;Running covariance (x,y) rbenchmark\u0026quot;, subtitle = \u0026quot;Vector length of y = 100\u0026quot;) + theme(legend.position = \u0026quot;bottom\u0026quot;) plt2 \u0026lt;- plt1 + scale_y_log10() + labs(y = \u0026quot;Log of elapsed [s]\u0026quot;, title = \u0026quot;\u0026quot;) cowplot::plot_grid(plt1, plt2, nrow = 1, labels = c('A', 'B'))  Compare RunningCov {runstats} with sliding_cov {dvmisc} c++ implementation dvmisc package (GitHub, CRAN) is a package for Convenience Functions, Moving Window Statistics, and Graphics, and includes functions for calculating moving-window statistics efficiently via c++, written by Dane Van Domelen. Here, we compare RunningCov {runstats} performance with c++ implementation from sliding_cov {dvmisc}. Dane contributed the code in its large part.\n# devtools::install_github(\u0026quot;vandomed/dvmisc\u0026quot;) library(dvmisc) set.seed(20100315) x.N.seq \u0026lt;- 10^(3:6) x.list \u0026lt;- lapply(x.N.seq, function(N) runif(N)) get.out.df \u0026lt;- function(y){ out.df \u0026lt;- data.frame() for (x.tmp in x.list){ if (length(x.tmp) \u0026lt; length(y)){ out.df.tmp \u0026lt;- data.frame( test = NA, replications = NA, elapsed = NA, relative = NA, user.self = NA, sys.self = NA) } else { out.df.tmp \u0026lt;- benchmark( \u0026quot;runstats\u0026quot; = runstats::RunningCov(x.tmp, y), \u0026quot;dvmisc\u0026quot; = dvmisc::sliding_cov(y, x.tmp), replications = 10, columns = c(\u0026quot;test\u0026quot;, \u0026quot;replications\u0026quot;, \u0026quot;elapsed\u0026quot;, \u0026quot;relative\u0026quot;, \u0026quot;user.self\u0026quot;, \u0026quot;sys.self\u0026quot;)) } out.df.tmp$x_length \u0026lt;- length(x.tmp) out.df \u0026lt;- rbind(out.df, out.df.tmp) } return(out.df) } out.df_y10 \u0026lt;- get.out.df(runif(10^1)) out.df_y100 \u0026lt;- get.out.df(runif(10^2)) out.df_y1000 \u0026lt;- get.out.df(runif(10^3)) out.df_y10000 \u0026lt;- get.out.df(runif(10^4))  Benchmark results\nget.plt \u0026lt;- function(data, subtitle){ ggplot(data, aes(x = x_length, y = elapsed, color = test)) + geom_line() + geom_point(size = 3) + scale_x_log10() + theme_minimal(base_size = 14) + scale_y_log10() + labs(x = \u0026quot;Vector length of x\u0026quot;, y = \u0026quot;Log of elapsed [s]\u0026quot;, color = \u0026quot;Method\u0026quot;, subtitle = subtitle) + theme(legend.position = \u0026quot;bottom\u0026quot;) } plt1 \u0026lt;- get.plt(out.df_y10, \u0026quot;Vector length of y = 10\u0026quot;) + labs(title = \u0026quot;Running covariance (x,y) rbenchmark\u0026quot;) plt2 \u0026lt;- get.plt(out.df_y100, \u0026quot;Vector length of y = 100\u0026quot;) plt3 \u0026lt;- get.plt(out.df_y1000, \u0026quot;Vector length of y = 1,000\u0026quot;) plt4 \u0026lt;- get.plt(out.df_y10000, \u0026quot;Vector length of y = 1,0000\u0026quot;) cowplot::plot_grid(plt1, plt2, plt3, plt4, nrow = 2, labels = c('A', 'B', 'C', 'D'))  Session info sessioninfo::session_info()   ## ─ Session info ─────────────────────────────────────────────────────────────── ## setting value ## version R version 3.5.2 (2018-12-20) ## os macOS Mojave 10.14.2 ## system x86_64, darwin15.6.0 ## ui X11 ## language (EN) ## collate en_US.UTF-8 ## ctype en_US.UTF-8 ## tz America/New_York ## date 2019-11-14 ## ## ─ Packages ─────────────────────────────────────────────────────────────────── ## package * version date lib source ## assertthat 0.2.1 2019-03-21 [1] CRAN (R 3.5.2) ## cli 1.1.0 2019-03-19 [1] CRAN (R 3.5.2) ## colorspace 1.4-1 2019-03-18 [1] CRAN (R 3.5.2) ## crayon 1.3.4 2017-09-16 [1] CRAN (R 3.5.0) ## digest 0.6.22 2019-10-21 [1] CRAN (R 3.5.2) ## dplyr 0.8.3 2019-07-04 [1] CRAN (R 3.5.2) ## evaluate 0.14 2019-05-28 [1] CRAN (R 3.5.2) ## fftwtools 0.9-8 2017-03-25 [1] CRAN (R 3.5.0) ## ggplot2 * 3.2.1 2019-08-10 [1] CRAN (R 3.5.2) ## glue 1.3.1 2019-03-12 [1] CRAN (R 3.5.2) ## gtable 0.3.0 2019-03-25 [1] CRAN (R 3.5.2) ## htmltools 0.3.6 2017-04-28 [1] CRAN (R 3.5.0) ## knitr 1.26 2019-11-12 [1] CRAN (R 3.5.2) ## lazyeval 0.2.2 2019-03-15 [1] CRAN (R 3.5.2) ## magrittr 1.5 2014-11-22 [1] CRAN (R 3.5.0) ## munsell 0.5.0 2018-06-12 [1] CRAN (R 3.5.0) ## pillar 1.4.2 2019-06-29 [1] CRAN (R 3.5.2) ## pkgconfig 2.0.3 2019-09-22 [1] CRAN (R 3.5.2) ## purrr 0.3.3 2019-10-18 [1] CRAN (R 3.5.2) ## R6 2.4.1 2019-11-12 [1] CRAN (R 3.5.2) ## rbenchmark * 1.0.0 2012-08-30 [1] CRAN (R 3.5.0) ## Rcpp 1.0.3 2019-11-08 [1] CRAN (R 3.5.2) ## rlang 0.4.1 2019-10-24 [1] CRAN (R 3.5.2) ## rmarkdown 1.15 2019-08-21 [1] CRAN (R 3.5.2) ## rstudioapi 0.10 2019-03-19 [1] CRAN (R 3.5.2) ## runstats * 1.1.0 2019-11-14 [1] CRAN (R 3.5.2) ## scales 1.0.0 2018-08-09 [1] CRAN (R 3.5.0) ## sessioninfo 1.1.1 2018-11-05 [1] CRAN (R 3.5.0) ## stringi 1.4.3 2019-03-12 [1] CRAN (R 3.5.2) ## stringr 1.4.0 2019-02-10 [1] CRAN (R 3.5.2) ## tibble 2.1.3 2019-06-06 [1] CRAN (R 3.5.2) ## tidyselect 0.2.5 2018-10-11 [1] CRAN (R 3.5.0) ## withr 2.1.2 2018-03-15 [1] CRAN (R 3.5.0) ## xfun 0.11 2019-11-12 [1] CRAN (R 3.5.2) ## yaml 2.2.0 2018-07-25 [1] CRAN (R 3.5.0) ## ## [1] /Library/Frameworks/R.framework/Versions/3.5/Resources/library  ","date":1552665293,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1552665293,"objectID":"ea6382068256e38a65bee2ac1c0c71c5","permalink":"/project/project_runstats/","publishdate":"2019-03-15T11:54:53-04:00","relpermalink":"/project/project_runstats/","section":"project","summary":"Package runstats provides methods for fast computation of running sample statistics for time series via Fast Fourier Transform.","tags":["rstats","signal processing","methods"],"title":"'runstats' R package: Fast Computation of Running Statistics for Time Series","type":"project"},{"authors":null,"categories":null,"content":"Ben\u0026rsquo;s tweet thread with some pics from the talk: [link]\n","date":1549991750,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549991750,"objectID":"ebcc04aa3a40e3bdeeefc154957b4e54","permalink":"/talk/2019-02-12-computing_club/","publishdate":"2019-02-12T12:15:50-05:00","relpermalink":"/talk/2019-02-12-computing_club/","section":"talk","summary":"Ben\u0026rsquo;s tweet thread with some pics from the talk: [link]","tags":[],"title":"Some of the topics seen at RStudio conference 2019","type":"talk"},{"authors":[],"categories":[],"content":"Create slides in Markdown with Academic Academic | Documentation\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026quot;blueberry\u0026quot; if porridge == \u0026quot;blueberry\u0026quot;: print(\u0026quot;Eating...\u0026quot;)   Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}}  Press Space to play!\nOne  Two  Three \n A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}}  Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026quot;/media/boards.jpg\u0026quot; \u0026gt;}} {{\u0026lt; slide background-color=\u0026quot;#0000FF\u0026quot; \u0026gt;}} {{\u0026lt; slide class=\u0026quot;my-style\u0026quot; \u0026gt;}}   Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }   Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using Academic's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":null,"categories":[],"content":"I and Guoqing Wang hosted the Department of Biostatistics students Computing Club in 2018-19 academic year.\n  Computing Club is one-hour session which gives students opportunities to share and develop computing skills useful for biostatisticians. It is held every other Tuesday 12:15-1:15 PM at JH SPH. Importantly, food 🍕🍌 is provided in every session!\n  Past and upcoming topics and locations for 2018-19 academic year are listed below.\n     Date (Location) Topic Presenter     Sep 18, 2018 (E9519) Data science project workflow (slides) Ben Ackerman   Oct 16, 2018 (W2029) The JHPCE Cluster: What does it do? Does it do things? Let\u0026rsquo;s find out! (cluster-example GitHub repo) Jacob Fiksel   Nov 13, 2018 (E9519) R Packages for the first year PhD curriculum (useful for non-phd students too!) Lacey Etzkorn   Nov 27, 2018 (W4030) Speeding up R with Rcpp (talk GitHub repo, talk slides) Stephen Cristiano   Jan 29, 2019 (W2029) Intro to Deep Learning (slides) Guoqing Wang   Feb 12, 2019 (E9519) Some topics seen at RStudio 2019 conference (slides) Marta Karas   Feb 26, 2019 (W2029) Blogdown tutorial: how to make your own personal academic website! (abstract and slides link) Ben Ackerman   Apr 16, 2019 (W4030) Computational and statistical methods in single-cell genomics (slides) Zhicheng (Jason) Ji   Apr 30, 2019 (W4030) Simulation studies on the cluster, and parallel computing in R with limited memory (slides) Lamar Hunt      Links to the previous years' Computing Club could be found here:\n 2017-18: - 2016-17 2015-16 2014-15: - (overwritten by 2016-17) 2013-14 2012-13 2011-12 2010-11 2009-10 2008-09 2007-08 2006-07 2005-06    ","date":1548544643,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548544643,"objectID":"8bc599c7dc1a958a9e0c5e477da2ee80","permalink":"/resources/computing-club/","publishdate":"2019-01-26T19:17:23-04:00","relpermalink":"/resources/computing-club/","section":"resources","summary":"I and Guoqing Wang hosted the Department of Biostatistics students Computing Club in 2018-19 academic year.\n  Computing Club is one-hour session which gives students opportunities to share and develop computing skills useful for biostatisticians.","tags":[],"title":"JH Biostat Computing Club 2018-19","type":"resources"},{"authors":null,"categories":[],"content":"I spent the last days of 2018-19 winter break in Austin, TX where I traveled for RStudio 2019 conference. I attended e-poster session and two days of the conference: Jan 17-18. Below I list some of the topics from the talks that I particularly liked and I think I am likely to benefit from in the future.\nAlso:\n The official repo with abstracts for every session, workshop (together with workshop files free to download for most if not all of them), and e-poster can be accessed here.  Table of Contents  Data visualization   Tyler Morgan-Wall (website, twitter) showing stunning gifs and pics while presenting \u0026ldquo;3D mapping, plotting, and printing with rayshader\u0026rdquo;.\nFeatures showed include shadowing, rotating, water transparency, visualization of a simulation of different water levels. The talk had possibly the most exciting \u0026ldquo;future work\u0026rdquo; announced - because who doesn\u0026rsquo;t get excited about the idea of \u0026ldquo;rotating 3D ggplots\u0026rdquo;? 💘 The rayshader package is available on GitHub (link).\n       (The gif on the left and the image on the right are both sourced from \u0026lsquo;rayshader\u0026rsquo; package GitHub website (link), as accessed on Jan 26, 2019.)\n  Thomas Lin Pedersen (website, twitter) giving a \u0026ldquo;gganimate live cookbook\u0026rdquo; speech (and joining Tyler\u0026rsquo;s talk on the podium of most fun presentations, I guess 😂)\nThe gganimate package was introduced per \u0026ldquo;extension to ggplot2\u0026rdquo; providing \u0026ldquo;implementation of the grammar of animated graphics\u0026rdquo;. Presentation slides are available online here; IMHO worth checking out for inspiring ways of presenting data over time! Besides, https://gganimate.com comes with a bunch of examples, like the one below.\n   (The gif sourced from \u0026lsquo;gganimate\u0026rsquo; package website (link), as accessed on Jan 26, 2019.)\n  Documents building   Yihui Xie (website, twitter) leaving the audience very enthusiastic (or maybe more like: blown away) with all the recent development in document building.\nThe presentation starts with \u0026ldquo;In HTML and the Web I trust\u0026rdquo; (😍 I share like 99% of my work summaries with advisors and colleagues in a form HTML). With pagedown we can now go ahead and get paged HTML documents, e.g. business card, resume, poster. Fairly 👶 development (\u0026quot;status: experimental\u0026quot;). Presentation slides are available here.\n(Slides 10, 11, 15, 17 from RStudio 2019 conference talk \u0026ldquo;pagedown: Creating Beautiful PDFs with R Markdown + CSS + Your Web Browser\u0026rdquo; by Yihui Xie and Romain Lesur on Jan 18, 2019 in Austin, TX.)\n  Rich Iannone (website, twitter) introducing the gt package.\nWith gt package (link), one can turn a data table into \u0026ldquo;information-rich, publication-quality\u0026rdquo; 🎯 table outputs. The table outputs can be in HTML, LaTeX, and RTF. The \u0026ldquo;modular\u0026rdquo; way of building these reminds me of ggplot2 plots construction. Presentation slides are available here.\n(Slide 5. from RStudio 2019 conference talk \u0026ldquo;Introducing the \u0026lsquo;gt\u0026rsquo; package\u0026rdquo; by Rich Iannone on Jan 18, 2019 in Austin, TX.)\n  Community and personal development   David Robinson (website, twitter) delivering a keynote talk \u0026ldquo;The unreasonable effectiveness of public work\u0026rdquo; (slides).\nIMHO a phenomenal speech: a kind and resonating talk after which one not only wants to stand up and change the world right now 🔥 but also maintains the same feeling a week after 💪. Review the slides to: (a) learn Author\u0026rsquo;s points on why it so worth it to spend time on public work, (b) for a pack of actual how-to examples and guidelines on building a public portfolio, (c) for a bunch of interesting points made about a value of work (Author\u0026rsquo;s work, but fairly generalizable IMHO); my favorite is copy-pasted below! I particularly appreciated that all stages of advancement in building online portfolio were addressed, 👶-steps including!\n(Slide 63. from RStudio 2019 conference keynote talk \u0026ldquo;The unreasonable effectiveness of public work\u0026rdquo; by David Robinson on Jan 18, 2019 in Austin, TX.)\n  Jesse Mostipak (website, twitter) talking about the experience of building R4DS online learning community.\nSome empowering messages came in the lines of this talk - just look at the slides pics below to get the flavor 🙌. My take-home ones include a description of a data scientist: \u0026ldquo;constantly learning, constantly making mistakes, constantly learning from them\u0026rdquo;.\n(Pictures I took during RStudio 2019 conference talk \u0026ldquo;R4DS online learning community\u0026rdquo; by Jesse Mostipak on Jan 17, 2019 in Austin, TX.)\n  ","date":1548544643,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1548544643,"objectID":"407a07504cdfe9d45ea39d9750295820","permalink":"/post/2019-01-26-favs-of-rstudio2019-conference/","publishdate":"2019-01-26T19:17:23-04:00","relpermalink":"/post/2019-01-26-favs-of-rstudio2019-conference/","section":"post","summary":"I spent the last days of 2018-19 winter break in Austin, TX where I traveled for RStudio 2019 conference. I attended e-poster session and two days of the conference: Jan 17-18.","tags":[],"title":"Some of the topics seen at RStudio conference 2019","type":"post"},{"authors":["Karas","M.","Bai","J.","Straczkiewicz","M.","Harezlak","J.","Glynn","N.W.","Harris","T.","Zipunnikov","V.","Crainiceanu","C.","Urbanek","J.K."],"categories":[],"content":"","date":1547316518,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1547316518,"objectID":"b19d1bd487ae119a3c7ee55bad8a5872","permalink":"/publication/accelerometry-data-in-health-research/","publishdate":"2019-01-12T18:08:38Z","relpermalink":"/publication/accelerometry-data-in-health-research/","section":"publication","summary":"Wearable accelerometers provide detailed, objective, and continuous measurementsof physical activity (PA). Recent advances in technology and the decreasing cost ofwearable devices led to an explosion in the popularity of wearable technology inhealth research. An ever-increasing number of studies collect high-throughput, sub-second level raw acceleration data. In this paper, we discuss problems related to thecollection and analysis of raw accelerometry data and refer to published solutions. Inparticular, we describe the size and complexity of the data, the within- and between-subject variability, and the effects of sensor location on the body. We also discusschallenges related to sampling frequency, device calibration, data labeling, and multi-ple PA monitors synchronization. We illustrate these points using the DevelopmentalEpidemiological Cohort Study (DECOS), which collected raw accelerometry data onindividuals both in a controlled and the free-living environment.","tags":["wearable accelerometers","physical activity","actigraphy"],"title":"Accelerometry Data in Health Research - Challenges and Opportunities. Review and Examples","type":"publication"},{"authors":null,"categories":[],"content":"W 2015 roku zdobylam tytul Magister na kierunku Matematyka na Politechnice Wroclawskiej. W sierpniu 2016 rozpoczelam aplikacje na programy doktoranckie na kierunku Biostatystyka w Stanach Zjednoczonych rozpoczynajace sie jesienia 2017. Dostalam i zaakeptowalam oferte z Departamentu Biostatystyki na Johns Hopkins Bloomberg School of Public Health w Baltimore (stan Maryland), oferujacego jeden z 3 najlepszych (ex aequo) programow na kierunku Biostatystyka w USA wg rankingu usnews.com z 2018 roku. W notatce opisuje etapy rekrutacji i podaje swoje obserwacje i wskazowki.\nTable of Contents  Uwagi wstepne   Ponizszy opis jest napisany w kontekscie aplikacji na kierunek Biostatystyka. Spodziewam sie, ze aplikacje na inne (w szczegolnosci: zblizone tematycznie) programy moga przebiegac podobnie, jednoczesnie nigdy nie zrobilam porownania. Przykladowa roznica o ktorej wiem, ze istnieje: na programy PhD na kierunku Biostatystyka aplikuje sie ogolnie na program i po 0-2 latach okresla lub zaweza obszar badawczy i jednoczesnie decyduje na konkretnych opiekunow naukowych (w tym: promotora/promotorow); typowe dla niektorych innych kierunkow STEM jest z kolei aplikowanie do konkretnego laboratorium (czesto: pod opieke naukowa konkretnego profesora).\n  Ponizszy opis jest napisany w oparciu o tylko i wylacznie moje doswiadczenia (za wyjatkiem miejsc, gdzie zaznaczam explicite, ze wiem cos ze slyszenia od innych studentow).\n  Ponizszy opis jest napisany w kontekscie aplikacji na programy rozpoczynajace sie jesienia 2017, wypelnianymi zgodnie z wymaganiami i terminami, ktorych pilnowalam aplikujac w 2016. Nie sprawdzilam, czy i co zmienilo sie od tego czasu.\n  Wyodrebniam nastepujace glowne komponenty procesu aplikacji:\n Wstepny wybor uczelni i zbudowanie planu czasowego procesu aplikacji Przygotowanie do egzaminow: GRE + TOEFL Przygotowanie Personal Statement Zorganizowanie listow rekomendacyjnych Networking Aplikowanie: wykonanie i nadzorowanie post-procesu  1. Wstepny wybor uczelni i zbudowanie planu czasowego timeline aplikacji Generalne ramy czasowe.\n Uczelnie roznia sie datami ostatecznego terminu aplikacji, jednoczesnie daty te zjezdzaja sie na przestrzeni ok. 1,5 miesiaca: od 1. grudnia roku N do polowy stycznia roku (N+1) na programy rozpoczynajace sie jesienia roku (N+1). 60% terminow to wlasnie 1. grudnia. 📣 Stad prace nad aplikacjami zaczynamy najpozniej 12-14 miesiecy przed faktycznym rozpoczeciem studiow.  Wstepny wybor uczelni.\n  Wybieralam uczelnie na podstawie: a) rankingow programu Biostatystyka (podzbior \u0026ldquo;Biostatystyka\u0026rdquo; z usnews.com; w 2016 korzystalam glownie z tego dokumentu); b) rozmow z profesorami i studentami w USA w czasie pracy jako Research Assistant na Indiana University w pierwszej polowie 2016; c) przegladania grup naukowych i zainteresowan profesorow na stronach uczelni; d) lokalizacji; tu: zarowno ogolny obszar USA jak i konkretne miasto (przyklad: nie chcialam studiowac w NYC).\n  Bylo mi ciezko ocenic a priori, w jaki sposob moja aplikacja bedzie ewaluowana przez komitety rekrutacyjne na uczelniach w USA.\n Aplikacja studenta z Polski na Biostatystyke u USA to wciaz egzotyczna sprawa. Komisje rekrutacyjne sa najczesniej niezaznajomione z polskim systemem szkolnictwa wyzszego (gdzie sa np. z chinskimi i hinduskimi), stad ocena \u0026ldquo;5.0\u0026rdquo; z Analizy matematycznej I na Politechnice Wroclawskiej nie mowi im bardzo duzo na temat tego, co w zwiazku z tym powinnam umiec. (Przyklad: w czasie etapu rekrutacji \u0026ldquo;on site\u0026rdquo; na jednej z uczelni Ivy League, dziekan departamentu Biostatystyki, z ktorym mialam 20-minutowe spotkanie, poprosil mnie o chocby ogolny opis tego \u0026ldquo;jak wygladaly moje studia w Polsce\u0026rdquo; 👽).     Aplikowalam do 16 uczelni (relatywnie wielu), wg nastepujacego schematu: 1/3 uczelni na liscie to \u0026ldquo;dream schools\u0026rdquo; - uczelnie z czolowki rankingow programu Biostatystyka; 1/3 to uczelnie \u0026ldquo;safe\u0026rdquo; - spodziewam sie, ze mam spore szanse sie tam dostac; 1/3 to uczelnie \u0026ldquo;po srodku\u0026rdquo;.\n  Dostalam sie / dostalam zaproszenie do 2. etapu rekrutacji od nieco ponad 50% z uczelni, do ktorych aplikowalam (2. etap to zaproszenie na rozmowe \u0026ldquo;on site\u0026rdquo; lub jego odpowiednik dla studentow przebywajacych poza USA: rozmowa telefoniczna z czlonkami komitetu rekrutacyjnego; w kilku przypadkach nie kontynuowalam procesu do 2. etapu wiedzac, ze dostalam sie juz do miejsca wyzej na liscie moich preferencji).\n  Co interesujace, uczelnie, ktore byly mna zainteresowanie, to niemal dokladnie: polowa uczelni z grupy \u0026ldquo;dream schools\u0026rdquo; + polowa z \u0026ldquo;safe\u0026rdquo; + polowa z \u0026ldquo;po srodku\u0026rdquo; 👏👍\n    Plan czasowy procesu aplikacji powinien zawierac:\n  Daty ostatecznego terminu aplikacji (\u0026ldquo;deadline\u0026rdquo;) do wybranych uczelni, tzn. daty, po ktorych zamykaja sie systemy online umozliwiajace wykonanie aplikacji.\n  Liste wymaganych egzaminow i najdalszy termin ich realizacji (np. w Polsce), ktory zapewnia bezpieczne okno czasowe na przeslanie wynikow do uczelni. Najczesciej beda to dwa egzaminy: TOEFL i GRE General.\n Wyniki TOEFL i GRE General wysylane sa automatycznie do uczelni, ktorych liste zadeklarujemy przy zapisie na egzamin lub ktore dodamy kiedykolwiek pozniej przy uzyciu systemow online do zarzadzania wynikami. Zwyczajnie trzeba przypilnowac, czy dni robocze, ktore organizator daje sobie na a) sprawdzenie egzaminu + b) wyslanie wynikow do uczelni, dodaja nam sie z ostatecznymi terminami aplikacji.     Okno czasowe na tlumaczenie przysiegle oraz przeewaluowanie i/lub wyslanie poczta transkryptow ocen.\n  Przyklady tego, co moze nas \u0026ldquo;zaskoczyc\u0026rdquo;. a) ~5 uczelni wymagalo wyslania tlumaczonych (tl. przysiegly) transkryptow ocen fizycznie na adres uczelni w USA (w kilku przypadkach: w terminie wczesniejszym, niz generalny ostateczny termin aplikacji podany na stronie uczelni). b) ~7 uczelni realizowalo aplikacje przez system SOPHAS - jednolity system aplikacji na wydzialy Zdrowia Publicznego (\u0026ldquo;School of Public Health\u0026rdquo;) w USA, z ktorych korzysta czesc uczelni.\n  SOPHAS jest dla aplikanta wygodny o tyle, ze duza czesc dokumentow ladujemy do systemu tylko raz, niezaleznie od liczby uczelni na ktore aplikujemy via SOPHAS. Z drugiej strony, SOPHAS akceptuje zagraniczne transkrypty ocen tylko i wylacznie po przeewaluowaniu ich na system amerykanski przez World Education Services (WES). Musimy wiec wyslac do WES komplet swoich przetlumaczonych (tl. przysiegly 👼) transkryptow, a WES je ewaluuje i wysyla wyniki do SOPHAS. (To byl dla mnie najbardziej stresujacy element tego etapu rekrutacji: WES nie zrealizowal ewaluacji moich ocen w oknie czasowym, ktore deklaruje od momentu zmiany statusu mojej sprawy na \u0026ldquo;otrzymalismy twoje transkrypty\u0026rdquo;; pomimo moich prosb, \u0026ldquo;grozb\u0026rdquo; i placzu, dokumenty zostaly wyslane dopiero na kilka dobrych dni po deklarowanym terminie 🐢, szczesliwie dokladnie \u0026ldquo;na styk\u0026rdquo; z ostatecznymi terminami aplikacji do kilku uczelni. 🎣)\n    Wybor uczelni i organizowanie planu czasowy procesu aplikacji sa ze soba powiazane: specyficzne wymagania i terminy poszczegolnych uczelni wymuszaja plan czasowy, natomiast wymagania czasowe modyfikowaly moje wybory uczelni (przyklad: widzialam, ze nie dam rady czasowo przygotowac sie do egzaminu GRE Mathematics; z drugiej strony, jako ze duza czesc moich dokumentow typowo potrzebnych do aplikacji byla juz w SOPHAS, zaaplikowalam do kilku dodatkowych uczelni via SOPHAS).\n2. Przygotowanie do egzaminow: GRE General + TOEFL   GRE General jest niezbedny do aplikowania na studia \u0026ldquo;graduate\u0026rdquo; (Masters / PhD) w USA. W nielicznych przypadkach wymagany jest rowniez GRE specjalistyczny (Stanford: GRE Mathematics Subject Test). Egzamin jezykowy jest wymagany dla absolwentow uczelni, na ktorych jezyk angielski nie jest podstawowym jezykiem wykladowym. W USA wymaganym standardowo egzaminem jezykowym jest TOEFL (w niektorych szkolach zamienny z IELTS, w niektorych nie).\n  Zdawalam GRE General i TOEFL. Oba egzaminy zdawalam w Polsce (najpierw GRE General w Krakowie, tydzien pozniej: TOEFL w Poznaniu).\n  Przejscie przez przewodnik po typach zadan na obu egzaminach to absolutne minimum w przygotowaniach.\n  Sugeruje przeznaczyc 90-95% czasu przygotowan do obu egzaminow na GRE General.\n  GRE General sklada sie z ~5 komponentow testujacych dwa obszary: j. angielski i matematyke. Czesc matematyczna jest prosta (poziom matury podstawowej z matematyki), jednoczesnie nalezy zalozyc duzo czasu na przygotowanie sie pod bardzo konkretna forme tego egzaminu. Jest bardzo malo miejsca na jakiekolwiek pomylki, w szczegolnosci: bledy przy pierwszych zadaniach (poziom trudnosci zadan rosnie z biegiem czasu egzaminu), stad trzeba wyklepac do stanu \u0026ldquo;rozwiazuje z automatu\u0026rdquo; problemy pojawiajace sie na czesci matematycznej. Wlasnosci katow w trojkacie wpisanym i opisanym na okregu 💘, zadania \u0026ldquo;z dwoma pociagami\u0026rdquo;, te sprawy.\n  GRE General cz. jezykowa sklada sie zadan testujacych umiejetnosc krytycznego myslenia i argumentowania w formie pisemnej oraz z zadan testujacych slownictwo. Czesc testujaca slownictwo jest moim zdaniem zwyczajnie nierobialna i ze swojej perspektywy oceniam, ze nie ma duzej roznicy, czy przeznaczy sie na nauke slowek 10h czy 300h przed egzaminem, tu trzeba isc do kosciola i pomodlic sie o celnosc strzalow w odpowiedziach. Na serio: przygotowanie do GRE General cz. jezykowa jest czasochlonne. 😐 Pomocne: w internecie sa dostepne roznej dlugosci listy \u0026ldquo;slownictwa na GRE\u0026rdquo; oraz dedykowane aplikacje do nauki \u0026ldquo;slowek na GRE\u0026rdquo;.\n  Po napisaniu GRE General cz. jezykowa, poziom TOEFL wydaje sie latwy, a sam egzamin - przyjemny 😌.\n    Sugeruje nie zdawac obu egzaminow w jeden weekend; w szczegolnosci GRE jest bardzo wyczerpujacy (mysle, ze to byl najbardziej wyczerpujacy egzamin w moim zyciu; po wyjsciu z sali przez kilka minut zbieralam mysli o tym co to sie robilo zeby zamowic Ubera) 🚀.\n  3. Przygotowanie Personal Statement Nota bene: transkrypty ocen i wyniki z egzaminow sluza glownie do wyznaczenia punktu odciecia, ponizej ktorego \u0026ldquo;uczelnia generalnie nie rozmawia z potencjalnym studentem\u0026rdquo;. Latwo wyobrazic sobie, ze punkt odciecia przekracza wielu bardzo dobrych studentow (podpowiadam przypomniec sobie populacje np. Chin i Indii). To, czym \u0026ldquo;kupujemy\u0026rdquo; sobie przychylnosc komisji rekrutacyjnej i zdobywamy oferte programu, to kombinacja zawartosci CV, Personal Statement, listow rekomendacyjnych i naszego networkingu.\nPersonal Statement (zamiennie: Statement of Purpose) to wazny dokument, ktorego szablon - wersje beta tworzylam przez kilka dni z rzedu.\n  Szablon mojego Personal Statement mial 2 pelne strony A4 i skladal sie z nastepujacych paragrafow:\n chwytliwe otwarcie, w ktorym wzbudzam zainteresowanie swoja motywacja i indywidualna historia, notka biograficzna, skupiajaca sie na doswiadczeniu w badaniach naukowych (poparte konkretnymi przykladami), notka biograficzna, skupiajaca sie na cechach osobowosciowych, ktore predestynuja mnie do bycia odnoszacym sukcesy studentem (poparte konkretnymi przykladami), motywacja do studiowania kierunku Biostatystyka, \u0026ldquo;big-picture\u0026rdquo; tego, co chce osiagnac dzieki studiom PhD - na przestrzeni studiow i calego zycia, motywacja do studiowania kierunku Biostatystyka na tej konkretnej uczelni, zgrabne zakonczenie.    Sugestie:\n  W Personal Statement nie ma miejsca na \u0026ldquo;chcialem byc marynarzem chcialem miec tatuaze podrozowac\u0026rdquo;. Wszystko, co tam wrzucamy tytulem osiagniec i aktywnosci, powinno byc poparte/umotywowane konkretnymi wydarzeniami z zycia akademickiego/poza-akademickiego.\n  W Personal Statement nie ma miejsca na bycie skromnym. Inni aplikanci nie beda skromni.\n  Warto zgooglowac, jakie elementy powinny zawsze znalezc sie w tym dokumencie - czesto uczelnie zamieszczaja te infromacje na swoich podstronach dot. rekrutacji. Jednoczesnie sugeruje NIE szukac i NIE \u0026ldquo;inspirowac sie\u0026rdquo; gotowymi dokumentami instniejacymi w sieci; im bardziej \u0026ldquo;twoj\u0026rdquo; jest Personal Statement, tym lepiej.\n  Sugeruje szablon dac do sprawdzenia 3-5 znajomym 🙏 pod katem tresci; wyszlifowana tresciowo wersje sugeruje dac do sprawdzenia pod wzgledem jezykowym dobremu tlumaczowi. Do sprawdzenia (i do tlumaczenia transkryptow) polecam Panie z BT Centrum we Wroclawiu - absolutnie profesjonale i elastyczne, de facto \u0026ldquo;uratowaly\u0026rdquo; moje aplikacje (a mogly wysmiac mnie w progu za termin, w ktorym potrzebowalam miec gotowe tlumaczenia: \u0026ldquo;na jutro\u0026rdquo; 🐩).\n  4. Listy rekomendacyjne  Potrzebowalam 3 listy rekomendacyjne. Szczegolny przypadek: SOPHAS (wspomniany wyzej) bardzo mocno sugerowal zadbanie o co najmniej jeden list pochodzacy od osoby spoza akademickiego srodowiska; w tym przypadku zalaczylam 4 listy rekomendacyjne.  Uwagi:\n  W dobrym guscie jest zwrocenie sie z prosba o rozwazenie mozliwosci wystosowania takiego listu na min. 1 miesiac przed data, na kiedy list ma byc gotowy. Warto w takim pierwszym mejlu zaznaczyc wyraznie, na kiedy list bedziemy potrzebowac.\n  Listy rekomendacyjne pisane przez profesorow w USA sa zawsze pelne \u0026ldquo;ochow\u0026rdquo; i \u0026ldquo;achow\u0026rdquo;. Choc ciezko mi wyobrazic sobie poproszenie explicite o \u0026ldquo;pelen pochwal list\u0026rdquo;, warto sprobowac zaznaczyc, ze np. ze wzgledu na wysoka konkurencyjnosc uczelni, potrzebujemy \u0026ldquo;silnego\u0026rdquo; listu rekomendacyjnego. (Z tego co rozumiem, wylistowanie aktywnosci aplikanta i jednozdaniowe wyrazenie przekonania, ze to dobry kandydat, to - generalnie rzecz biarac - tresc na slaby list.)\n  5. Networking Ciezko mi podkreslic wystarczajaco mocno, jak istotnym elementem aplikacji jest networking, tzn. nawiazanie kontaktu z profesorami na wybranych przez nas uczelniach.\n  Faktycznie, dostalam sie na niektore uczelnie (ze swoich kategorii \u0026ldquo;safe\u0026rdquo; i \u0026ldquo;po srodku\u0026rdquo;), z ktorymi nie nawiazalam wczesniej zadnego bezposredniego kontaktu.\n  Twierdze, ze bardzo trudne byloby dostanie sie do \u0026ldquo;dream school\u0026rdquo; bez nawiazania wczesniej bezposredniego kontaktu. Tu, aplikacja bardzo potrzebuje \u0026ldquo;wsparcia od srodka\u0026rdquo; departamentu, do ktorego chcemy sie dostac.\n  Przyklady tego, jak mozna budowac potrzebny networking:\n  Wyjechanie do USA na kilka miesiecy do tymczasowej pracy / na staz na wybrana uczelnie.\n Pierwszym krokiem do zaproszenia na taki wyjazd moze byc krotki mejl do wybranego profesora, mowiacy: \u0026ldquo;Czesc, jestem bardzo zainteresowany twoja praca o X, sam od dawna czytam i pracuje nad rzeczami zwiazanymi z Y, czy znalazlbys 10 minut na rozmowe na Skajpie? Dostosuje sie do terminu, ktory zaproponujesz.\u0026rdquo;     Wyjechanie do USA na konferencje / szkole wakacyjna / warsztaty, gdzie beda profesorowie z interesujacej nas uczelni.\n  Nawiazanie kontaktu telefonicznego / mejlowego z profesorami z interesujacej nas uczelni. (Por. wskazowka w podpunkcie wyzej).\n  6. Aplikowanie   Wypelnianie aplikacji jest czasochlonne.\n Nalezy wziac margines na specjalne przypadki nawet w ramach wykonywania aplikacji w formularzu online. Przyklady: a) z ostatniej strony formularza aplikacyjnego: nie przechodzi platnosc polska karta platnicza; b) z ktorejkolwiek / wielu stron formularza aplikacyjnego: nie jest jasne, co oznacza to pole - musze wyslac mejla z pytaniem do dzialu rekrutacji tej uczelni 😎.      Aplikacje sa drogie.\n  Niektore koszty sa jednorazowe i prawie niezalezne od liczby aplikacji (wyjazd na egzamin TOEFL / GRE). Aplikacja do kazdej kolejnej szkoly generuje koszt (bezzwrotna oplata aplikacyjna: 30-130 USD, przeslanie wyniku GRE: 20 USD, przeslanie wyniku TOEFL: 20 USD).\n  Mysle, ze calkowity koszt moich aplikacji (w tym: tlumacz przysiegly, dojazdy i noclegi na egzaminy w innych miastach Polski, wyslanie paczek UPS z dokumentami do kilku szkol w USA) to lacznie 7.000-10.000 PLN ☂️.\n    ","date":1535930243,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1535930243,"objectID":"72f641a34cee5120d548eacbfe44450e","permalink":"/post/2018-09-02-applying-for-phd-in-usa/","publishdate":"2018-09-02T19:17:23-04:00","relpermalink":"/post/2018-09-02-applying-for-phd-in-usa/","section":"post","summary":"W 2015 roku zdobylam tytul Magister na kierunku Matematyka na Politechnice Wroclawskiej. W sierpniu 2016 rozpoczelam aplikacje na programy doktoranckie na kierunku Biostatystyka w Stanach Zjednoczonych rozpoczynajace sie jesienia 2017. Dostalam i zaakeptowalam oferte z Departamentu Biostatystyki na Johns Hopkins Bloomberg School of Public Health w Baltimore (stan Maryland), oferujacego jeden z 3 najlepszych (ex aequo) programow na kierunku Biostatystyka w USA wg rankingu usnews.","tags":[],"title":"[in Polish] Aplikacja na studia doktoranckie (Biostatystyka) w USA: notatki nt. procesu rekrutacji","type":"post"},{"authors":null,"categories":null,"content":"","date":1533150350,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1533150350,"objectID":"e4f5d29a3fc3d5530df9a883b576809f","permalink":"/talk/2018-08-01-jsm/","publishdate":"2018-08-01T15:05:50-04:00","relpermalink":"/talk/2018-08-01-jsm/","section":"talk","summary":"","tags":[],"title":"ADaptive Empirical Pattern Transformation (ADEPT) with application to walking stride segmentation","type":"talk"},{"authors":null,"categories":null,"content":"","date":1530628200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1530628200,"objectID":"7bcdfdce3065a48533c65a88fb0465e2","permalink":"/talk/2018-07-03-whyr/","publishdate":"2018-07-03T10:30:00-04:00","relpermalink":"/talk/2018-07-03-whyr/","section":"talk","summary":"","tags":[],"title":"Wearable accelerometers, accelerometry data and automatic steps segmentation in R: strideter and convo R packages","type":"talk"},{"authors":[],"categories":[],"content":"We propose to estimate association between the brain structure features and a scalar outcome in a regression model while utilizing additional information about structural connectivity between the brain regions.\nSpecifically, we propose a novel regularization method \u0026ndash; riPEER (ridgified Partially Empirical Eigenvectors for Regression) \u0026ndash; that defines a regularization penalty term based on the structural connectivity-derived Laplacian matrix.\n**See images citation and/or credit information** [below](#custom). -- Table of Contents    Scientific problem Challenges Proposed solution Published work Software Images used in the post \u0026ndash; credit/references     Scientific problem The motivation for the work was to quantify the association between alcohol abuse phenotypes (outcome) and cortical thickness of the brain (covariates) in a study sample of young social-to-heavy drinking males. The data included measurements of average cortical thickness estimated for 68 brain regions.\nThis image (see images credit below) visualizes process of obtaining cortical thickness measurements from structural MRI images.\nChallenges Commonly shared issues in such settings are:\n  high dimensionality of the data - we typically parcel the brain into tens, or hundreds of units from which we take measurements, and each unit may then correspond to a covariate in the data set,\n  correlation of the covariates - measurements from spatially neighbouring or otherwise connected brain regions are likely to be correlated,\n  small sample size - brain imaging studies often recruit a few tens of participants only.\n  Proposed solution We propose penalized regression method riPEER to estimate a linear model: $$y = Zb + X\\beta + \\varepsilon$$ where:\n $y$ - response (here: alcohol abuse phenotypes), $Z$ - input data matrix (here: cortical thickness measurements), $X$ - input data matrix (here: demographics data), $\\beta$ - regression coefficients, not penalized in estimation process $b$ - regression coefficients, penalized in estimation process and for whom there is a prior graph of similarity / graph of connections. available.  The riPEER estimation method uses a penalty being a linear combination of a graph-based and ridge penalty terms: $$ \\hat{\\beta}, \\hat{b} = \\underset{\\beta,b}{\\text{arg min}} \\left[ (y - X\\beta - Zb)^T(y - X\\beta - Zb) + \\lambda_Qb^TQb + \\lambda_Rb^Tb \\right ] $$\nwhere:\n $Q$ - a graph-originated penalty matrix; typically: a graph Laplacian matrix (here: a graph Laplacian derived from structural connectivity of brain regions), $\\lambda_Q$ - regularization parameter for a graph-based penalty term, $\\lambda_R$ - regularization parameter for ridge penalty term.  riPEER penalty term In the riPEER penalty term $(\\lambda_Qb^TQb + \\lambda_Rb^Tb)$,\n  A graph-originated penalty matrix $Q$ allows imposing similarity between coefficients of variables which are connected.\n  A ridge penalty term, $\\lambda_Rb^Tb$, allows for L2 regularization component; in addition, even with very small $\\lambda_R$, eliminates computational issues arising from singularity of $Q$.\n  Regularization parameters $\\lambda_R$, $\\lambda_Q$ are estimated automatically as ML estimators of equivalent Linear Mixed Models optimization problem.\n  Published work   We published the proposed riPEER method in work Brain connectivity-informed regularization methods for regression (Karas, M., Brzyski, D., Dzemidzic, M., Goni, J., Kareken, D.A., Randolph, T., Harezlak J. (2017)).\n  We published the riPEER extension to generalized linear regression, addressing both theoretical and computational issues, in work Connectivity‐informed adaptive regularization for generalized outcomes (Brzyski, D., Karas, M., Ances, B.M., Dzemidzic, M., Goni, J., Randolph, T., Harezlak J. (2021)).\n  Software We provided open-source implementation of the proposed riPEER estimation method in R package mdpeer (CRAN index). The package provides functions for graph-constrained regression methods in which regularization parameters are selected automatically via estimation of equivalent Linear Mixed Model formulation.\nThe R package is accompanied by Intro and usage examples vignette.\nImages used in the post \u0026ndash; credit/references   Featured image - top left component. Cortical thickness. Resources of Neurorecovery Laboratory at MGH/MIT/HMS Athinoula A. Martinos Center for Biomedical Imaging. Accessed at: link (last accessed on Nov 20, 2020).\n  Featured image - top middle component. Diffusion MRI Tractography in the brain white matter. Xavier Gigandet et. al. - Gigandet X, Hagmann P, Kurant M, Cammoun L, Meuli R, et al. (2008) Estimating the Confidence Level of White Matter Connections Obtained with MRI Tractography. PLoS ONE 3(12): e4006. doi:10.1371/journal.pone.0004006. Accessed at: link (last accessed on Nov 20, 2020).\n  Featured image - top right component. Databases of Statistical Information. Resources of Berkeley Advanced Media Institute Graduate School of Journalism. Accessed at: link (last accessed on Nov 20, 2020).\n  ","date":1512575693,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512575693,"objectID":"fa0d0a2410adef850e7703f24d3f7da6","permalink":"/project/project_mdpeer/","publishdate":"2017-12-06T11:54:53-04:00","relpermalink":"/project/project_mdpeer/","section":"project","summary":"We propose riPEER regularization method to estimate association between the brain structure features and a scalar outcome in a regression model while utilizing additional information about structural connectivity between the brain regions.","tags":["brain imaging","statistical methods","regularization","regression"],"title":"Brain connectivity-informed regularization methods for regression","type":"project"},{"authors":["Karas","M.","Brzyski","D.","Dzemidzic","M.","Goni","J.","Karaken","D.A.","Randolph","T.W.","Harezlak","J."],"categories":[],"content":"","date":1511472289,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1511472289,"objectID":"b3945fc3404b55b2d198cfa10f8aebe7","permalink":"/publication/brain-connectivity-informed-regularization/","publishdate":"2017-11-23T17:24:49-04:00","relpermalink":"/publication/brain-connectivity-informed-regularization/","section":"publication","summary":"One of the challenging problems in brain imaging research is a principled incorporation of information from different imaging modalities. Frequently, each modality is analyzed separately using, for instance, dimensionality reduction techniques, which result in a loss of mutual information. We propose a novel regularization method to estimate the association between the brain structure features and a scalar outcome within the linear regression framework. Our regularization technique provides a principled approach to use external information from the structural brain connectivity and inform the estimation of the regression coefficients. Our proposal extends the classical Tikhonov regularization framework by defining a penalty term based on the structural connectivity-derived Laplacian matrix. Here, we address both theoretical and computational issues. The approach is first illustrated using simulated data and compared with other penalized regression methods. We then apply our regularization method to study the associations between the alcoholism phenotypes and brain cortical thickness using a diffusion imaging derived measure of structural connectivity. Using the proposed methodology in 148 young male subjects with a risk for alcoholism, we found a negative associations between cortical thickness and drinks per drinking day in bilateral caudal anterior cingulate cortex, left lateral OFC, and left precentral gyrus.","tags":["brain imaging","statistical methods","regularization","regression"],"title":"Brain Connectivity Informed Regularization Methods for Regression","type":"publication"},{"authors":null,"categories":null,"content":"  tidyverse The tidyverse is an ecosystem of packages designed with a shared underlying design philosophy, grammar, and data structures.\nThe tidyverse v1.3.0 loads 8 packages via library(tidyverse): ggplot2, dplyr, tidyr, purrr, tibble, stringr, forcats, readr.\nIt also includes (installs) but does not automatically load some other (i.e. lubridate).\n Presentation motivation Me using tidyverse packages in 2015:  Typically do stuff like\ndat %\u0026gt;% filter(age \u0026gt;= 50) %\u0026gt;% group_by(var1, var2) %\u0026gt;% summarize(y_mean = mean(y), y_median = median(y)) and google “r ggplot2 rotate x axis labels”\n Me using tidyverse packages in 2020:  Typically do stuff like\ndat %\u0026gt;% filter(age \u0026gt;= 50) %\u0026gt;% group_by(var1, var2) %\u0026gt;% summarize(y_mean = mean(y), y_median = median(y)) and google “r ggplot2 rotate x axis labels”\n  Presentation content credits  (Post link here.) These slides from now on are like 80-90% of this post content, with small alterations from my side.\n  Outline Palmer Penguins dataset\nSelecting columns in data Reordering columns in data Controlling mutated column location Transforming from wide to long Transforming from long to wide Running group statistics across multiple columns Control how output columns are named when summarising across multiple columns Running models across subsets of data Nesting data Graphing across subsets   Palmer Penguins dataset “The goal of palmerpenguins is to provide a great dataset for data exploration \u0026amp; visualization, as an alternative to iris.”\nData set contains size measurements for three penguin species (Adelie, Chinstrap, Gentoo) observed on three islands in the Palmer Archipelago, Antarctica (Biscoe, Dream, Torgersen).\nlibrary(tidyverse) library(palmerpenguins) str(penguins) ## tibble [344 × 8] (S3: tbl_df/tbl/data.frame) ## $ species : Factor w/ 3 levels \u0026quot;Adelie\u0026quot;,\u0026quot;Chinstrap\u0026quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ island : Factor w/ 3 levels \u0026quot;Biscoe\u0026quot;,\u0026quot;Dream\u0026quot;,..: 3 3 3 3 3 3 3 3 3 3 ... ## $ bill_length_mm : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ... ## $ bill_depth_mm : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ... ## $ flipper_length_mm: int [1:344] 181 186 195 NA 193 190 181 195 193 190 ... ## $ body_mass_g : int [1:344] 3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ... ## $ sex : Factor w/ 2 levels \u0026quot;female\u0026quot;,\u0026quot;male\u0026quot;: 2 1 1 NA 1 2 1 2 NA NA ... ## $ year : int [1:344] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ...    1. Selecting columns in data To select columns using dplyr::select() or tidyr::pivot_longer() based on common conditions, use helper functions.\n To select variables contained in a character vector: all_of(), any_of() To select all (i.e. remaining) or last column: everything(), last_col() To select variables by matching patterns in their names: starts_with(), ends_with(), contains(), matches(), num_range() To apply a custom function and select those for which the function returns TRUE: where()  penguins %\u0026gt;% dplyr::select(!contains(\u0026quot;_\u0026quot;), starts_with(\u0026quot;bill\u0026quot;)) %\u0026gt;% head(n = 3) ## # A tibble: 3 x 6 ## species island sex year bill_length_mm bill_depth_mm ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Adelie Torgersen male 2007 39.1 18.7 ## 2 Adelie Torgersen female 2007 39.5 17.4 ## 3 Adelie Torgersen female 2007 40.3 18   my_select_func \u0026lt;- function(var_name){ return(is.factor(var_name)) } penguins %\u0026gt;% dplyr::select(where(my_select_func))  ## # A tibble: 344 x 3 ## species island sex ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; ## 1 Adelie Torgersen male ## 2 Adelie Torgersen female ## 3 Adelie Torgersen female ## 4 Adelie Torgersen \u0026lt;NA\u0026gt; ## 5 Adelie Torgersen female ## 6 Adelie Torgersen male ## 7 Adelie Torgersen female ## 8 Adelie Torgersen male ## 9 Adelie Torgersen \u0026lt;NA\u0026gt; ## 10 Adelie Torgersen \u0026lt;NA\u0026gt; ## # … with 334 more rows  2. Reordering columns in data To reorder specific columns or sets of columns, use dplyr::relocate() with .before or .after\npenguins %\u0026gt;% dplyr::relocate(contains(\u0026quot;_\u0026quot;), .after = year) %\u0026gt;% head(n = 3) ## # A tibble: 3 x 8 ## species island sex year bill_length_mm bill_depth_mm flipper_length_… ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; ## 1 Adelie Torge… male 2007 39.1 18.7 181 ## 2 Adelie Torge… fema… 2007 39.5 17.4 186 ## 3 Adelie Torge… fema… 2007 40.3 18 195 ## # … with 1 more variable: body_mass_g \u0026lt;int\u0026gt; penguins %\u0026gt;% dplyr::relocate(species, .after = last_col()) %\u0026gt;% head(n = 3) ## # A tibble: 3 x 8 ## island bill_length_mm bill_depth_mm flipper_length_… body_mass_g sex year ## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; ## 1 Torge… 39.1 18.7 181 3750 male 2007 ## 2 Torge… 39.5 17.4 186 3800 fema… 2007 ## 3 Torge… 40.3 18 195 3250 fema… 2007 ## # … with 1 more variable: species \u0026lt;fct\u0026gt;  3. Controlling mutated column location To control the location of the newly added column, use dplyr::mutate()’s option (similar to above’s dplyr::relocate())\npenguins \u0026lt;- penguins %\u0026gt;% dplyr::mutate(penguinid = row_number(), .before = everything()) penguins  ## # A tibble: 344 x 9 ## penguinid species island bill_length_mm bill_depth_mm flipper_length_… ## \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; ## 1 1 Adelie Torge… 39.1 18.7 181 ## 2 2 Adelie Torge… 39.5 17.4 186 ## 3 3 Adelie Torge… 40.3 18 195 ## 4 4 Adelie Torge… NA NA NA ## 5 5 Adelie Torge… 36.7 19.3 193 ## 6 6 Adelie Torge… 39.3 20.6 190 ## 7 7 Adelie Torge… 38.9 17.8 181 ## 8 8 Adelie Torge… 39.2 19.6 195 ## 9 9 Adelie Torge… 34.1 18.1 193 ## 10 10 Adelie Torge… 42 20.2 190 ## # … with 334 more rows, and 3 more variables: body_mass_g \u0026lt;int\u0026gt;, sex \u0026lt;fct\u0026gt;, ## # year \u0026lt;int\u0026gt;  4. Transforming from wide to long To transform data set from wide(r) to long(er) form, use tidyr::pivot_longer() which is an updated approach to an older tidyr::gather().\npenguins %\u0026gt;% tidyr::pivot_longer(cols = contains(\u0026quot;_\u0026quot;), # pivot these columns names_to = \u0026quot;variable_name\u0026quot;, # name of column containing \u0026quot;old columns\u0026quot; names values_to = \u0026quot;variable_value\u0026quot;) # name of column containing \u0026quot;old columns\u0026quot; values ## # A tibble: 1,376 x 7 ## penguinid species island sex year variable_name variable_value ## \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 Adelie Torgersen male 2007 bill_length_mm 39.1 ## 2 1 Adelie Torgersen male 2007 bill_depth_mm 18.7 ## 3 1 Adelie Torgersen male 2007 flipper_length_mm 181 ## 4 1 Adelie Torgersen male 2007 body_mass_g 3750 ## 5 2 Adelie Torgersen female 2007 bill_length_mm 39.5 ## 6 2 Adelie Torgersen female 2007 bill_depth_mm 17.4 ## 7 2 Adelie Torgersen female 2007 flipper_length_mm 186 ## 8 2 Adelie Torgersen female 2007 body_mass_g 3800 ## 9 3 Adelie Torgersen female 2007 bill_length_mm 40.3 ## 10 3 Adelie Torgersen female 2007 bill_depth_mm 18 ## # … with 1,366 more rows   # as previous example, but simultaneously split the names of columns # which we pivot into longer format by \u0026quot;_\u0026quot; separator penguins_longer \u0026lt;- penguins %\u0026gt;% tidyr::pivot_longer(cols = contains(\u0026quot;_\u0026quot;), # pivot these columns names_sep = \u0026quot;_\u0026quot;, names_to = c(\u0026quot;part\u0026quot;, \u0026quot;measure\u0026quot;, \u0026quot;unit\u0026quot;), # name of column(s) containing \u0026quot;old columns\u0026quot; names values_to = \u0026quot;measure_value\u0026quot; ) # name of column containing \u0026quot;old columns\u0026quot; values penguins_longer  ## # A tibble: 1,376 x 9 ## penguinid species island sex year part measure unit measure_value ## \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 Adelie Torgersen male 2007 bill length mm 39.1 ## 2 1 Adelie Torgersen male 2007 bill depth mm 18.7 ## 3 1 Adelie Torgersen male 2007 flipper length mm 181 ## 4 1 Adelie Torgersen male 2007 body mass g 3750 ## 5 2 Adelie Torgersen female 2007 bill length mm 39.5 ## 6 2 Adelie Torgersen female 2007 bill depth mm 17.4 ## 7 2 Adelie Torgersen female 2007 flipper length mm 186 ## 8 2 Adelie Torgersen female 2007 body mass g 3800 ## 9 3 Adelie Torgersen female 2007 bill length mm 40.3 ## 10 3 Adelie Torgersen female 2007 bill depth mm 18 ## # … with 1,366 more rows  5. Transforming from long to wide To transform data set from long(er) to wide(r) form, use tidyr::pivot_wider() which is an updated approach to an older tidyr::spread().\n# revert from long form from previous example penguins_wider \u0026lt;- penguins_longer %\u0026gt;% tidyr::pivot_wider(names_from = c(\u0026quot;part\u0026quot;, \u0026quot;measure\u0026quot;, \u0026quot;unit\u0026quot;), # pivot these columns values_from = \u0026quot;measure_value\u0026quot;, # take the values from here names_sep = \u0026quot;_\u0026quot;) # combine col names using an underscore penguins_wider ## # A tibble: 344 x 9 ## penguinid species island sex year bill_length_mm bill_depth_mm ## \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 Adelie Torge… male 2007 39.1 18.7 ## 2 2 Adelie Torge… fema… 2007 39.5 17.4 ## 3 3 Adelie Torge… fema… 2007 40.3 18 ## 4 4 Adelie Torge… \u0026lt;NA\u0026gt; 2007 NA NA ## 5 5 Adelie Torge… fema… 2007 36.7 19.3 ## 6 6 Adelie Torge… male 2007 39.3 20.6 ## 7 7 Adelie Torge… fema… 2007 38.9 17.8 ## 8 8 Adelie Torge… male 2007 39.2 19.6 ## 9 9 Adelie Torge… \u0026lt;NA\u0026gt; 2007 34.1 18.1 ## 10 10 Adelie Torge… \u0026lt;NA\u0026gt; 2007 42 20.2 ## # … with 334 more rows, and 2 more variables: flipper_length_mm \u0026lt;dbl\u0026gt;, ## # body_mass_g \u0026lt;dbl\u0026gt;  6. Running group statistics across multiple columns To apply multiple summary statistics simultaneously in an efficient way, use across() verb.\n# calculate mean and sd for each variable ending in mm, across three species penguin_stats \u0026lt;- penguins %\u0026gt;% dplyr::group_by(species) %\u0026gt;% dplyr::summarise(across(.cols = ends_with(\u0026quot;mm\u0026quot;), .fns = list(~mean(.x, na.rm = TRUE), ~sd(.x, na.rm = TRUE)))) penguin_stats ## # A tibble: 3 x 7 ## species bill_length_mm_1 bill_length_mm_2 bill_depth_mm_1 bill_depth_mm_2 ## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Adelie 38.8 2.66 18.3 1.22 ## 2 Chinst… 48.8 3.34 18.4 1.14 ## 3 Gentoo 47.5 3.08 15.0 0.981 ## # … with 2 more variables: flipper_length_mm_1 \u0026lt;dbl\u0026gt;, flipper_length_mm_2 \u0026lt;dbl\u0026gt;  7. Control how output columns are named when summarising across multiple columns To apply multiple summary statistics simultaneously in an efficient way with across() verb and to use other than default column names of summary variables, use the .names argument.\npenguins_stats \u0026lt;- penguins %\u0026gt;% dplyr::group_by(species) %\u0026gt;% dplyr::summarise(across(.cols = ends_with(\u0026quot;mm\u0026quot;), .fns = list(mean = ~mean(.x, na.rm = TRUE), sd = ~sd(.x, na.rm = TRUE)), .names = \u0026quot;{gsub(\u0026#39;_|_mm\u0026#39;, \u0026#39;\u0026#39;, col)}_{.fn}\u0026quot;)) penguins_stats ## # A tibble: 3 x 7 ## species billlength_mean billlength_sd billdepth_mean billdepth_sd ## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Adelie 38.8 2.66 18.3 1.22 ## 2 Chinst… 48.8 3.34 18.4 1.14 ## 3 Gentoo 47.5 3.08 15.0 0.981 ## # … with 2 more variables: flipperlength_mean \u0026lt;dbl\u0026gt;, flipperlength_sd \u0026lt;dbl\u0026gt;  8. Running models across subsets of data Use dplyr::summarise() to compute different types of outcomes stored in a list, for example, summary vectors, data frames or other objects like models or graphs.\npenguin_models \u0026lt;- penguins %\u0026gt;% dplyr::group_by(species) %\u0026gt;% dplyr::summarise(model = list(lm(body_mass_g ~ flipper_length_mm + bill_length_mm + bill_depth_mm))) penguin_models ## # A tibble: 3 x 2 ## species model ## \u0026lt;fct\u0026gt; \u0026lt;list\u0026gt; ## 1 Adelie \u0026lt;lm\u0026gt; ## 2 Chinstrap \u0026lt;lm\u0026gt; ## 3 Gentoo \u0026lt;lm\u0026gt; model_tmp \u0026lt;- penguin_models[1, 2][[1]][[1]] # summary(model_tmp)   library(broom) penguin_models \u0026lt;- penguins %\u0026gt;% dplyr::group_by(species) %\u0026gt;% dplyr::summarise(broom::glance(lm(body_mass_g ~ flipper_length_mm + bill_length_mm + bill_depth_mm))) penguin_models ## # A tibble: 3 x 13 ## species r.squared adj.r.squared sigma statistic p.value df logLik AIC ## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Adelie 0.508 0.498 325. 50.6 1.55e-22 3 -1086. 2181. ## 2 Chinst… 0.504 0.481 277. 21.7 8.48e-10 3 -477. 964. ## 3 Gentoo 0.625 0.615 313. 66.0 3.39e-25 3 -879. 1768. ## # … with 4 more variables: BIC \u0026lt;dbl\u0026gt;, deviance \u0026lt;dbl\u0026gt;, df.residual \u0026lt;int\u0026gt;, ## # nobs \u0026lt;int\u0026gt;  9. Nesting data To partition data into subsets so that we can apply a common function or operation across all subsets of the data, use dplyr::nest_by().\npenguins %\u0026gt;% nest_by(species) %\u0026gt;% mutate(data_model = list(lm(body_mass_g ~ flipper_length_mm + bill_length_mm + bill_depth_mm, data = data))) ## # A tibble: 3 x 3 ## # Rowwise: species ## species data data_model ## \u0026lt;fct\u0026gt; \u0026lt;list\u0026lt;tbl_df[,8]\u0026gt;\u0026gt; \u0026lt;list\u0026gt; ## 1 Adelie [152 × 8] \u0026lt;lm\u0026gt; ## 2 Chinstrap [68 × 8] \u0026lt;lm\u0026gt; ## 3 Gentoo [124 × 8] \u0026lt;lm\u0026gt;  10. Graphing across subsets To generate plots across data subsets and store them for further usage, use dplyr::nest_by() combined with plotting.\n# generic function for generating a simple scatter plot in ggplot2 scatter_fn \u0026lt;- function(df, col1, col2, title) { df %\u0026gt;% ggplot2::ggplot(aes(x = {{col1}}, y = {{col2}})) + ggplot2::geom_point() + ggplot2::geom_smooth() + ggplot2::labs(title = title) } # run function across species and store plots in a list column penguin_scatters \u0026lt;- penguins %\u0026gt;% dplyr::nest_by(species) %\u0026gt;% dplyr::mutate(plot = list(scatter_fn(data, bill_length_mm, bill_depth_mm, species)))    penguin_scatters ## # A tibble: 3 x 3 ## # Rowwise: species ## species data plot ## \u0026lt;fct\u0026gt; \u0026lt;list\u0026lt;tbl_df[,8]\u0026gt;\u0026gt; \u0026lt;list\u0026gt; ## 1 Adelie [152 × 8] \u0026lt;gg\u0026gt; ## 2 Chinstrap [68 × 8] \u0026lt;gg\u0026gt; ## 3 Gentoo [124 × 8] \u0026lt;gg\u0026gt;   p_all \u0026lt;- scatter_fn(penguins, bill_length_mm, bill_depth_mm, \u0026quot;All Species\u0026quot;) # get species scatters from penguin_scatters dataframe for (i in 1:3) { assign(paste(\u0026quot;p\u0026quot;, i, sep = \u0026quot;_\u0026quot;), penguin_scatters$plot[i][[1]]) } # display nicely using patchwork in R Markdown library(patchwork) p_all / (p_1 | p_2 | p_3)  Thank you! Credit:\n Slides in this presentation are very heavily based on “Ten Up-To-Date Ways to do Common Data Tasks in R” post by Keith McNulty.  Resources:\n Tidy data paper by Wickham, Hadley (2013). Journal of Statistical Software.\n Welcome to the Tidyverse paper by Wickham, Hadley et al. (2019). Journal of Open Source Software.\n Tidyverse blog with updates (in a form of a blog post)\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"7fb56537b98e3c8056c1aec260a36da7","permalink":"/resources/computing_club_materials/2020-12-01-computing_club/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/resources/computing_club_materials/2020-12-01-computing_club/","section":"resources","summary":"tidyverse The tidyverse is an ecosystem of packages designed with a shared underlying design philosophy, grammar, and data structures.\nThe tidyverse v1.3.0 loads 8 packages via library(tidyverse): ggplot2, dplyr, tidyr, purrr, tibble, stringr, forcats, readr.","tags":null,"title":"Recent developments in R tidyverse","type":"resources"},{"authors":null,"categories":null,"content":"  tidyverse The tidyverse is an ecosystem of packages designed with a shared underlying design philosophy, grammar, and data structures.\nThe tidyverse v1.3.0 loads 8 packages via library(tidyverse): ggplot2, dplyr, tidyr, purrr, tibble, stringr, forcats, readr.\nIt also includes (installs) but does not automatically load some other (i.e. lubridate).\n Presentation motivation Me using tidyverse packages in 2015:  Typically do stuff like\ndat %\u0026gt;% filter(age \u0026gt;= 50) %\u0026gt;% group_by(var1, var2) %\u0026gt;% summarize(y_mean = mean(y), y_median = median(y)) and google “r ggplot2 rotate x axis labels”\n Me using tidyverse packages in 2020:  Typically do stuff like\ndat %\u0026gt;% filter(age \u0026gt;= 50) %\u0026gt;% group_by(var1, var2) %\u0026gt;% summarize(y_mean = mean(y), y_median = median(y)) and google “r ggplot2 rotate x axis labels”\n  Presentation content credits  (Post link here.) These slides from now on are like 80-90% of this post content, with small alterations from my side.\n  Outline Palmer Penguins dataset\nSelecting columns in data Reordering columns in data Controlling mutated column location Transforming from wide to long Transforming from long to wide Running group statistics across multiple columns Control how output columns are named when summarising across multiple columns Running models across subsets of data Nesting data Graphing across subsets   Palmer Penguins dataset “The goal of palmerpenguins is to provide a great dataset for data exploration \u0026amp; visualization, as an alternative to iris.”\nData set contains size measurements for three penguin species (Adelie, Chinstrap, Gentoo) observed on three islands in the Palmer Archipelago, Antarctica (Biscoe, Dream, Torgersen).\nlibrary(tidyverse) library(palmerpenguins) str(penguins) ## tibble [344 × 8] (S3: tbl_df/tbl/data.frame) ## $ species : Factor w/ 3 levels \u0026quot;Adelie\u0026quot;,\u0026quot;Chinstrap\u0026quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ island : Factor w/ 3 levels \u0026quot;Biscoe\u0026quot;,\u0026quot;Dream\u0026quot;,..: 3 3 3 3 3 3 3 3 3 3 ... ## $ bill_length_mm : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ... ## $ bill_depth_mm : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ... ## $ flipper_length_mm: int [1:344] 181 186 195 NA 193 190 181 195 193 190 ... ## $ body_mass_g : int [1:344] 3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ... ## $ sex : Factor w/ 2 levels \u0026quot;female\u0026quot;,\u0026quot;male\u0026quot;: 2 1 1 NA 1 2 1 2 NA NA ... ## $ year : int [1:344] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ...    1. Selecting columns in data To select columns using dplyr::select() or tidyr::pivot_longer() based on common conditions, use helper functions.\n To select variables contained in a character vector: all_of(), any_of() To select all (i.e. remaining) or last column: everything(), last_col() To select variables by matching patterns in their names: starts_with(), ends_with(), contains(), matches(), num_range() To apply a custom function and select those for which the function returns TRUE: where()  penguins %\u0026gt;% dplyr::select(!contains(\u0026quot;_\u0026quot;), starts_with(\u0026quot;bill\u0026quot;)) %\u0026gt;% head(n = 3) ## # A tibble: 3 x 6 ## species island sex year bill_length_mm bill_depth_mm ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Adelie Torgersen male 2007 39.1 18.7 ## 2 Adelie Torgersen female 2007 39.5 17.4 ## 3 Adelie Torgersen female 2007 40.3 18   my_select_func \u0026lt;- function(var_name){ return(is.factor(var_name)) } penguins %\u0026gt;% dplyr::select(where(my_select_func))  ## # A tibble: 344 x 3 ## species island sex ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; ## 1 Adelie Torgersen male ## 2 Adelie Torgersen female ## 3 Adelie Torgersen female ## 4 Adelie Torgersen \u0026lt;NA\u0026gt; ## 5 Adelie Torgersen female ## 6 Adelie Torgersen male ## 7 Adelie Torgersen female ## 8 Adelie Torgersen male ## 9 Adelie Torgersen \u0026lt;NA\u0026gt; ## 10 Adelie Torgersen \u0026lt;NA\u0026gt; ## # … with 334 more rows  2. Reordering columns in data To reorder specific columns or sets of columns, use dplyr::relocate() with .before or .after\npenguins %\u0026gt;% dplyr::relocate(contains(\u0026quot;_\u0026quot;), .after = year) %\u0026gt;% head(n = 3) ## # A tibble: 3 x 8 ## species island sex year bill_length_mm bill_depth_mm flipper_length_… ## \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; ## 1 Adelie Torge… male 2007 39.1 18.7 181 ## 2 Adelie Torge… fema… 2007 39.5 17.4 186 ## 3 Adelie Torge… fema… 2007 40.3 18 195 ## # … with 1 more variable: body_mass_g \u0026lt;int\u0026gt; penguins %\u0026gt;% dplyr::relocate(species, .after = last_col()) %\u0026gt;% head(n = 3) ## # A tibble: 3 x 8 ## island bill_length_mm bill_depth_mm flipper_length_… body_mass_g sex year ## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; ## 1 Torge… 39.1 18.7 181 3750 male 2007 ## 2 Torge… 39.5 17.4 186 3800 fema… 2007 ## 3 Torge… 40.3 18 195 3250 fema… 2007 ## # … with 1 more variable: species \u0026lt;fct\u0026gt;  3. Controlling mutated column location To control the location of the newly added column, use dplyr::mutate()’s option (similar to above’s dplyr::relocate())\npenguins \u0026lt;- penguins %\u0026gt;% dplyr::mutate(penguinid = row_number(), .before = everything()) penguins  ## # A tibble: 344 x 9 ## penguinid species island bill_length_mm bill_depth_mm flipper_length_… ## \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;int\u0026gt; ## 1 1 Adelie Torge… 39.1 18.7 181 ## 2 2 Adelie Torge… 39.5 17.4 186 ## 3 3 Adelie Torge… 40.3 18 195 ## 4 4 Adelie Torge… NA NA NA ## 5 5 Adelie Torge… 36.7 19.3 193 ## 6 6 Adelie Torge… 39.3 20.6 190 ## 7 7 Adelie Torge… 38.9 17.8 181 ## 8 8 Adelie Torge… 39.2 19.6 195 ## 9 9 Adelie Torge… 34.1 18.1 193 ## 10 10 Adelie Torge… 42 20.2 190 ## # … with 334 more rows, and 3 more variables: body_mass_g \u0026lt;int\u0026gt;, sex \u0026lt;fct\u0026gt;, ## # year \u0026lt;int\u0026gt;  4. Transforming from wide to long To transform data set from wide(r) to long(er) form, use tidyr::pivot_longer() which is an updated approach to an older tidyr::gather().\npenguins %\u0026gt;% tidyr::pivot_longer(cols = contains(\u0026quot;_\u0026quot;), # pivot these columns names_to = \u0026quot;variable_name\u0026quot;, # name of column containing \u0026quot;old columns\u0026quot; names values_to = \u0026quot;variable_value\u0026quot;) # name of column containing \u0026quot;old columns\u0026quot; values ## # A tibble: 1,376 x 7 ## penguinid species island sex year variable_name variable_value ## \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 Adelie Torgersen male 2007 bill_length_mm 39.1 ## 2 1 Adelie Torgersen male 2007 bill_depth_mm 18.7 ## 3 1 Adelie Torgersen male 2007 flipper_length_mm 181 ## 4 1 Adelie Torgersen male 2007 body_mass_g 3750 ## 5 2 Adelie Torgersen female 2007 bill_length_mm 39.5 ## 6 2 Adelie Torgersen female 2007 bill_depth_mm 17.4 ## 7 2 Adelie Torgersen female 2007 flipper_length_mm 186 ## 8 2 Adelie Torgersen female 2007 body_mass_g 3800 ## 9 3 Adelie Torgersen female 2007 bill_length_mm 40.3 ## 10 3 Adelie Torgersen female 2007 bill_depth_mm 18 ## # … with 1,366 more rows   # as previous example, but simultaneously split the names of columns # which we pivot into longer format by \u0026quot;_\u0026quot; separator penguins_longer \u0026lt;- penguins %\u0026gt;% tidyr::pivot_longer(cols = contains(\u0026quot;_\u0026quot;), # pivot these columns names_sep = \u0026quot;_\u0026quot;, names_to = c(\u0026quot;part\u0026quot;, \u0026quot;measure\u0026quot;, \u0026quot;unit\u0026quot;), # name of column(s) containing \u0026quot;old columns\u0026quot; names values_to = \u0026quot;measure_value\u0026quot; ) # name of column containing \u0026quot;old columns\u0026quot; values penguins_longer  ## # A tibble: 1,376 x 9 ## penguinid species island sex year part measure unit measure_value ## \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;chr\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 Adelie Torgersen male 2007 bill length mm 39.1 ## 2 1 Adelie Torgersen male 2007 bill depth mm 18.7 ## 3 1 Adelie Torgersen male 2007 flipper length mm 181 ## 4 1 Adelie Torgersen male 2007 body mass g 3750 ## 5 2 Adelie Torgersen female 2007 bill length mm 39.5 ## 6 2 Adelie Torgersen female 2007 bill depth mm 17.4 ## 7 2 Adelie Torgersen female 2007 flipper length mm 186 ## 8 2 Adelie Torgersen female 2007 body mass g 3800 ## 9 3 Adelie Torgersen female 2007 bill length mm 40.3 ## 10 3 Adelie Torgersen female 2007 bill depth mm 18 ## # … with 1,366 more rows  5. Transforming from long to wide To transform data set from long(er) to wide(r) form, use tidyr::pivot_wider() which is an updated approach to an older tidyr::spread().\n# revert from long form from previous example penguins_wider \u0026lt;- penguins_longer %\u0026gt;% tidyr::pivot_wider(names_from = c(\u0026quot;part\u0026quot;, \u0026quot;measure\u0026quot;, \u0026quot;unit\u0026quot;), # pivot these columns values_from = \u0026quot;measure_value\u0026quot;, # take the values from here names_sep = \u0026quot;_\u0026quot;) # combine col names using an underscore penguins_wider ## # A tibble: 344 x 9 ## penguinid species island sex year bill_length_mm bill_depth_mm ## \u0026lt;int\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;fct\u0026gt; \u0026lt;int\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 1 Adelie Torge… male 2007 39.1 18.7 ## 2 2 Adelie Torge… fema… 2007 39.5 17.4 ## 3 3 Adelie Torge… fema… 2007 40.3 18 ## 4 4 Adelie Torge… \u0026lt;NA\u0026gt; 2007 NA NA ## 5 5 Adelie Torge… fema… 2007 36.7 19.3 ## 6 6 Adelie Torge… male 2007 39.3 20.6 ## 7 7 Adelie Torge… fema… 2007 38.9 17.8 ## 8 8 Adelie Torge… male 2007 39.2 19.6 ## 9 9 Adelie Torge… \u0026lt;NA\u0026gt; 2007 34.1 18.1 ## 10 10 Adelie Torge… \u0026lt;NA\u0026gt; 2007 42 20.2 ## # … with 334 more rows, and 2 more variables: flipper_length_mm \u0026lt;dbl\u0026gt;, ## # body_mass_g \u0026lt;dbl\u0026gt;  6. Running group statistics across multiple columns To apply multiple summary statistics simultaneously in an efficient way, use across() verb.\n# calculate mean and sd for each variable ending in mm, across three species penguin_stats \u0026lt;- penguins %\u0026gt;% dplyr::group_by(species) %\u0026gt;% dplyr::summarise(across(.cols = ends_with(\u0026quot;mm\u0026quot;), .fns = list(~mean(.x, na.rm = TRUE), ~sd(.x, na.rm = TRUE)))) penguin_stats ## # A tibble: 3 x 7 ## species bill_length_mm_1 bill_length_mm_2 bill_depth_mm_1 bill_depth_mm_2 ## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Adelie 38.8 2.66 18.3 1.22 ## 2 Chinst… 48.8 3.34 18.4 1.14 ## 3 Gentoo 47.5 3.08 15.0 0.981 ## # … with 2 more variables: flipper_length_mm_1 \u0026lt;dbl\u0026gt;, flipper_length_mm_2 \u0026lt;dbl\u0026gt;  7. Control how output columns are named when summarising across multiple columns To apply multiple summary statistics simultaneously in an efficient way with across() verb and to use other than default column names of summary variables, use the .names argument.\npenguins_stats \u0026lt;- penguins %\u0026gt;% dplyr::group_by(species) %\u0026gt;% dplyr::summarise(across(.cols = ends_with(\u0026quot;mm\u0026quot;), .fns = list(mean = ~mean(.x, na.rm = TRUE), sd = ~sd(.x, na.rm = TRUE)), .names = \u0026quot;{gsub(\u0026#39;_|_mm\u0026#39;, \u0026#39;\u0026#39;, col)}_{.fn}\u0026quot;)) penguins_stats ## # A tibble: 3 x 7 ## species billlength_mean billlength_sd billdepth_mean billdepth_sd ## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Adelie 38.8 2.66 18.3 1.22 ## 2 Chinst… 48.8 3.34 18.4 1.14 ## 3 Gentoo 47.5 3.08 15.0 0.981 ## # … with 2 more variables: flipperlength_mean \u0026lt;dbl\u0026gt;, flipperlength_sd \u0026lt;dbl\u0026gt;  8. Running models across subsets of data Use dplyr::summarise() to compute different types of outcomes stored in a list, for example, summary vectors, data frames or other objects like models or graphs.\npenguin_models \u0026lt;- penguins %\u0026gt;% dplyr::group_by(species) %\u0026gt;% dplyr::summarise(model = list(lm(body_mass_g ~ flipper_length_mm + bill_length_mm + bill_depth_mm))) penguin_models ## # A tibble: 3 x 2 ## species model ## \u0026lt;fct\u0026gt; \u0026lt;list\u0026gt; ## 1 Adelie \u0026lt;lm\u0026gt; ## 2 Chinstrap \u0026lt;lm\u0026gt; ## 3 Gentoo \u0026lt;lm\u0026gt; model_tmp \u0026lt;- penguin_models[1, 2][[1]][[1]] # summary(model_tmp)   library(broom) penguin_models \u0026lt;- penguins %\u0026gt;% dplyr::group_by(species) %\u0026gt;% dplyr::summarise(broom::glance(lm(body_mass_g ~ flipper_length_mm + bill_length_mm + bill_depth_mm))) penguin_models ## # A tibble: 3 x 13 ## species r.squared adj.r.squared sigma statistic p.value df logLik AIC ## \u0026lt;fct\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; \u0026lt;dbl\u0026gt; ## 1 Adelie 0.508 0.498 325. 50.6 1.55e-22 3 -1086. 2181. ## 2 Chinst… 0.504 0.481 277. 21.7 8.48e-10 3 -477. 964. ## 3 Gentoo 0.625 0.615 313. 66.0 3.39e-25 3 -879. 1768. ## # … with 4 more variables: BIC \u0026lt;dbl\u0026gt;, deviance \u0026lt;dbl\u0026gt;, df.residual \u0026lt;int\u0026gt;, ## # nobs \u0026lt;int\u0026gt;  9. Nesting data To partition data into subsets so that we can apply a common function or operation across all subsets of the data, use dplyr::nest_by().\npenguins %\u0026gt;% nest_by(species) %\u0026gt;% mutate(data_model = list(lm(body_mass_g ~ flipper_length_mm + bill_length_mm + bill_depth_mm, data = data))) ## # A tibble: 3 x 3 ## # Rowwise: species ## species data data_model ## \u0026lt;fct\u0026gt; \u0026lt;list\u0026lt;tbl_df[,8]\u0026gt;\u0026gt; \u0026lt;list\u0026gt; ## 1 Adelie [152 × 8] \u0026lt;lm\u0026gt; ## 2 Chinstrap [68 × 8] \u0026lt;lm\u0026gt; ## 3 Gentoo [124 × 8] \u0026lt;lm\u0026gt;  10. Graphing across subsets To generate plots across data subsets and store them for further usage, use dplyr::nest_by() combined with plotting.\n# generic function for generating a simple scatter plot in ggplot2 scatter_fn \u0026lt;- function(df, col1, col2, title) { df %\u0026gt;% ggplot2::ggplot(aes(x = {{col1}}, y = {{col2}})) + ggplot2::geom_point() + ggplot2::geom_smooth() + ggplot2::labs(title = title) } # run function across species and store plots in a list column penguin_scatters \u0026lt;- penguins %\u0026gt;% dplyr::nest_by(species) %\u0026gt;% dplyr::mutate(plot = list(scatter_fn(data, bill_length_mm, bill_depth_mm, species)))    penguin_scatters ## # A tibble: 3 x 3 ## # Rowwise: species ## species data plot ## \u0026lt;fct\u0026gt; \u0026lt;list\u0026lt;tbl_df[,8]\u0026gt;\u0026gt; \u0026lt;list\u0026gt; ## 1 Adelie [152 × 8] \u0026lt;gg\u0026gt; ## 2 Chinstrap [68 × 8] \u0026lt;gg\u0026gt; ## 3 Gentoo [124 × 8] \u0026lt;gg\u0026gt;   p_all \u0026lt;- scatter_fn(penguins, bill_length_mm, bill_depth_mm, \u0026quot;All Species\u0026quot;) # get species scatters from penguin_scatters dataframe for (i in 1:3) { assign(paste(\u0026quot;p\u0026quot;, i, sep = \u0026quot;_\u0026quot;), penguin_scatters$plot[i][[1]]) } # display nicely using patchwork in R Markdown library(patchwork) p_all / (p_1 | p_2 | p_3)  Thank you! Credit:\n Slides in this presentation are very heavily based on “Ten Up-To-Date Ways to do Common Data Tasks in R” post by Keith McNulty.  Resources:\n Tidy data paper by Wickham, Hadley (2013). Journal of Statistical Software.\n Welcome to the Tidyverse paper by Wickham, Hadley et al. (2019). Journal of Open Source Software.\n Tidyverse blog with updates (in a form of a blog post)\n   ","date":-62135596800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":-62135596800,"objectID":"2f7733506400ead155278a902a7bed2e","permalink":"/slides/2020-12-01-computing_club/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/slides/2020-12-01-computing_club/","section":"slides","summary":"tidyverse The tidyverse is an ecosystem of packages designed with a shared underlying design philosophy, grammar, and data structures.\nThe tidyverse v1.3.0 loads 8 packages via library(tidyverse): ggplot2, dplyr, tidyr, purrr, tibble, stringr, forcats, readr.","tags":null,"title":"Recent developments in R tidyverse","type":"slides"}]