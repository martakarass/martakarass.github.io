<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts | Home</title>
    <link>/post/</link>
      <atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    <description>Posts</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Wed, 05 Jan 2022 17:19:28 -0400</lastBuildDate>
    <image>
      <url>/images/icon_hufdd866d90d76849587aac6fbf27da1ac_464_512x512_fill_lanczos_center_2.png</url>
      <title>Posts</title>
      <link>/post/</link>
    </image>
    
    <item>
      <title>GPS data in R: parse and plot GPX data exported from Strava</title>
      <link>/post/2022-01-05-gps_strava_read_and_viz/</link>
      <pubDate>Wed, 05 Jan 2022 17:19:28 -0400</pubDate>
      <guid>/post/2022-01-05-gps_strava_read_and_viz/</guid>
      <description>&lt;p&gt;In this post, we:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;use a GPX file with with geographic information, exported from
Strava from one running activity,&lt;/li&gt;
&lt;li&gt;parse and plot the data.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#gpx-data-from-strava&#34;&gt;GPX data from Strava&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#parsing-gpx&#34;&gt;Parsing GPX&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#computing-distance-time-elapsed-and-speed&#34;&gt;Computing distance, time elapsed and speed&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#plot-elevation-speed&#34;&gt;Plot elevation, speed&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#plot-run-path&#34;&gt;Plot run path&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#acknowledgements&#34;&gt;Acknowledgements&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;h2 id=&#34;gpx-data-from-strava&#34;&gt;GPX data from Strava&lt;/h2&gt;
&lt;p&gt;GPX stands for “GPS Exchange Format”. It is an XML schema commonly used
for storing GPS data.&lt;/p&gt;
&lt;p&gt;Strava (Strava, Inc; San Francisco, CA) is a popular activity tracker
app I have been using for a few weeks. Strava allows to export GPS data
collected during a recorded activity in a GPX format. To export the
data, go to Strava activity page &amp;gt; “three dots” button &amp;gt; Export
GPX.&lt;/p&gt;
&lt;p&gt;I downloaded GPX data from a run I did on Jan 1, 2022. The run distance
is 10.88 km and spans 1:03:49 time. The data is available on my GitHub
and can be downloaded using the code below.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;
(Click to see the code to download the data.)
&lt;/summary&gt;
&lt;pre&gt;&lt;code&gt;url &amp;lt;- paste0(
  &amp;quot;https://raw.githubusercontent.com/martakarass/gps-stats/main/data/&amp;quot;,
  &amp;quot;/Morning_Run_2022-01-01.gpx&amp;quot;)
fpath &amp;lt;- paste0(
  &amp;quot;/Users/martakaras/Downloads&amp;quot;,
  &amp;quot;/Morning_Run_2022-01-01.gpx&amp;quot;)
# download 
result &amp;lt;- curl::curl_download(url, destfile = fpath, quiet = FALSE)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;/br&gt;
&lt;h2 id=&#34;parsing-gpx&#34;&gt;Parsing GPX&lt;/h2&gt;
&lt;p&gt;First, we parse the GPX file and put the extracted data trajectories
into a data frame:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;timestamp,&lt;/li&gt;
&lt;li&gt;latitude,&lt;/li&gt;
&lt;li&gt;longitude,&lt;/li&gt;
&lt;li&gt;elevation.&lt;/li&gt;
&lt;/ul&gt;
&lt;details&gt;
&lt;summary&gt;
(Click to see the code.)
&lt;/summary&gt;
&lt;pre&gt;&lt;code&gt;# rm(list = ls())
library(tidyverse)
library(here)
library(XML)
library(lubridate)
library(ggmap)
library(geosphere)
options(digits.secs = 3)
options(scipen = 999)

# parse GPX file
path_tmp &amp;lt;- paste0(&amp;quot;/Users/martakaras/Downloads/Morning_Run_2022-01-01.gpx&amp;quot;)
parsed &amp;lt;- htmlTreeParse(file = path_tmp, useInternalNodes = TRUE)

# get values via via the respective xpath
coords &amp;lt;- xpathSApply(parsed, path = &amp;quot;//trkpt&amp;quot;, xmlAttrs)
elev   &amp;lt;- xpathSApply(parsed, path = &amp;quot;//trkpt/ele&amp;quot;, xmlValue)
ts_chr &amp;lt;- xpathSApply(parsed, path = &amp;quot;//trkpt/time&amp;quot;, xmlValue)

# combine into df 
dat_df &amp;lt;- data.frame(
  ts_POSIXct = ymd_hms(ts_chr, tz = &amp;quot;EST&amp;quot;),
  lat = as.numeric(coords[&amp;quot;lat&amp;quot;,]), 
  lon = as.numeric(coords[&amp;quot;lon&amp;quot;,]), 
  elev = as.numeric(elev)
)
head(dat_df)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;pre&gt;&lt;code&gt;           ts_POSIXct      lat       lon elev
1 2022-01-01 09:42:01 42.32791 -71.10868 23.0
2 2022-01-01 09:42:06 42.32791 -71.10868 23.0
3 2022-01-01 09:42:08 42.32795 -71.10866 23.2
4 2022-01-01 09:42:10 42.32817 -71.10872 24.7
5 2022-01-01 09:42:11 42.32816 -71.10875 24.7
6 2022-01-01 09:42:12 42.32814 -71.10875 23.8
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;computing-distance-time-elapsed-and-speed&#34;&gt;Computing distance, time elapsed and speed&lt;/h2&gt;
&lt;p&gt;Next, we compute:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;distance (in meters) between subsequent GPS recordings&lt;/li&gt;
&lt;li&gt;time elapsed (in seconds) between subsequent GPS recordings,&lt;/li&gt;
&lt;li&gt;speed (metres per seconds, kilometres per hour) – temporal, based on
subsequent GPS recordings.&lt;/li&gt;
&lt;/ul&gt;
&lt;details&gt;
&lt;summary&gt;
(Click to see the code.)
&lt;/summary&gt;
&lt;pre&gt;&lt;code&gt;# compute distance (in meters) between subsequent GPS points
dat_df &amp;lt;- 
  dat_df %&amp;gt;%
  mutate(lat_lead = lead(lat)) %&amp;gt;%
  mutate(lon_lead = lead(lon)) %&amp;gt;%
  rowwise() %&amp;gt;%
  mutate(dist_to_lead_m = distm(c(lon, lat), c(lon_lead, lat_lead), fun = distHaversine)[1,1]) %&amp;gt;%
  ungroup()

# compute time elapsed (in seconds) between subsequent GPS points
dat_df &amp;lt;- 
  dat_df %&amp;gt;%
  mutate(ts_POSIXct_lead = lead(ts_POSIXct)) %&amp;gt;%
  mutate(ts_diff_s = as.numeric(difftime(ts_POSIXct_lead, ts_POSIXct, units = &amp;quot;secs&amp;quot;))) 

# compute metres per seconds, kilometres per hour 
dat_df &amp;lt;- 
  dat_df %&amp;gt;%
  mutate(speed_m_per_sec = dist_to_lead_m / ts_diff_s) %&amp;gt;%
  mutate(speed_km_per_h = speed_m_per_sec * 3.6)

# remove some columns we won&#39;t use anymore 
dat_df &amp;lt;- 
  dat_df %&amp;gt;% 
  select(-c(lat_lead, lon_lead, ts_POSIXct_lead, ts_diff_s))
head(dat_df) %&amp;gt;% as.data.frame()
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;pre&gt;&lt;code&gt;           ts_POSIXct      lat       lon elev dist_to_lead_m ts_diff_s speed_m_per_sec speed_km_per_h
1 2022-01-01 09:42:01 42.32791 -71.10868 23.0       0.000000         5        0.000000       0.000000
2 2022-01-01 09:42:06 42.32791 -71.10868 23.0       4.406590         2        2.203295       7.931862
3 2022-01-01 09:42:08 42.32795 -71.10866 23.2      25.403927         2       12.701963      45.727068
4 2022-01-01 09:42:10 42.32817 -71.10872 24.7       2.510645         1        2.510645       9.038324
5 2022-01-01 09:42:11 42.32816 -71.10875 24.7       2.264098         1        2.264098       8.150751
6 2022-01-01 09:42:12 42.32814 -71.10875 23.8       1.899407         1        1.899407       6.837864
&lt;/code&gt;&lt;/pre&gt;
&lt;h2 id=&#34;plot-elevation-speed&#34;&gt;Plot elevation, speed&lt;/h2&gt;
&lt;p&gt;Plot elevation&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;
(Click to see the code.)
&lt;/summary&gt;
&lt;pre&gt;&lt;code&gt;plt_elev &amp;lt;- 
  ggplot(dat_df, aes(x = ts_POSIXct, y = elev)) + 
  geom_line() + labs(x = &amp;quot;Time&amp;quot;, y = &amp;quot;Elevation [m]&amp;quot;) + 
  theme_grey(base_size = 14)
plt_elev
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;p&gt;&lt;img src=&#34;plt_elev.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;Plot speed&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;
(Click to see the code.)
&lt;/summary&gt;
&lt;pre&gt;&lt;code&gt;plt_speed_km_per_h &amp;lt;- 
  ggplot(dat_df, aes(x = ts_POSIXct, y = speed_km_per_h)) + 
  geom_line() + labs(x = &amp;quot;Time&amp;quot;, y = &amp;quot;Speed [km/h]&amp;quot;) + 
  theme_grey(base_size = 14)
plt_speed_km_per_h
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;p&gt;&lt;img src=&#34;plt_speed_km_per_h.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;The above plot is very wiggly due to small time increment over which the
speed statistic was computed. It could be made more smooth by computing
speed using e.g. 10 seconds intervals or using data smother such LOWESS.&lt;/p&gt;
&lt;h2 id=&#34;plot-run-path&#34;&gt;Plot run path&lt;/h2&gt;
&lt;p&gt;The simple, &lt;code&gt;graphics&lt;/code&gt;-based version of the trajectory plot:&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;
(Click to see the code.)
&lt;/summary&gt;
&lt;pre&gt;&lt;code&gt;plot(rev(dat_df$lon), rev(dat_df$lat), 
     type = &amp;quot;l&amp;quot;, col = &amp;quot;red&amp;quot;, lwd = 3, bty = &amp;quot;n&amp;quot;, 
     ylab = &amp;quot;Latitude&amp;quot;, xlab = &amp;quot;Longitude&amp;quot;)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;p&gt;&lt;img src=&#34;plt_path_simple.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;More fancy plot can be generated with &lt;code&gt;ggmap&lt;/code&gt; package. I used labels to
mark each kilometer passed. My Google API key is registered hence I
could also &lt;a href=&#34;https://cran.r-project.org/web/packages/ggmap/readme/README.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;access the Google map for use with
&lt;code&gt;ggmap&lt;/code&gt;&lt;/a&gt;
(code for API key registration not showed.)&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;
(Click to see the code.)
&lt;/summary&gt;
&lt;pre&gt;&lt;code&gt;# get the map background 
bbox &amp;lt;- make_bbox(range(dat_df$lon), range(dat_df$lat))
dat_df_map &amp;lt;- get_map(bbox, maptype = &amp;quot;roadmap&amp;quot;, source = &amp;quot;google&amp;quot;)
dat_df_map &amp;lt;- get_googlemap(center = c(mean(range(dat_df$lon)), mean(range(dat_df$lat))), zoom = 15)
# no Google token alternative: 
# dat_df_map &amp;lt;- get_map(bbox, maptype = &amp;quot;toner-lite&amp;quot;, source = &amp;quot;stamen&amp;quot;)

# data frame to add distance marks
dat_df_dist_marks &amp;lt;- 
  dat_df %&amp;gt;% 
  mutate(dist_m_cumsum = cumsum(dist_to_lead_m)) %&amp;gt;%
  mutate(dist_m_cumsum_km_floor = floor(dist_m_cumsum / 1000)) %&amp;gt;%
  group_by(dist_m_cumsum_km_floor) %&amp;gt;%
  filter(row_number() == 1, dist_m_cumsum_km_floor &amp;gt; 0) 

# generate 
plt_path_fancy &amp;lt;- 
  ggmap(dat_df_map) + 
  geom_point(data = dat_df, aes(lon, lat, col = elev),
             size = 1, alpha = 0.7) +
  geom_label(data = dat_df_dist_marks, aes(lon, lat, label = dist_m_cumsum_km_floor),
             size = 3) +
  labs(x = &amp;quot;Longitude&amp;quot;, y = &amp;quot;Latitude&amp;quot;, color = &amp;quot;Elev. [m]&amp;quot;,
       title=&amp;quot;Track of one Marta&#39;s run on 2022-01-01&amp;quot;)
plt_path_fancy
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;p&gt;&lt;img src=&#34;plt_path_fancy.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;acknowledgements&#34;&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;The above content is inspired by AND borrows some code from &lt;a href=&#34;https://rpubs.com/ials2un/gpx1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Plotting GPS tracks with
R&lt;/a&gt; by Ivan Lizarazo, who in turn based
their content on &lt;a href=&#34;https://rpubs.com/ials2un/gpx1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Stay on track: Plotting GPS tracks with
R&lt;/a&gt; by Sascha Wolfer. My main
contributions are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;identifying (TTBOMK) an error in their shared code for computing
of subsequent locations distance, and using an alternative way,&lt;/li&gt;
&lt;li&gt;parsing GPX timestamp with &lt;code&gt;lubridate&lt;/code&gt; function,&lt;/li&gt;
&lt;li&gt;ggplots, including Google maps-based ggplot of the run path,&lt;/li&gt;
&lt;li&gt;providing my run data as open source GPS data set.&lt;/li&gt;
&lt;/ul&gt;</description>
    </item>
    
    <item>
      <title>Computing minute-level summary measures of physical activity from raw accelerometry data in R: AC, MIMS, ENMO, MAD, and AI</title>
      <link>/post/2021-06-29-pa_measures_and_summarizedactigraphy/</link>
      <pubDate>Sat, 10 Jul 2021 17:19:28 -0400</pubDate>
      <guid>/post/2021-06-29-pa_measures_and_summarizedactigraphy/</guid>
      <description>&lt;p&gt;In this post, we:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;use dataset “Labeled raw accelerometry data captured during walking,
stair climbing and driving” that is freely available on PhysioNet;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;derive four minute-level summary measures of physical activity – AC,
MIMS, ENMO, MAD, AI – from raw accelerometry data using
&lt;code&gt;SummarizedActigraphy&lt;/code&gt; R package;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;summarize minute-level summary measures across walking and driving
activities.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#dataset-labeled-raw-accelerometry-data&#34;&gt;Dataset “Labeled raw accelerometry data”&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#computing-minute-level-summary-measures-of-raw-accelerometry-data&#34;&gt;Computing minute-level summary measures of raw accelerometry data&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#summary-measures&#34;&gt;Summary measures&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#summarizedactigraphy-r-package&#34;&gt;SummarizedActigraphy R package&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#preparing-the-data-input&#34;&gt;Preparing the data input&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#computing-the-measures&#34;&gt;Computing the measures&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#minute-level-physical-activity-measures-during-walking-and-driving-activities&#34;&gt;Minute-level physical activity measures during walking and driving activities&lt;/a&gt;
      &lt;ul&gt;
        &lt;li&gt;&lt;a href=&#34;#merging-physical-activity-measures-with-physical-activity-labels&#34;&gt;Merging physical activity measures with physical activity labels&lt;/a&gt;&lt;/li&gt;
        &lt;li&gt;&lt;a href=&#34;#visualizing-physical-activity-measures-across-activity-types&#34;&gt;Visualizing physical activity measures across activity types&lt;/a&gt;&lt;/li&gt;
      &lt;/ul&gt;
    &lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#acknowledgements&#34;&gt;Acknowledgements&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#span-stylecolorredcitation-infospan&#34;&gt;&lt;span style=&#34;color:red&#34;&gt;Citation info&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;h2 id=&#34;dataset-labeled-raw-accelerometry-data&#34;&gt;Dataset “Labeled raw accelerometry data”&lt;/h2&gt;
&lt;p&gt;The dataset contains raw accelerometry data collected during outdoor
walking, stair climbing, and driving for n=32 healthy adults. It is
freely is available to download on PhysioNet at:
&lt;a href=&#34;https://doi.org/10.13026/51h0-a262&#34;&gt;https://doi.org/10.13026/51h0-a262&lt;/a&gt;&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The study was led by Dr. Jaroslaw Harezlak, assisted by Drs. William
Fadel and Jacek Urbanek.&lt;/li&gt;
&lt;li&gt;Accelerometry data were collected simultaneously at four body
locations: left wrist, left hip, left ankle, and right ankle, at a
sampling frequency of 100 Hz.&lt;/li&gt;
&lt;li&gt;The 3-axial ActiGraph GT3X+ devices were used to collect the data.&lt;/li&gt;
&lt;li&gt;The data include labels of activity type performed for each time
point of data collection (1=walking; 2=descending stairs;
3=ascending stairs; 4=driving; 77=clapping; 99=non-study activity).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We downloaded the data zip from PhysioNet and unpacked. This can be done
manually from the website, or using download the data programmatically.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;
(Click to see the code to download the data.)
&lt;/summary&gt;
&lt;pre&gt;&lt;code&gt;url &amp;lt;- paste0(
  &amp;quot;https://physionet.org/static/published-projects&amp;quot;,
  &amp;quot;/accelerometry-walk-climb-drive&amp;quot;,
  &amp;quot;/labeled-raw-accelerometry-data-captured-during-walking-stair-climbing-and-driving-1.0.0.zip&amp;quot;)
destfile &amp;lt;- paste0(
  &amp;quot;/Users/martakaras/Downloads&amp;quot;,
  &amp;quot;/labeled-raw-accelerometry-data-captured-during-walking-stair-climbing-and-driving-1.0.0.zip&amp;quot;)
# download zip
result &amp;lt;- curl::curl_download(url, destfile = destfile, quiet = FALSE)
# unzip zip 
unzip(zipfile = destfile, exdir = dirname(destfile))
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;/br&gt;
&lt;p&gt;First, let’s look at raw accelerometry data of a single subject.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;
(Click to see the code.)
&lt;/summary&gt;
&lt;pre&gt;&lt;code&gt;library(tidyverse)
library(lubridate)
library(lme4)
library(knitr)
# remotes::install_github(&amp;quot;muschellij2/SummarizedActigraphy&amp;quot;)
library(SummarizedActigraphy)
library(MIMSunit)
library(activityCounts)
options(digits.secs = 3)
options(scipen=999)

# define path to raw data files directory
raw_files_dir &amp;lt;- paste0(
  &amp;quot;/Users/martakaras/Downloads&amp;quot;,
  &amp;quot;/labeled-raw-accelerometry-data-captured-during-walking-stair-climbing-and-driving-1.0.0&amp;quot;,
  &amp;quot;/raw_accelerometry_data&amp;quot;)

# single participant&#39;s raw accelerometry data
dat_fpaths &amp;lt;- list.files(raw_files_dir, full.names = TRUE)
dat_i &amp;lt;- read_csv(dat_fpaths[1])
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;pre&gt;&lt;code&gt;dat_i

# A tibble: 303,300 x 14
   activity time_s   lw_x   lw_y   lw_z   lh_x   lh_y   lh_z   la_x  la_y   la_z
      &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
 1       99   0.01  0.039  1.02  -0.02  -0.18   1.23   0.023  0.156 0.855 -0.582
 2       99   0.02 -0.629 -0.461  0.973 -0.246  0.137  0.969 -0.707 0.559  0.449
 3       99   0.03 -0.926 -1.26   0.691  0.238 -0.328  1.22  -1.44  1.37   0.367
 4       99   0.04 -0.871 -1.50  -0.246  0.711 -0.484  0.414 -1.66  1.64  -0.543
 5       99   0.05 -0.727 -1.62  -0.559  1.03  -0.297  0.145 -1.76  1.68  -0.918
 6       99   0.06 -0.543 -1.66  -0.629  1.12  -0.246  0.137 -1.80  1.65  -0.988
 7       99   0.07 -0.348 -1.64  -0.609  1.24  -0.426  0.047 -1.76  1.57  -0.992
 8       99   0.08 -0.16  -1.60  -0.566  1.18  -0.539 -0.008 -1.63  1.53  -1.02 
 9       99   0.09 -0.012 -1.53  -0.523  1.03  -0.633 -0.043 -1.13  1.87  -0.738
10       99   0.1   0.117 -1.43  -0.484  0.922 -0.766 -0.047  0.285 1.37  -0.156
# … with 303,290 more rows, and 3 more variables: ra_x &amp;lt;dbl&amp;gt;, ra_y &amp;lt;dbl&amp;gt;,
#   ra_z &amp;lt;dbl&amp;gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Each file contains 14 variables:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;activity&lt;/code&gt; – type of activity (1=walking; 2=descending stairs;
3=ascending stairs; 4=driving; 77=clapping; 99=non-study activity);&lt;/li&gt;
&lt;li&gt;&lt;code&gt;time_s&lt;/code&gt; – time from device initiation (seconds [s]);&lt;/li&gt;
&lt;li&gt;&lt;code&gt;lw_x&lt;/code&gt;, &lt;code&gt;lw_y&lt;/code&gt;, &lt;code&gt;lw_z&lt;/code&gt; – acceleration [&lt;em&gt;g&lt;/em&gt;] measured by a left
wrist-worn sensor at axis x, y, z;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;lh_x&lt;/code&gt;, &lt;code&gt;lh_y&lt;/code&gt;, &lt;code&gt;lh_z&lt;/code&gt; – acceleration [&lt;em&gt;g&lt;/em&gt;] measured by a left
hip-worn sensor at axis x, y, z;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;la_x&lt;/code&gt;, &lt;code&gt;la_y&lt;/code&gt;, &lt;code&gt;la_z&lt;/code&gt; – acceleration [&lt;em&gt;g&lt;/em&gt;] measured by a left
ankle-worn sensor at axis x, y, z;&lt;/li&gt;
&lt;li&gt;&lt;code&gt;ra_x&lt;/code&gt;, &lt;code&gt;ra_y&lt;/code&gt;, &lt;code&gt;ra_z&lt;/code&gt; – acceleration [&lt;em&gt;g&lt;/em&gt;] measured by a right
ankle-worn sensor at axis x, y, z.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;An exemplary few seconds of raw data from of walking and driving
activities, collected by sensors at four different locations:&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;
(Click to see the code.)
&lt;/summary&gt;
&lt;pre&gt;&lt;code&gt;loc_id_levels &amp;lt;- c(&amp;quot;lw&amp;quot;, &amp;quot;lh&amp;quot;, &amp;quot;la&amp;quot;, &amp;quot;ra&amp;quot;)
loc_id_labels &amp;lt;- c(&amp;quot;left wrist&amp;quot;, &amp;quot;left hip&amp;quot;, &amp;quot;left ankle&amp;quot;, &amp;quot;right ankle&amp;quot;)
activity_levels &amp;lt;- c(&amp;quot;walking&amp;quot;, &amp;quot;driving&amp;quot;)
plt_df &amp;lt;- 
  dat_i %&amp;gt;%
  filter(activity %in% c(1,4)) %&amp;gt;%
  group_by(activity) %&amp;gt;%
  mutate(time_s = time_s - min(time_s)) %&amp;gt;%
  ungroup() %&amp;gt;%
  filter(time_s &amp;gt;= (5 + 5), time_s &amp;lt; (5 + 10)) %&amp;gt;%
  mutate(activity = recode(activity, &#39;1&#39; = &#39;walking&#39;, &#39;4&#39; = &#39;driving&#39;)) %&amp;gt;%
  pivot_longer(cols = -c(activity, time_s)) %&amp;gt;%
  separate(name, c(&amp;quot;loc_id&amp;quot;, &amp;quot;axis_id&amp;quot;), sep = &amp;quot;_&amp;quot;) %&amp;gt;%
  mutate(activity = factor(activity, levels = activity_levels)) %&amp;gt;%
  mutate(loc_id = factor(loc_id, levels = loc_id_levels, labels = loc_id_labels)) 
plt_raw_data &amp;lt;- 
  ggplot(plt_df, aes(x = time_s, y = value, color = axis_id)) + 
  geom_line() + 
  facet_grid(loc_id ~ activity) + 
  scale_y_continuous(limits = c(-1, 1) * max(abs(plt_df$value))) + 
  labs(y = &amp;quot;Acceleration [g]&amp;quot;,
       x = &amp;quot;Time since activity started [s]&amp;quot;,
       color = &amp;quot;Sensor axis: &amp;quot;) + 
  theme_gray(base_size = 14) + 
  theme(legend.position=&amp;quot;bottom&amp;quot;) + 
  scale_color_manual(values = c(&amp;quot;red&amp;quot;, &amp;quot;darkgreen&amp;quot;, &amp;quot;blue&amp;quot;))
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;pre&gt;&lt;code&gt;plt_raw_data
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;plt_raw_data.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;computing-minute-level-summary-measures-of-raw-accelerometry-data&#34;&gt;Computing minute-level summary measures of raw accelerometry data&lt;/h2&gt;
&lt;h3 id=&#34;summary-measures&#34;&gt;Summary measures&lt;/h3&gt;
&lt;p&gt;A number of open-source measures have been proposed to aggregate
subsecond-level accelerometry data. These include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Monitor Independent Movement Summary (MIMS) (&lt;a href=&#34;https://journals.humankinetics.com/view/journals/jmpb/2/4/article-p268.xml&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;John et al.,
2019&lt;/a&gt;),&lt;/li&gt;
&lt;li&gt;Euclidean Norm Minus One (ENMO) (&lt;a href=&#34;https://pubmed.ncbi.nlm.nih.gov/25103964/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;van Hees et al.,
2013&lt;/a&gt;),&lt;/li&gt;
&lt;li&gt;Mean Amplitude Deviation (MAD) (&lt;a href=&#34;https://pubmed.ncbi.nlm.nih.gov/24393233/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Vähä-Ypyä et al.,
2015&lt;/a&gt;),&lt;/li&gt;
&lt;li&gt;Activity Index (AI) (&lt;a href=&#34;https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0160644&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Bai et al.,
2016&lt;/a&gt;).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;In addition, attempts have been made to reverse-engineer proprietary
activity counts (AC) generated by ActiLife software from data collected
by ActiGraph accelerometers. These include:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;activityCounts&lt;/code&gt; R package – allows generating “ActiLife counts from
raw acceleration data for different accelerometer brands and it is
developed based on the study done by &lt;a href=&#34;https://pubmed.ncbi.nlm.nih.gov/28604558/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Brond et al.,
2017&lt;/a&gt;”.&lt;/li&gt;
&lt;/ul&gt;
&lt;h3 id=&#34;summarizedactigraphy-r-package&#34;&gt;SummarizedActigraphy R package&lt;/h3&gt;
&lt;p&gt;We derive the measures using &lt;code&gt;SummarizedActigraphy&lt;/code&gt; R package. The
package was authored by &lt;a href=&#34;https://github.com/muschellij2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;John Muschelli&lt;/a&gt;
and is available on GitHub
(&lt;a href=&#34;https://github.com/muschellij2/SummarizedActigraphy&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;link&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;It uses some of the existing R software under the hood to compute some
part of the measures:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;MIMSunit&lt;/code&gt; R package to compute MIMS,&lt;/li&gt;
&lt;li&gt;&lt;code&gt;activityCounts&lt;/code&gt; R package to compute AC.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;It computes some other measures straightfoward from their definition,
e.g.: MAD, AI.&lt;/p&gt;
&lt;p&gt;Note: the recommendations in the ENMO paper state data calibration
preprocessing step. I observed an
&lt;a href=&#34;https://github.com/muschellij2/SummarizedActigraphy/issues/2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;issue&lt;/a&gt;
while attempting to run the calibration for files of less than 2h of
duration which is the case for the exemplary data we use here. Therefore, here, we do not use data calibration. However, we
successfully used &lt;code&gt;SummarizedActigraphy&lt;/code&gt; R package to calibrate data and compute ENMO for hundreds
of longer duration data files in the past &amp;ndash; &lt;a href=&#34;https://github.com/muschellij2/blsa_mims/blob/master/code/data_preprocessing/mat_calibrated_to_open_source_measures.R&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;see this R
script&lt;/a&gt;.&lt;/p&gt;
&lt;h3 id=&#34;preparing-the-data-input&#34;&gt;Preparing the data input&lt;/h3&gt;
&lt;p&gt;We use raw accelerometry data from a wrist-worn sensor.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;
(Click to see notes and code.)
&lt;/summary&gt;
&lt;p&gt;Note:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;code&gt;SummarizedActigraphy&lt;/code&gt; R package needs data with observation
timestamp in the form of &lt;code&gt;POSIXct&lt;/code&gt; class. This timestamp column must
be named &lt;code&gt;HEADER_TIME_STAMP&lt;/code&gt;; the specific column naming is needed
due to &lt;a href=&#34;https://github.com/mHealthGroup/MIMSunit/issues/29&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;some
issue&lt;/a&gt; reported
for &lt;code&gt;MIMSunit&lt;/code&gt; R package.&lt;/li&gt;
&lt;li&gt;We hence create a “fake” &lt;code&gt;POSIXct&lt;/code&gt; column named
&lt;code&gt;HEADER_TIME_STAMP&lt;/code&gt;.&lt;/li&gt;
&lt;li&gt;The option &lt;code&gt;options(digits.secs = 3)&lt;/code&gt; was used on the top of this R
script to allow displaying decimal time part.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;We first demonstrate the code for one participant.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;dat_input_i &amp;lt;- 
  dat_i %&amp;gt;% 
  mutate(HEADER_TIME_STAMP = seq(from = ymd_hms(&amp;quot;2021-01-01 00:00:00&amp;quot;), by = 0.01, length.out = nrow(dat_i))) %&amp;gt;%
  select(HEADER_TIME_STAMP, X = lw_x, Y = lw_y, Z = lw_z)
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;pre&gt;&lt;code&gt;dat_input_i

# A tibble: 303,300 x 4
   HEADER_TIME_STAMP            X      Y      Z
   &amp;lt;dttm&amp;gt;                   &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;
 1 2021-01-01 00:00:00.000  0.039  1.02  -0.02 
 2 2021-01-01 00:00:00.009 -0.629 -0.461  0.973
 3 2021-01-01 00:00:00.019 -0.926 -1.26   0.691
 4 2021-01-01 00:00:00.029 -0.871 -1.50  -0.246
 5 2021-01-01 00:00:00.039 -0.727 -1.62  -0.559
 6 2021-01-01 00:00:00.049 -0.543 -1.66  -0.629
 7 2021-01-01 00:00:00.059 -0.348 -1.64  -0.609
 8 2021-01-01 00:00:00.069 -0.16  -1.60  -0.566
 9 2021-01-01 00:00:00.079 -0.012 -1.53  -0.523
10 2021-01-01 00:00:00.089  0.117 -1.43  -0.484
# … with 303,290 more rows
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;We use package’s helper function to check if sample rate is being
determined correctly (should be 100 Hz).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;get_sample_rate(dat_input_i)

[1] 100
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;h3 id=&#34;computing-the-measures&#34;&gt;Computing the measures&lt;/h3&gt;
&lt;p&gt;We calculate minute-level summary measures: AC, MIMS, ENMO, MAD, AI. The
output shows values of the measures per each minute.&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;out_i = SummarizedActigraphy::calculate_measures(
  df = dat_input_i, 
  dynamic_range = c(-8, 8),  # dynamic range
  fix_zeros = FALSE,         # fixes zeros from idle sleep mode -- not needed in our case 
  calculate_mims = TRUE,     # uses algorithm from MIMSunit package
  calculate_ac = TRUE,       # uses algorithm from activityCounts package 
  flag_data = FALSE,         # runs raw data quality control flags algorithm -- not used in our case  
  verbose = FALSE)

======================================================================================

out_i &amp;lt;- out_i %&amp;gt;% select(HEADER_TIME_STAMP = time, AC, MIMS = MIMS_UNIT, ENMO = ENMO_t, MAD, AI)
out_i

# A tibble: 51 × 6
   HEADER_TIME_STAMP            AC  MIMS     ENMO     MAD    AI
   &amp;lt;dttm&amp;gt;                    &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;    &amp;lt;dbl&amp;gt;   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;
 1 2021-01-01 00:00:00.000  3623.  19.3  0.0882   0.140    7.62
 2 2021-01-01 00:01:00.000  2460.  14.8  0.0544   0.0971   5.84
 3 2021-01-01 00:02:00.000   648.   6.52 0.00607  0.0192   2.44
 4 2021-01-01 00:03:00.000    36.2  1.86 0.000769 0.00862  1.03
 5 2021-01-01 00:04:00.000  8084.  34.8  0.323    0.316   15.1 
 6 2021-01-01 00:05:00.000 13845.  60.1  0.442    0.358   25.3 
 7 2021-01-01 00:06:00.000 10549.  44.1  0.475    0.248   18.7 
 8 2021-01-01 00:07:00.000 11219.  45.8  0.508    0.244   19.6 
 9 2021-01-01 00:08:00.000 11624.  51.6  0.393    0.338   22.1 
10 2021-01-01 00:09:00.000 11049.  45.8  0.404    0.288   20.0 
# … with 41 more rows
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Next, we calculate the measures for all n=32 subjects.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;
(Click to see the code.)
&lt;/summary&gt;
&lt;pre&gt;&lt;code&gt;out_all &amp;lt;- data.frame()
for (dat_fpath_i in dat_fpaths){ #  dat_fpath_i &amp;lt;- dat_fpaths[1]
  message(basename(dat_fpath_i))
  # read data 
  basename_i &amp;lt;- gsub(&amp;quot;.csv&amp;quot;, &amp;quot;&amp;quot;, basename(dat_fpath_i))
  dat_i &amp;lt;- read.csv(dat_fpath_i) 
  # prepare data input
  ts_i &amp;lt;- seq(from = ymd_hms(&amp;quot;2021-01-01 00:00:00&amp;quot;), by = 0.01, length.out = nrow(dat_i))
  dat_input_i &amp;lt;- 
    dat_i %&amp;gt;% 
    mutate(HEADER_TIME_STAMP = ts_i) %&amp;gt;%
    select(HEADER_TIME_STAMP, X = lw_x, Y = lw_y, Z = lw_z)
  # compute the measures 
  out_i = SummarizedActigraphy::calculate_measures(
    df = dat_input_i, 
    dynamic_range = c(-8, 8),  # dynamic range
    fix_zeros = FALSE,         # fixes zeros from idle sleep mode -- not needed in our case 
    calculate_mims = TRUE,     # uses algorithm from MIMSunit package
    calculate_ac = TRUE,       # uses algorithm from activityCounts package 
    flag_data = FALSE,         # runs raw data quality control flags algorithm -- not used in our case  
    verbose = FALSE)
  out_i &amp;lt;- out_i %&amp;gt;% select(HEADER_TIME_STAMP = time, AC, MIMS = MIMS_UNIT, MAD, AI)
  out_i &amp;lt;- mutate(out_i, subj_id = basename_i, .before = everything())
  # append subject-specific measures to all subjects file 
  out_all &amp;lt;- rbind(out_all, out_i)
}
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;h2 id=&#34;minute-level-physical-activity-measures-during-walking-and-driving-activities&#34;&gt;Minute-level physical activity measures during walking and driving activities&lt;/h2&gt;
&lt;h3 id=&#34;merging-physical-activity-measures-with-physical-activity-labels&#34;&gt;Merging physical activity measures with physical activity labels&lt;/h3&gt;
&lt;p&gt;Next, we merge the minute-level physical-activity measures with the
activity labels. We only keep the minutes for which all the measurements
were labelled with no more than one type of activity.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;
(Click to see the code.)
&lt;/summary&gt;
&lt;pre&gt;&lt;code&gt;labels_df_all &amp;lt;- data.frame()
for (dat_fpath_i in dat_fpaths){ #  dat_fpath_i &amp;lt;- dat_fpaths[1]
  message(basename(dat_fpath_i))
  # read data 
  basename_i &amp;lt;- gsub(&amp;quot;.csv&amp;quot;, &amp;quot;&amp;quot;, basename(dat_fpath_i))
  dat_i &amp;lt;- read.csv(dat_fpath_i) 
  # aggregate activity labels
  ts_i &amp;lt;- seq(from = ymd_hms(&amp;quot;2021-01-01 00:00:00&amp;quot;), by = 0.01, length.out = nrow(dat_i))
  labels_df_i &amp;lt;- 
    dat_i %&amp;gt;% 
    mutate(HEADER_TIME_STAMP = ts_i) %&amp;gt;%
    mutate(HEADER_TIME_STAMP = lubridate::floor_date(HEADER_TIME_STAMP, &amp;quot;1 min&amp;quot;)) %&amp;gt;%
    group_by(HEADER_TIME_STAMP) %&amp;gt;%
    filter(n_distinct(activity) == 1) %&amp;gt;%
    filter(row_number() == 1) %&amp;gt;%
    # 1=walking; 2=descending stairs; 3=ascending stairs; 4=driving
    filter(activity %in% c(1,2,3,4)) %&amp;gt;%
    ungroup() %&amp;gt;%
    select(HEADER_TIME_STAMP, activity) 
  labels_df_i &amp;lt;- mutate(labels_df_i, subj_id = basename_i, .before = everything())
  # append subject-specific measures to all subjects file 
  labels_df_all &amp;lt;- rbind(labels_df_all, labels_df_i)
}
# recode activity label
labels_df_all &amp;lt;- labels_df_all %&amp;gt;%
  mutate(activity = recode(activity, &#39;1&#39; = &#39;walking&#39;, &#39;2&#39; = &#39;descending_stairs&#39;, 
                           &#39;3&#39; = &#39;ascending_stairs&#39;, &#39;4&#39; = &#39;driving&#39;))

# merge: 
# (1) physical activity minute-level measures, 
# (2) physical activity minute-level activity labels
out_all_merged &amp;lt;- 
  out_all %&amp;gt;% 
  inner_join(labels_df_all, by = c(&amp;quot;subj_id&amp;quot;, &amp;quot;HEADER_TIME_STAMP&amp;quot;)) 
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;pre&gt;&lt;code&gt;out_all_merged

# A tibble: 686 × 8
   subj_id    HEADER_TIME_STAMP           AC  MIMS   ENMO    MAD    AI activity
   &amp;lt;chr&amp;gt;      &amp;lt;dttm&amp;gt;                   &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt;  &amp;lt;dbl&amp;gt; &amp;lt;dbl&amp;gt; &amp;lt;chr&amp;gt;   
 1 id00b70b13 2021-01-01 00:06:00.000 10549. 44.1  0.475  0.248  18.7  walking 
 2 id00b70b13 2021-01-01 00:07:00.000 11219. 45.8  0.508  0.244  19.6  walking 
 3 id00b70b13 2021-01-01 00:11:00.000 11026. 42.3  0.498  0.215  18.9  walking 
 4 id00b70b13 2021-01-01 00:12:00.000 10561. 42.1  0.486  0.231  19.1  walking 
 5 id00b70b13 2021-01-01 00:13:00.000 13297. 53.9  0.591  0.325  23.1  walking 
 6 id00b70b13 2021-01-01 00:21:00.000  2603. 14.5  0.0501 0.0413  4.80 driving 
 7 id00b70b13 2021-01-01 00:23:00.000   499.  6.65 0.0534 0.0537  3.59 driving 
 8 id00b70b13 2021-01-01 00:24:00.000   398.  6.06 0.0537 0.0496  3.49 driving 
 9 id00b70b13 2021-01-01 00:25:00.000   130.  5.92 0.0542 0.0560  3.67 driving 
10 id00b70b13 2021-01-01 00:26:00.000   414.  7.00 0.0469 0.0561  3.55 driving 
# … with 676 more rows

table(out_all_merged$activity)


driving walking 
    569     117 
&lt;/code&gt;&lt;/pre&gt;
&lt;h3 id=&#34;visualizing-physical-activity-measures-across-activity-types&#34;&gt;Visualizing physical activity measures across activity types&lt;/h3&gt;
&lt;p&gt;We visualize values of minute-level physical activity measures. Note
these only used data collected with a sensor located at left wrist.&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;
(Click to see the code.)
&lt;/summary&gt;
&lt;pre&gt;&lt;code&gt;name_levels &amp;lt;- c(&amp;quot;AC&amp;quot;, &amp;quot;MIMS&amp;quot;, &amp;quot;ENMO&amp;quot;,  &amp;quot;MAD&amp;quot;, &amp;quot;AI&amp;quot;)
activity_levels &amp;lt;- c(&amp;quot;walking&amp;quot;, &amp;quot;driving&amp;quot;)
# define data subsets with one activity type only 
df_walk &amp;lt;- out_all_merged %&amp;gt;% filter(activity == &amp;quot;walking&amp;quot;) 
df_driv &amp;lt;- out_all_merged %&amp;gt;% filter(activity == &amp;quot;driving&amp;quot;)
# estimate sample population mean for each measure via LMM
measure_mean_vals &amp;lt;- c(
  # walking 
  fixef(lmer(AC ~ (1 | subj_id), data = df_walk))[1],
  fixef(lmer(MIMS ~ (1 | subj_id), data = df_walk))[1],
  fixef(lmer(ENMO ~ (1 | subj_id), data = df_walk))[1],
  fixef(lmer(MAD ~ (1 | subj_id), data = df_walk))[1],
  fixef(lmer(AI ~ (1 | subj_id), data = df_walk))[1],
   # driving 
  fixef(lmer(AC ~ (1 | subj_id), data = df_driv))[1],
  fixef(lmer(MIMS ~ (1 | subj_id), data = df_driv))[1],
  fixef(lmer(ENMO ~ (1 | subj_id), data = df_driv))[1],
  fixef(lmer(MAD ~ (1 | subj_id), data = df_driv))[1],
  fixef(lmer(AI ~ (1 | subj_id), data = df_driv))[1]
)
measure_mean_df &amp;lt;- data.frame(
  value = measure_mean_vals,
  name = rep(name_levels, times = 2),
  activity = rep(activity_levels, each = 5)
)
measure_mean_df_w &amp;lt;- 
  measure_mean_df %&amp;gt;%
  mutate(value = round(value, 2)) %&amp;gt;% 
  pivot_wider(names_from = activity)
# define subject-specific means 
out_all_l &amp;lt;- 
  out_all_merged %&amp;gt;%
  pivot_longer(cols = all_of(name_levels)) 
out_all_l_agg &amp;lt;- 
  out_all_l %&amp;gt;%
  group_by(subj_id, name, activity) %&amp;gt;%
  summarise(value = mean(value))
subj_id_levels &amp;lt;- 
  out_all_l_agg %&amp;gt;%
  filter(name == &amp;quot;AC&amp;quot;, activity == &amp;quot;walking&amp;quot;) %&amp;gt;%
  arrange(desc(value)) %&amp;gt;%
  pull(subj_id)
# mutate data frames used on the plot to have factors at certain levels order
measure_mean_df &amp;lt;- 
  measure_mean_df %&amp;gt;%
  mutate(activity = factor(activity, levels = activity_levels)) %&amp;gt;%
  mutate(name = factor(name, levels = name_levels))
out_all_l &amp;lt;- 
  out_all_l %&amp;gt;%
  mutate(subj_id = factor(subj_id, levels = subj_id_levels),
         activity = factor(activity, levels = activity_levels)) %&amp;gt;%
  mutate(name = factor(name, levels = name_levels))
out_all_l_agg &amp;lt;-
  out_all_l_agg %&amp;gt;%
   mutate(subj_id = factor(subj_id, levels = subj_id_levels),
         activity = factor(activity, levels = activity_levels)) %&amp;gt;%
  mutate(name = factor(name, levels = name_levels))
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;p&gt;First, we estimated sample population mean for each minute-level measure
via linear mixed model (LMM).&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;kable(measure_mean_df_w)
&lt;/code&gt;&lt;/pre&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr class=&#34;header&#34;&gt;
&lt;th style=&#34;text-align: left;&#34;&gt;name&lt;/th&gt;
&lt;th style=&#34;text-align: right;&#34;&gt;walking&lt;/th&gt;
&lt;th style=&#34;text-align: right;&#34;&gt;driving&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;AC&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;7616.48&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;801.15&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;MIMS&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;34.86&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;7.10&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;ENMO&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;0.24&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;0.04&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;even&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;MAD&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;0.26&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;0.05&lt;/td&gt;
&lt;/tr&gt;
&lt;tr class=&#34;odd&#34;&gt;
&lt;td style=&#34;text-align: left;&#34;&gt;AI&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;15.01&lt;/td&gt;
&lt;td style=&#34;text-align: right;&#34;&gt;3.31&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Next, we plot minute-level physical activity measures:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;individual measure values (red) for each subject,&lt;/li&gt;
&lt;li&gt;mean measure value (black) for each subject,&lt;/li&gt;
&lt;li&gt;sample population mean for each measure (dashed horizontal line)
estimated via LMM.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;On each plot panel, subject IDs (x-axis) are sorted by average AC during
walking (left-top plot panel).&lt;/p&gt;
&lt;details&gt;
&lt;summary&gt;
(Click to see the code.)
&lt;/summary&gt;
&lt;pre&gt;&lt;code&gt;plt_measures &amp;lt;- 
  ggplot(out_all_l, 
         aes(x = subj_id, y = value)) + 
  geom_hline(data = measure_mean_df,
             aes(yintercept = value), 
             linetype = 2, size = 0.2) + 
  geom_point(size = 0.5, alpha = 0.3, color = &amp;quot;red&amp;quot;) + 
  geom_point(data = out_all_l_agg, 
             aes(x = subj_id, y = value),
             size = 0.5, color = &amp;quot;black&amp;quot;) + 
  facet_grid(name ~ activity, scales = &amp;quot;free&amp;quot;) + 
  labs(x = &amp;quot;Subject ID&amp;quot;, y = &amp;quot;&amp;quot;) + 
  theme_gray(base_size = 14) +    
  theme(panel.grid.major = element_line(size = 0.3),  
        panel.grid.minor = element_blank(),
        axis.text.x = element_text(size = 6, angle = 90, vjust = 0.5, hjust = 1)) 
&lt;/code&gt;&lt;/pre&gt;
&lt;/details&gt;
&lt;pre&gt;&lt;code&gt;plt_measures
&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;plt_measures.png&#34; alt=&#34;&#34;&gt;&lt;/p&gt;
&lt;p&gt;The above plot highlights what are minute-level summary measures of
physical activity from raw accelerometry data: AC, MIMS, ENMO, MAD, and AI,
across two different activities (walking, driving) in the study sample
of n=32 healthy individuals.&lt;/p&gt;
&lt;h2 id=&#34;acknowledgements&#34;&gt;Acknowledgements&lt;/h2&gt;
&lt;p&gt;I’d like to thank &lt;a href=&#34;https://github.com/muschellij2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;John Muschelli&lt;/a&gt; for
feedback on this tutorial.&lt;/p&gt;
&lt;h2 id=&#34;span-stylecolorredcitation-infospan&#34;&gt;&lt;span style=&#34;color:red&#34;&gt;Citation info&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;Please cite &lt;code&gt;SummarizedActigraphy&lt;/code&gt; R package, if applicable.&lt;/p&gt;
&lt;p&gt;Please cite raw accelerometry dataset DOI, if applicable (go to see
&lt;a href=&#34;https://physionet.org/content/accelerometry-walk-climb-drive/1.0.0/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this dataset citation
options&lt;/a&gt;).&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>When to use t distribution versus normal distribution quantiles in constructing confidence interval for the mean</title>
      <link>/post/2019-10-28-ttest-versus-ztest/</link>
      <pubDate>Mon, 28 Oct 2019 17:19:28 -0400</pubDate>
      <guid>/post/2019-10-28-ttest-versus-ztest/</guid>
      <description>&lt;p&gt;To construct confidence interval for the mean, we often use quantiles of standardized sample mean distribution. Here, I include a list of cases where I&amp;rsquo;d use quantiles of t-distribution versus quantiles of normal distribution for that purpose.&lt;/p&gt;
&lt;p&gt;Note: the below text could be directly translated to answer when to use t-test versus z-test in testing hypothesis about the mean parameter.&lt;/p&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;
  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#span-stylecolordarkblueexample-1-constructing-confidence-interval-for-mu-with-z-quantilesspan&#34;&gt;&lt;span style=&#34;color:darkblue&#34;&gt;Example 1: constructing confidence interval for $\mu$ with $z$-quantiles&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#span-stylecolordarkblueexample-2-constructing-confidence-interval-for-mu-with-t-quantilesspan&#34;&gt;&lt;span style=&#34;color:darkblue&#34;&gt;Example 2: constructing confidence interval for $\mu$ with $t$-quantiles&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;

  &lt;ul&gt;
    &lt;li&gt;&lt;a href=&#34;#span-stylecolordarkbluecase-1-observations-from-normal-distribution-sigma-known-any-nspan&#34;&gt;&lt;span style=&#34;color:darkblue&#34;&gt;Case 1: observations from normal distribution, $\sigma$ known, any $n$&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#span-stylecolordarkbluecase-2-observations-from-normal-distribution-sigma-unknown-small-nspan&#34;&gt;&lt;span style=&#34;color:darkblue&#34;&gt;Case 2: observations from normal distribution, $\sigma$ unknown, small $n$&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#span-stylecolordarkbluecase-3-observations-from-normal-distribution-sigma-unknown-large-nspan&#34;&gt;&lt;span style=&#34;color:darkblue&#34;&gt;Case 3: observations from normal distribution, $\sigma$ unknown, large $n$&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#span-stylecolordarkbluecase-4-observations-from-any-distribution-sigma-known-small-nspan&#34;&gt;&lt;span style=&#34;color:darkblue&#34;&gt;Case 4: observations from any distribution, $\sigma$ known, small $n$&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#span-stylecolordarkbluecase-5-observations-from-any-distribution-sigma-known-large-nspan&#34;&gt;&lt;span style=&#34;color:darkblue&#34;&gt;Case 5: observations from any distribution, $\sigma$ known, large $n$&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#span-stylecolordarkbluecase-6-observations-from-any-distribution-sigma-unknown-small-nspan&#34;&gt;&lt;span style=&#34;color:darkblue&#34;&gt;Case 6: observations from any distribution, $\sigma$ unknown, small $n$&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
    &lt;li&gt;&lt;a href=&#34;#span-stylecolordarkbluecase-7-observations-from-any-distribution-sigma-unknown-large-nspan&#34;&gt;&lt;span style=&#34;color:darkblue&#34;&gt;Case 7: observations from any distribution, $\sigma$ unknown, large $n$&lt;/span&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;/ul&gt;
&lt;/nav&gt;
&lt;h1 id=&#34;standardized-sample-mean&#34;&gt;Standardized sample mean&lt;/h1&gt;
&lt;p&gt;Consider $X_1, \ldots, X_n$ &amp;ndash; a sequence of i.i.d. random variables with mean $E(X_i) = \mu$ and  variance $\text{var}(X_i) = \sigma^2$. To construct confidence intervals for $\mu$ parameter, we often use a standardized sample mean,&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\frac{\overline{X}_n - \mu}{\sigma/\sqrt{n}},
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;or its version where $S_n$ &amp;ndash; a consistent estimator of true standard deviation $\sigma$ &amp;ndash; is used, $\frac{\overline{X}_n - \mu}{S_n/\sqrt{n}}$; the latter is common in practice as we typically do not know $\sigma$ and must estimate it from the data. Knowing distribution of a standardized sample mean allows us to construct confidence interval for a mean $\mu$ parameter.&lt;/p&gt;
&lt;h2 id=&#34;span-stylecolordarkblueexample-1-constructing-confidence-interval-for-mu-with-z-quantilesspan&#34;&gt;&lt;span style=&#34;color:darkblue&#34;&gt;Example 1: constructing confidence interval for $\mu$ with $z$-quantiles&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;Assume $X_1, \ldots, X_n$ are i.i.d. $\sim N(\mu,\sigma^2)$ and $\sigma$ is known. Then we have an exact distributional result for a standardized sample mean,&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\frac{\overline{X}_n - \mu}{\sigma/\sqrt{n}} \sim N(0,1).
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Let us denote $z_{ 1-\frac{\alpha}{2}}$ to be $(1-\frac{\alpha}{2})$-th quantile of standard normal distribution $N(0,1)$. Since  $N(0,1)$ is symmetric around $0$, we have $z_{\frac{\alpha}{2}} = -z_{1-\frac{\alpha}{2}}$ and we can write&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
1-\alpha = P\left(-z_{1-\frac{\alpha}{2}} \leq \frac{\overline{X}_n-\mu}{\sigma/\sqrt{n}} \leq z_{1-\frac{\alpha}{2}} \right)
=  P\left(\bar{X}_n-z_{1-\frac{\alpha}{2}}  \frac{\sigma}{\sqrt{n}} \leq \mu \leq \bar{X}_n+z_{1-\frac{\alpha}{2}}  \frac{\sigma}{\sqrt{n}} \right),
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;which yields that $\left[ \bar{X}_n-z_{1-\frac{\alpha}{2}} \frac{\sigma}{\sqrt{n}}, ; \bar{X}_n+z_{1-\frac{\alpha}{2}}  \frac{\sigma}{\sqrt{n}}\right]$ is a $(1-\alpha )$-confidence interval for a mean parameter $\mu$.&lt;/p&gt;
&lt;h2 id=&#34;span-stylecolordarkblueexample-2-constructing-confidence-interval-for-mu-with-t-quantilesspan&#34;&gt;&lt;span style=&#34;color:darkblue&#34;&gt;Example 2: constructing confidence interval for $\mu$ with $t$-quantiles&lt;/span&gt;&lt;/h2&gt;
&lt;p&gt;Assume $X_1, \ldots, X_n$ are i.i.d. $\sim N(\mu,\sigma^2)$ and $\sigma$ is &lt;strong&gt;unknown&lt;/strong&gt;. We use $S_n$ &amp;ndash; a consistent sample estimator of true standard deviation &amp;ndash; to approximate $\sigma$, and have an exact distributional result for a standardized sample mean,&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
\frac{\overline{X}_n - \mu}{S_n/\sqrt{n}} \sim t_{n-1}.
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;Let us denote $t_{n-1,1-\frac{\alpha}{2}}$ to be a $(n-1,1-\frac{\alpha}{2})$-th quantile of $t$-distributuon with $n-1$ degrees of freedom. Since $t$ is symmetric around $0$, we have&lt;/p&gt;
&lt;p&gt;$$
\begin{aligned}
1-\alpha =P\left(-t_{n-1,1-\frac{\alpha}{2}} \leq \frac{\bar{X}_{n}-\mu}{S_{n} / \sqrt{n}} \leq t_{n-1,1-\frac{\alpha}{2}}\right)
= P\left(\bar{X}_n-t_{n-1, 1-\frac{\alpha}{2}}\frac{S_n}{\sqrt{n}} \leq \mu \leq \bar{X}_n+t_{n-1, 1-\frac{\alpha}{2}} \frac{S_n}{\sqrt{n}} \right),
\end{aligned}
$$&lt;/p&gt;
&lt;p&gt;which yields that $\left[ \bar{X}_n-t_{n-1, 1-\frac{\alpha}{2}} \frac{S_n}{\sqrt{n}}, ; \bar{X}_n+t_{n-1, 1-\frac{\alpha}{2}} \frac{S_n}{\sqrt{n}}\right]$ is a $(1-\alpha )$-confidence interval for a mean parameter $\mu$.&lt;/p&gt;
&lt;h1 id=&#34;cases&#34;&gt;Cases&lt;/h1&gt;
&lt;p&gt;In many cases, whether to use quantiles of $t$-student distribution versus standard normal distribution is based on:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;distribution of $X_1, \ldots, X_n$ variables,&lt;/li&gt;
&lt;li&gt;whether  $\sigma$ is known or not (and we need to estimate it i.e. with $S_n$),&lt;/li&gt;
&lt;li&gt;what is sample size $n$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note: the below cases could be directly translated to answer when to use $t$-test versus $z$-test in testing hypothesis about the mean $\mu$ parameter, i.e. test for $H_0: \mu = \mu_0$ versus $H_1: \mu &amp;lt; \mu_0$, or $H_1: \mu \neq \mu_0$, or $H_1: \mu &amp;gt; \mu_0$.&lt;/p&gt;
&lt;h2 id=&#34;span-stylecolordarkbluecase-1-observations-from-normal-distribution-sigma-known-any-nspan&#34;&gt;&lt;span style=&#34;color:darkblue&#34;&gt;Case 1: observations from normal distribution, $\sigma$ known, any $n$&lt;/span&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Observations $X_1, \ldots, X_n$ are from normal $N(\mu, \sigma^2)$ distribution.&lt;/li&gt;
&lt;li&gt;$\sigma$ known.&lt;/li&gt;
&lt;li&gt;Any sample size $n$.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$\Rightarrow$ We have exact result that  $\frac{\bar{X}_{n}-\mu}{\sigma / \sqrt{n}} \sim N(0,1)$ and hence we use quantiles of normal distribution in constructing the CI.&lt;/p&gt;
&lt;h2 id=&#34;span-stylecolordarkbluecase-2-observations-from-normal-distribution-sigma-unknown-small-nspan&#34;&gt;&lt;span style=&#34;color:darkblue&#34;&gt;Case 2: observations from normal distribution, $\sigma$ unknown, small $n$&lt;/span&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Observations $X_1, \ldots, X_n$ are from normal $N(\mu, \sigma^2)$ distribution.&lt;/li&gt;
&lt;li&gt;$\sigma$ unknown.&lt;/li&gt;
&lt;li&gt;Small sample size ($n \leq 50$).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$\Rightarrow$ We use $S_{n}$ to approximate $\sigma$. We have exact result that  $\frac{\bar{X}_{n}-\mu}{S_{n} / \sqrt{n}} \sim t_{n-1}$ and hence we use quantiles of $t$ distribution with $n-1$ degrees of freedom in constructing the CI.&lt;/p&gt;
&lt;h2 id=&#34;span-stylecolordarkbluecase-3-observations-from-normal-distribution-sigma-unknown-large-nspan&#34;&gt;&lt;span style=&#34;color:darkblue&#34;&gt;Case 3: observations from normal distribution, $\sigma$ unknown, large $n$&lt;/span&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Observations $X_1, \ldots, X_n$ are from normal $N(\mu, \sigma^2)$ distribution.&lt;/li&gt;
&lt;li&gt;$\sigma$ unknown.&lt;/li&gt;
&lt;li&gt;Moderate to large sample size ($n &amp;gt; 50$).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$\Rightarrow$ We use $S_{n}$ to approximate $\sigma$. Because of $n$ large enough, Slutsky&amp;rsquo;s theorem asymptotic ``kicks in&#39;&#39; and allows to replace $\sigma$ with $S_n$ &amp;ndash; a consistent estimator of true population standard deviation, and to write that $\frac{\bar{X}_{n}-\mu}{S_n / \sqrt{n}} \approx \sim N(0,1)$. Because of $n$ large enough, we assume $N(0,1)$ is approximated ($\approx$) well enough to use quantiles of normal distribution in constructing the CI.&lt;/p&gt;
&lt;p&gt;$\Rightarrow$ Another way to think about this case is that, as in Case 2, we have an exact result that $\frac{\bar{X}_{n}-\mu}{S_{n} / \sqrt{n}} \sim t_{n-1}$, and with large $n$, quantiles of $t$-distribution with $n-1$ degrees of freedom are almost equvalent to quantiles of normal distribution.&lt;/p&gt;
&lt;h2 id=&#34;span-stylecolordarkbluecase-4-observations-from-any-distribution-sigma-known-small-nspan&#34;&gt;&lt;span style=&#34;color:darkblue&#34;&gt;Case 4: observations from any distribution, $\sigma$ known, small $n$&lt;/span&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Observations $X_1, \ldots, X_n$ are from (other than normal) distribution  of mean $E(X_i) = \mu$ and variance $\text{var}(X_i) = \sigma^2$ (&lt;em&gt;for normally distributed $X_i$&amp;rsquo;s, see cases 1-3&lt;/em&gt;).&lt;/li&gt;
&lt;li&gt;$\sigma$ known.&lt;/li&gt;
&lt;li&gt;Small sample size ($n \leq 50$).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$\Rightarrow$ Use CLT to get that standardized sample mean is approximately normal, $\frac{\bar{X}_{n}-\mu}{\sigma / \sqrt{n}} \approx \sim N(0,1)$. Since there is CLT approximation and we have a small sample size, in practice, we typically use quantiles of $t$ distribution with $n-1$ degrees of freedom to get more conservative (wider) CI.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Note&lt;/strong&gt;: when $X_1, \ldots, X_n$ distribution of is very skewed (i.e. Poisson) it may be not plausible that CLT already ``kicks in&#39;&#39; and other techniques may be needed.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;span-stylecolordarkbluecase-5-observations-from-any-distribution-sigma-known-large-nspan&#34;&gt;&lt;span style=&#34;color:darkblue&#34;&gt;Case 5: observations from any distribution, $\sigma$ known, large $n$&lt;/span&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Observations $X_1, \ldots, X_n$ are from (other than normal) distribution  of mean $E(X_i) = \mu$ and variance $\text{var}(X_i) = \sigma^2$ (&lt;em&gt;for normally distributed $X_i$&amp;rsquo;s, see cases 1-3&lt;/em&gt;).&lt;/li&gt;
&lt;li&gt;$\sigma$ known.&lt;/li&gt;
&lt;li&gt;Moderate to large sample size ($n &amp;gt; 50$).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$\Rightarrow$   Use CLT to get that standardized sample mean is approximately normal. We have $\frac{\bar{X}_{n}-\mu}{\sigma/ \sqrt{n}} \approx \sim N(0,1)$. Since we have a moderate to small sample size, we assume that CLT ``kicks in&#39;&#39; and the approximation ($\approx$) is good enough to use quantiles of normal distribution.&lt;/p&gt;
&lt;h2 id=&#34;span-stylecolordarkbluecase-6-observations-from-any-distribution-sigma-unknown-small-nspan&#34;&gt;&lt;span style=&#34;color:darkblue&#34;&gt;Case 6: observations from any distribution, $\sigma$ unknown, small $n$&lt;/span&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Observations $X_1, \ldots, X_n$ are from (other than normal) distribution  of mean $E(X_i) = \mu$ and variance $\text{var}(X_i) = \sigma^2$ (&lt;em&gt;for normally distributed $X_i$&amp;rsquo;s, see cases 1-3&lt;/em&gt;).&lt;/li&gt;
&lt;li&gt;$\sigma$ unknown.&lt;/li&gt;
&lt;li&gt;Small sample size ($n \leq 50$).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$\Rightarrow$   Use CLT to get that standardized sample mean is approximately normal  and we also use Slutsky&amp;rsquo;s theorem to replace $\sigma$ with $S_n$ &amp;ndash; a consistent estimator of true population standard deviation. We have $\frac{\bar{X}_{n}-\mu}{S_n/ \sqrt{n}} \approx \sim N(0,1)$. Since there is CLT and Slutsky&amp;rsquo;s theorem approximation and we have a small sample size, in practice, we typically use quantiles of $t$ distribution with $n-1$ degrees of freedom to get more conservative (wider) CI.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Note&lt;/strong&gt;: two approximations are happening here!&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Note&lt;/strong&gt;: when $X_1, \ldots, X_n$ distribution of is very skewed (i.e. Poisson) it may be not plausible that CLT already ``kicks in&#39;&#39; and other techniques may be needed.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;span-stylecolordarkbluecase-7-observations-from-any-distribution-sigma-unknown-large-nspan&#34;&gt;&lt;span style=&#34;color:darkblue&#34;&gt;Case 7: observations from any distribution, $\sigma$ unknown, large $n$&lt;/span&gt;&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;Observations $X_1, \ldots, X_n$ are from (other than normal) distribution  of mean $E(X_i) = \mu$ and variance $\text{var}(X_i) = \sigma^2$ (&lt;em&gt;for normally distributed $X_i$&amp;rsquo;s, see cases 1-3&lt;/em&gt;).&lt;/li&gt;
&lt;li&gt;$\sigma$ unknown.&lt;/li&gt;
&lt;li&gt;Moderate to large sample size ($n &amp;gt; 50$).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;$\Rightarrow$   Use CLT to get that standardized sample mean is approximately normal  and we also use Slutsky&amp;rsquo;s theorem to replace $\sigma$ with $S_n$ &amp;ndash; a consistent estimator of true population standard deviation. We have $\frac{\bar{X}_{n}-\mu}{S_n/ \sqrt{n}} \approx \sim N(0,1)$. Since we have a moderate to small sample size, we assume that both CLT and Slutsky asymptotics ``kicks in&#39;&#39; and the approximation ($\approx$) is good enough to use quantiles of normal distribution.&lt;/p&gt;
&lt;h1 id=&#34;disclaimer&#34;&gt;Disclaimer&lt;/h1&gt;
&lt;p&gt;&lt;span style=&#34;color:red&#34;&gt;The views, thoughts, and opinions expressed in the text belong solely to the author, and not necessarily to the author’s employer, organization, committee or other group or individual.&lt;/span&gt;&lt;/p&gt;
&lt;h1 id=&#34;references&#34;&gt;References&lt;/h1&gt;
&lt;p&gt;[1]: Methods in Biostatistics with R. Ciprian Crainiceanu, Brian Caffo, John Muschelli (2019). Available online at &lt;a href=&#34;https://leanpub.com/biostatmethods&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://leanpub.com/biostatmethods&lt;/a&gt;.&lt;/p&gt;</description>
    </item>
    
    <item>
      <title>Some of the topics seen at RStudio conference 2019</title>
      <link>/post/2019-01-26-favs-of-rstudio2019-conference/</link>
      <pubDate>Sat, 26 Jan 2019 19:17:23 -0400</pubDate>
      <guid>/post/2019-01-26-favs-of-rstudio2019-conference/</guid>
      <description>&lt;p&gt;I spent the last days of 2018-19 winter break in Austin, TX where I traveled for RStudio 2019 conference. I attended e-poster session and two days of the conference: Jan 17-18. Below I list some of the topics from the talks that I particularly liked and I think I am likely to benefit from in the future.&lt;/p&gt;
&lt;p&gt;Also:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The official repo with abstracts for every session, workshop (together with &lt;strong&gt;workshop files free to download&lt;/strong&gt; for most if not all of them), and e-poster can be accessed &lt;a href=&#34;https://github.com/rstudio/rstudio-conf/tree/master/2019&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;&lt;/nav&gt;
&lt;h1 id=&#34;data-visualization&#34;&gt;Data visualization&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Tyler Morgan-Wall (&lt;a href=&#34;http://www.tylermw.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;website&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/tylermorganwall&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;twitter&lt;/a&gt;) showing stunning gifs and pics while presenting &amp;ldquo;&lt;em&gt;3D mapping, plotting, and printing with rayshader&lt;/em&gt;&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;Features showed include shadowing, rotating, water transparency, visualization of a simulation of different water levels. The talk had possibly the most exciting &amp;ldquo;future work&amp;rdquo; announced - because who doesn&amp;rsquo;t get excited about the idea of &amp;ldquo;rotating 3D ggplots&amp;rdquo;? 💘 The &lt;code&gt;rayshader&lt;/code&gt; package is available on GitHub (&lt;a href=&#34;https://github.com/tylermorganwall/rayshader&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;link&lt;/a&gt;).&lt;/p&gt;
  &lt;table style=&#34;table-layout:fixed&#34;&gt;
  &lt;col style=&#34;width:100%&#34; span=&#34;2&#34; /&gt;
  &lt;tr&gt;
  &lt;td&gt; &lt;img src=&#34;../../img/2019-01-26-favs-of-rstudio2019-conference/tyler.gif&#34; alt=&#34;Drawing&#34; style=&#34;width: 100%;&#34;/&gt; &lt;/td&gt;
  &lt;td&gt; &lt;img src=&#34;../../img/2019-01-26-favs-of-rstudio2019-conference/tyler2.png&#34;  alt=&#34;Drawing&#34; style=&#34;weight: 100%;&#34;/&gt; &lt;/td&gt;
  &lt;/tr&gt;
  &lt;/table&gt;
&lt;p&gt;&lt;sub&gt;&lt;sup&gt;(The gif on the left and the image on the right are both sourced from &amp;lsquo;rayshader&amp;rsquo; package GitHub website (&lt;a href=&#34;https://github.com/tylermorganwall/rayshader&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;link&lt;/a&gt;), as accessed on Jan 26, 2019.)&lt;/sup&gt;&lt;/sub&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Thomas Lin Pedersen (&lt;a href=&#34;https://www.data-imaginist.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;website&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/thomasp85&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;twitter&lt;/a&gt;) giving a &amp;ldquo;&lt;em&gt;gganimate live cookbook&lt;/em&gt;&amp;rdquo; speech (and joining Tyler&amp;rsquo;s talk on the podium of most fun presentations, I guess 😂)&lt;/p&gt;
&lt;p&gt;The &lt;code&gt;gganimate&lt;/code&gt; package was introduced per &amp;ldquo;&lt;em&gt;extension to ggplot2&lt;/em&gt;&amp;rdquo; providing &amp;ldquo;&lt;em&gt;implementation of the grammar of animated graphics&lt;/em&gt;&amp;rdquo;. Presentation slides are available online  &lt;a href=&#34;https://www.data-imaginist.com/slides/rstudioconf2019/assets/player/keynotedhtmlplayer#1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;; IMHO worth checking out for inspiring ways of presenting data over time! Besides, &lt;a href=&#34;https://gganimate.com&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://gganimate.com&lt;/a&gt; comes with a bunch of examples, like the one below.&lt;/p&gt;
  &lt;table style=&#34;table-layout:fixed&#34;&gt;
  &lt;col style=&#34;width:100%&#34;  /&gt;
  &lt;tr&gt; 
   &lt;img src=&#34;../../img/2019-01-26-favs-of-rstudio2019-conference/thomas.gif&#34;  alt=&#34;Drawing&#34; style=&#34;weight: 100%;&#34;/&gt; 
  &lt;/tr&gt;
  &lt;/table&gt;
&lt;p&gt;&lt;sub&gt;&lt;sup&gt;(The gif sourced from &amp;lsquo;gganimate&amp;rsquo; package website (&lt;a href=&#34;https://gganimate.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;link&lt;/a&gt;), as accessed on Jan 26, 2019.)&lt;/sup&gt;&lt;/sub&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;documents-building&#34;&gt;Documents building&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Yihui Xie (&lt;a href=&#34;https://yihui.name/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;website&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/xieyihui&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;twitter&lt;/a&gt;) leaving the audience very enthusiastic (or maybe more like: blown away) with all the recent development in document building.&lt;/p&gt;
&lt;p&gt;The presentation starts with &amp;ldquo;In HTML and the Web I trust&amp;rdquo; (😍 I share like 99% of my work summaries with advisors and colleagues in a form HTML). With &lt;code&gt;pagedown&lt;/code&gt; we can now go ahead and get &lt;em&gt;&lt;strong&gt;paged&lt;/strong&gt;&lt;/em&gt; HTML documents, e.g. business card, resume, poster. Fairly 👶 development (&amp;quot;&lt;em&gt;status: experimental&lt;/em&gt;&amp;quot;).
Presentation slides are available &lt;a href=&#34;https://slides.yihui.name/2019-rstudio-conf-pagedown.html#1&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../img/2019-01-26-favs-of-rstudio2019-conference/yihui.jpg&#34; alt=&#34;&#34;&gt;
&lt;sub&gt;&lt;sup&gt;(Slides 10, 11, 15, 17 from RStudio 2019 conference talk &amp;ldquo;pagedown: Creating Beautiful PDFs with R Markdown + CSS + Your Web Browser&amp;rdquo; by Yihui Xie and Romain Lesur on Jan 18, 2019 in Austin, TX.)&lt;/sup&gt;&lt;/sub&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Rich Iannone (&lt;a href=&#34;http://rich-iannone.github.io/DiagrammeR/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;website&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/riannone&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;twitter&lt;/a&gt;) introducing the &lt;code&gt;gt&lt;/code&gt; package.&lt;/p&gt;
&lt;p&gt;With &lt;code&gt;gt&lt;/code&gt; package (&lt;a href=&#34;https://github.com/rstudio/gt&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;link&lt;/a&gt;), one can turn a data table into &amp;ldquo;&lt;em&gt;information-rich, publication-quality&lt;/em&gt;&amp;rdquo; 🎯 table outputs. The table outputs can be in HTML, LaTeX, and RTF. The &amp;ldquo;modular&amp;rdquo; way of building these reminds me of ggplot2 plots construction. Presentation slides are available &lt;a href=&#34;https://github.com/rich-iannone/presentations/tree/master/2019_01-19-rstudio_conf_gt&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../img/2019-01-26-favs-of-rstudio2019-conference/rich2.png&#34; alt=&#34;&#34;&gt;
&lt;sub&gt;&lt;sup&gt;(Slide 5. from RStudio 2019 conference talk &amp;ldquo;Introducing the &amp;lsquo;gt&amp;rsquo; package&amp;rdquo; by Rich Iannone on Jan 18, 2019 in Austin, TX.)&lt;/sup&gt;&lt;/sub&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;community-and-personal-development&#34;&gt;Community and personal development&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;David Robinson (&lt;a href=&#34;http://varianceexplained.org/about/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;website&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/drob&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;twitter&lt;/a&gt;) delivering a keynote talk &amp;ldquo;The unreasonable effectiveness of public work&amp;rdquo; (&lt;a href=&#34;https://bit.ly/drob-rstudio-2019&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;slides&lt;/a&gt;).&lt;/p&gt;
&lt;p&gt;IMHO a phenomenal speech: a kind and resonating talk after which one not only wants to stand up and change the world right now 🔥 but also maintains the same feeling a week after 💪. Review the slides to: (a) learn Author&amp;rsquo;s points on why it so worth it to spend time on public work, (b) for a pack of actual how-to examples and guidelines on building a public portfolio, (c) for a bunch of interesting points made about a value of work (Author&amp;rsquo;s work, but fairly generalizable IMHO); my favorite is copy-pasted below! I particularly appreciated that all stages of advancement in building online portfolio were addressed, 👶-steps including!&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../img/2019-01-26-favs-of-rstudio2019-conference/david1.png&#34; alt=&#34;&#34;&gt;
&lt;sub&gt;&lt;sup&gt;(Slide 63. from RStudio 2019 conference keynote talk &amp;ldquo;The unreasonable effectiveness of public work&amp;rdquo; by David Robinson on Jan 18, 2019 in Austin, TX.)&lt;/sup&gt;&lt;/sub&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Jesse Mostipak (&lt;a href=&#34;https://www.jessemaegan.com/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;website&lt;/a&gt;, &lt;a href=&#34;https://twitter.com/kierisi&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;twitter&lt;/a&gt;) talking about the experience of building R4DS online learning community.&lt;/p&gt;
&lt;p&gt;Some empowering messages came in the lines of this talk - just look at the slides pics below to get the flavor 🙌. My take-home ones include a description of a data scientist: &amp;ldquo;&lt;em&gt;constantly learning, constantly making mistakes, constantly learning from them&lt;/em&gt;&amp;rdquo;.&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;../../img/2019-01-26-favs-of-rstudio2019-conference/jesse_b.jpg&#34; alt=&#34;&#34;&gt;
&lt;sub&gt;&lt;sup&gt;(Pictures I took during RStudio 2019 conference talk &amp;ldquo;R4DS online learning community&amp;rdquo; by Jesse Mostipak on Jan 17, 2019 in Austin, TX.)&lt;/sup&gt;&lt;/sub&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
    <item>
      <title>[in Polish] Aplikacja na studia doktoranckie (Biostatystyka) w USA: notatki nt. procesu rekrutacji</title>
      <link>/post/2018-09-02-applying-for-phd-in-usa/</link>
      <pubDate>Sun, 02 Sep 2018 19:17:23 -0400</pubDate>
      <guid>/post/2018-09-02-applying-for-phd-in-usa/</guid>
      <description>&lt;p&gt;W 2015 roku zdobylam tytul Magister na kierunku Matematyka na Politechnice Wroclawskiej. W sierpniu 2016 rozpoczelam aplikacje na programy doktoranckie na kierunku Biostatystyka w Stanach Zjednoczonych rozpoczynajace sie jesienia 2017. Dostalam i zaakeptowalam oferte z &lt;a href=&#34;https://www.jhsph.edu/departments/biostatistics/index.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Departamentu Biostatystyki&lt;/a&gt; na &lt;a href=&#34;https://www.jhsph.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Johns Hopkins Bloomberg School of Public Health&lt;/a&gt; w Baltimore (stan Maryland), oferujacego jeden z 3 najlepszych (ex aequo) programow na kierunku Biostatystyka w USA wg &lt;a href=&#34;https://www.usnews.com/best-graduate-schools/top-science-schools/statistics-rankings&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;rankingu usnews.com&lt;/a&gt; z 2018 roku. W notatce opisuje etapy rekrutacji i podaje swoje obserwacje i wskazowki.&lt;/p&gt;
&lt;h2&gt;Table of Contents&lt;/h2&gt;
&lt;nav id=&#34;TableOfContents&#34;&gt;&lt;/nav&gt;
&lt;h1 id=&#34;uwagi-wstepne&#34;&gt;Uwagi wstepne&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Ponizszy opis jest napisany w kontekscie aplikacji na kierunek Biostatystyka.&lt;/strong&gt; &lt;em&gt;Spodziewam sie&lt;/em&gt;, ze aplikacje na inne (w szczegolnosci: zblizone tematycznie) programy moga przebiegac &lt;em&gt;podobnie&lt;/em&gt;, jednoczesnie nigdy nie zrobilam porownania. Przykladowa roznica o ktorej wiem, ze istnieje: na programy PhD na kierunku Biostatystyka &lt;em&gt;aplikuje sie ogolnie&lt;/em&gt; na program i po 0-2 latach &lt;em&gt;okresla lub zaweza&lt;/em&gt; obszar badawczy i jednoczesnie decyduje na konkretnych opiekunow naukowych (w tym: promotora/promotorow); typowe dla niektorych innych kierunkow STEM jest z kolei &lt;em&gt;aplikowanie do konkretnego laboratorium (czesto: pod opieke naukowa konkretnego profesora)&lt;/em&gt;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Ponizszy opis jest napisany w oparciu o tylko i wylacznie moje doswiadczenia&lt;/strong&gt; (za wyjatkiem miejsc, gdzie zaznaczam explicite, ze wiem cos ze slyszenia od innych studentow).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Ponizszy opis jest napisany w kontekscie aplikacji na programy rozpoczynajace sie jesienia 2017&lt;/strong&gt;, wypelnianymi zgodnie z wymaganiami i terminami, ktorych pilnowalam aplikujac w 2016. Nie sprawdzilam, czy i co zmienilo sie od tego czasu.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Wyodrebniam nastepujace glowne komponenty procesu aplikacji:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Wstepny wybor uczelni i zbudowanie planu czasowego procesu aplikacji&lt;/li&gt;
&lt;li&gt;Przygotowanie do egzaminow: GRE + TOEFL&lt;/li&gt;
&lt;li&gt;Przygotowanie Personal Statement&lt;/li&gt;
&lt;li&gt;Zorganizowanie listow rekomendacyjnych&lt;/li&gt;
&lt;li&gt;Networking&lt;/li&gt;
&lt;li&gt;Aplikowanie: wykonanie i nadzorowanie post-procesu&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;1-wstepny-wybor-uczelni-i-zbudowanie-planu-czasowego-timeline-aplikacji&#34;&gt;1. Wstepny wybor uczelni i zbudowanie planu czasowego timeline aplikacji&lt;/h1&gt;
&lt;p&gt;Generalne ramy czasowe.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Uczelnie roznia sie datami ostatecznego terminu aplikacji, jednoczesnie daty te  zjezdzaja sie na przestrzeni ok. 1,5 miesiaca: od 1. grudnia roku N do polowy stycznia roku (N+1) na programy rozpoczynajace sie jesienia roku (N+1). 60% terminow to wlasnie 1. grudnia. 📣&lt;/li&gt;
&lt;li&gt;Stad &lt;strong&gt;prace nad aplikacjami zaczynamy &lt;em&gt;najpozniej&lt;/em&gt; 12-14 miesiecy przed&lt;/strong&gt; faktycznym rozpoczeciem studiow.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Wstepny wybor uczelni.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Wybieralam uczelnie na podstawie: a) rankingow programu Biostatystyka (podzbior &amp;ldquo;Biostatystyka&amp;rdquo;  z &lt;a href=&#34;https://www.usnews.com/best-graduate-schools/top-science-schools/statistics-rankings&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;usnews.com&lt;/a&gt;; w 2016 korzystalam glownie z &lt;a href=&#34;http://www.amstat.org/asa/files/pdfs/OGRP-USNews_BioStatisticsRankings.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;tego&lt;/a&gt; dokumentu); b) rozmow z profesorami i studentami w USA w czasie pracy jako Research Assistant na Indiana University w pierwszej polowie 2016; c) przegladania grup naukowych i zainteresowan profesorow na stronach uczelni; d) lokalizacji; tu: zarowno ogolny obszar USA jak i konkretne miasto (przyklad: nie chcialam studiowac w NYC).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Bylo mi ciezko ocenic a priori, w jaki sposob moja aplikacja bedzie ewaluowana przez komitety rekrutacyjne na uczelniach w USA.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Aplikacja studenta z Polski na Biostatystyke u USA to wciaz egzotyczna sprawa. Komisje rekrutacyjne sa najczesniej niezaznajomione z polskim systemem szkolnictwa wyzszego (gdzie sa np. z chinskimi i hinduskimi), stad ocena &amp;ldquo;5.0&amp;rdquo; z Analizy matematycznej I na Politechnice Wroclawskiej  nie mowi im bardzo duzo na temat tego, co w zwiazku z tym powinnam umiec. (Przyklad: w czasie etapu rekrutacji &amp;ldquo;on site&amp;rdquo; na jednej z uczelni Ivy League, dziekan departamentu Biostatystyki, z ktorym mialam 20-minutowe spotkanie, poprosil mnie o chocby ogolny opis tego &amp;ldquo;jak wygladaly moje studia w Polsce&amp;rdquo; 👽).  &lt;br/&gt;&lt;br/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Aplikowalam do 16 uczelni&lt;/strong&gt; (relatywnie wielu), &lt;strong&gt;wg nastepujacego schematu&lt;/strong&gt;:  1/3 uczelni na liscie to &amp;ldquo;dream schools&amp;rdquo; - uczelnie z czolowki rankingow programu Biostatystyka; 1/3 to uczelnie &amp;ldquo;safe&amp;rdquo; - spodziewam sie, ze mam spore szanse sie tam dostac; 1/3 to uczelnie &amp;ldquo;po srodku&amp;rdquo;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Dostalam sie / dostalam zaproszenie do 2. etapu rekrutacji od nieco &lt;strong&gt;ponad 50% z uczelni&lt;/strong&gt;, do ktorych aplikowalam (2. etap to zaproszenie na rozmowe &amp;ldquo;on site&amp;rdquo; lub jego odpowiednik dla studentow przebywajacych poza USA: rozmowa telefoniczna z czlonkami komitetu rekrutacyjnego; w kilku przypadkach nie kontynuowalam procesu do 2. etapu wiedzac, ze dostalam sie juz do miejsca wyzej na liscie moich preferencji).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Co interesujace, uczelnie, ktore byly mna zainteresowanie, to niemal dokladnie: polowa uczelni z grupy &amp;ldquo;dream schools&amp;rdquo; + polowa z &amp;ldquo;safe&amp;rdquo; + polowa z &amp;ldquo;po srodku&amp;rdquo; 👏👍&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Plan czasowy procesu aplikacji powinien zawierac:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Daty &lt;strong&gt;ostatecznego terminu aplikacji&lt;/strong&gt; (&amp;ldquo;deadline&amp;rdquo;) do wybranych uczelni, tzn. daty, po ktorych zamykaja sie systemy online umozliwiajace wykonanie aplikacji.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Liste &lt;strong&gt;wymaganych egzaminow&lt;/strong&gt; i najdalszy termin ich realizacji (np. w Polsce), ktory zapewnia bezpieczne okno czasowe na przeslanie wynikow do uczelni. Najczesciej beda to dwa egzaminy: TOEFL i GRE General.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Wyniki TOEFL i GRE General wysylane sa automatycznie do uczelni, ktorych liste zadeklarujemy przy zapisie na egzamin lub ktore dodamy kiedykolwiek pozniej przy uzyciu systemow online do zarzadzania wynikami. Zwyczajnie trzeba przypilnowac, czy dni robocze, ktore organizator daje sobie na a) sprawdzenie egzaminu + b) wyslanie wynikow do uczelni, dodaja nam sie z ostatecznymi terminami aplikacji. &lt;br/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Okno czasowe na &lt;strong&gt;tlumaczenie przysiegle&lt;/strong&gt; oraz &lt;strong&gt;przeewaluowanie i/lub wyslanie poczta transkryptow ocen&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Przyklady tego, co moze nas &amp;ldquo;zaskoczyc&amp;rdquo;. a) ~5 uczelni wymagalo wyslania tlumaczonych (tl. przysiegly) transkryptow ocen fizycznie na adres uczelni w USA (w kilku przypadkach: w terminie wczesniejszym, niz generalny ostateczny termin aplikacji podany na stronie uczelni). b) ~7 uczelni realizowalo aplikacje przez system &lt;a href=&#34;https://sophas.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;SOPHAS&lt;/a&gt; - jednolity system aplikacji na wydzialy Zdrowia Publicznego (&amp;ldquo;School of Public Health&amp;rdquo;) w USA, z ktorych korzysta czesc uczelni.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;SOPHAS jest dla aplikanta wygodny o tyle, ze duza czesc dokumentow ladujemy do systemu tylko raz, niezaleznie od liczby uczelni na ktore aplikujemy via SOPHAS. Z drugiej strony, SOPHAS akceptuje zagraniczne transkrypty ocen tylko i wylacznie po przeewaluowaniu ich na system amerykanski przez World Education Services (&lt;a href=&#34;https://www.wes.org/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;WES&lt;/a&gt;). Musimy wiec wyslac do WES komplet swoich przetlumaczonych (tl. przysiegly 👼) transkryptow, a WES je ewaluuje i wysyla wyniki do SOPHAS. (To byl dla mnie najbardziej stresujacy element tego etapu rekrutacji: WES nie zrealizowal ewaluacji moich ocen w oknie czasowym, ktore deklaruje od momentu zmiany statusu mojej sprawy na &amp;ldquo;otrzymalismy twoje transkrypty&amp;rdquo;; pomimo moich prosb, &amp;ldquo;grozb&amp;rdquo; i placzu, dokumenty zostaly wyslane dopiero na kilka dobrych dni po deklarowanym terminie 🐢, szczesliwie dokladnie &amp;ldquo;na styk&amp;rdquo; z ostatecznymi terminami aplikacji do kilku uczelni. 🎣)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Wybor uczelni i organizowanie planu czasowy procesu aplikacji sa ze soba powiazane: specyficzne wymagania i terminy poszczegolnych uczelni wymuszaja plan czasowy, natomiast wymagania czasowe modyfikowaly moje wybory uczelni (przyklad: widzialam, ze nie dam rady czasowo przygotowac sie do egzaminu GRE Mathematics; z drugiej strony, jako ze duza czesc moich dokumentow typowo potrzebnych do aplikacji byla juz w SOPHAS, zaaplikowalam do kilku dodatkowych uczelni via SOPHAS).&lt;/p&gt;
&lt;h1 id=&#34;2-przygotowanie-do-egzaminow-gre-general--toefl&#34;&gt;2. Przygotowanie do egzaminow: GRE General + TOEFL&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;GRE General jest niezbedny&lt;/strong&gt; do aplikowania na studia &amp;ldquo;graduate&amp;rdquo; (Masters / PhD) w USA. W nielicznych przypadkach &lt;em&gt;wymagany&lt;/em&gt; jest rowniez GRE specjalistyczny (Stanford: GRE Mathematics Subject Test). &lt;strong&gt;Egzamin jezykowy&lt;/strong&gt; jest wymagany dla absolwentow uczelni, na ktorych jezyk angielski nie jest podstawowym jezykiem wykladowym. W USA wymaganym standardowo egzaminem jezykowym jest TOEFL (w niektorych szkolach zamienny z IELTS, w niektorych nie).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Zdawalam GRE General i TOEFL. Oba egzaminy zdawalam w Polsce (najpierw GRE General w Krakowie, tydzien pozniej: TOEFL w Poznaniu).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Przejscie przez przewodnik po typach zadan&lt;/strong&gt; na obu egzaminach &lt;strong&gt;to absolutne minimum&lt;/strong&gt; w przygotowaniach.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Sugeruje przeznaczyc 90-95% czasu przygotowan do obu egzaminow na GRE General.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;GRE General sklada sie z ~5 komponentow testujacych dwa obszary: j. angielski i matematyke. Czesc matematyczna jest prosta (poziom matury podstawowej z matematyki), jednoczesnie nalezy zalozyc duzo czasu na przygotowanie sie pod bardzo konkretna forme tego egzaminu. Jest &lt;em&gt;bardzo&lt;/em&gt; malo miejsca na jakiekolwiek pomylki, w szczegolnosci: bledy przy pierwszych zadaniach (poziom trudnosci zadan rosnie z biegiem czasu egzaminu), stad trzeba wyklepac do stanu &amp;ldquo;rozwiazuje z automatu&amp;rdquo; problemy pojawiajace sie na czesci matematycznej. Wlasnosci katow w trojkacie wpisanym i opisanym na okregu 💘, zadania &amp;ldquo;z dwoma pociagami&amp;rdquo;, te sprawy.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;GRE General cz. jezykowa sklada sie zadan testujacych umiejetnosc krytycznego myslenia i argumentowania w formie pisemnej oraz z zadan testujacych slownictwo. Czesc testujaca slownictwo jest moim zdaniem zwyczajnie nierobialna i ze swojej perspektywy oceniam, ze nie ma duzej roznicy, czy przeznaczy sie na nauke slowek 10h czy 300h przed egzaminem, tu trzeba isc do kosciola i pomodlic sie o celnosc strzalow w odpowiedziach. Na serio: przygotowanie do GRE General cz. jezykowa jest  czasochlonne. 😐 Pomocne: w internecie sa dostepne roznej dlugosci listy &amp;ldquo;slownictwa na GRE&amp;rdquo; oraz dedykowane aplikacje do nauki &amp;ldquo;slowek na GRE&amp;rdquo;.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Po napisaniu GRE General cz. jezykowa, poziom TOEFL wydaje sie latwy, a sam egzamin - przyjemny 😌.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Sugeruje nie zdawac obu egzaminow w jeden weekend; w szczegolnosci GRE jest bardzo wyczerpujacy (mysle, ze to byl najbardziej wyczerpujacy egzamin w moim zyciu; po wyjsciu z sali przez kilka minut zbieralam mysli o tym co to sie robilo zeby zamowic Ubera) 🚀.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;3-przygotowanie-personal-statement&#34;&gt;3. Przygotowanie Personal Statement&lt;/h1&gt;
&lt;p&gt;&lt;span style=&#34;color:red&#34;&gt;Nota bene: transkrypty ocen i wyniki z egzaminow sluza glownie do wyznaczenia punktu odciecia, ponizej ktorego &amp;ldquo;uczelnia generalnie nie rozmawia z potencjalnym studentem&amp;rdquo;.&lt;/span&gt; Latwo wyobrazic sobie, ze punkt odciecia przekracza wielu bardzo dobrych studentow (podpowiadam przypomniec sobie populacje np. Chin i Indii). &lt;span style=&#34;color:red&#34;&gt;To, czym &amp;ldquo;kupujemy&amp;rdquo; sobie przychylnosc komisji rekrutacyjnej i zdobywamy oferte programu, to &lt;strong&gt;kombinacja zawartosci CV, Personal Statement, listow rekomendacyjnych i naszego networkingu&lt;/strong&gt;.&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Personal Statement (zamiennie: Statement of Purpose) to wazny dokument, ktorego szablon - wersje beta tworzylam przez kilka dni z rzedu.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Szablon mojego Personal Statement mial 2 pelne strony A4 i skladal sie z  nastepujacych paragrafow:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;chwytliwe otwarcie, w ktorym wzbudzam zainteresowanie swoja motywacja i indywidualna historia,&lt;/li&gt;
&lt;li&gt;notka biograficzna, skupiajaca sie na doswiadczeniu w badaniach naukowych (poparte konkretnymi przykladami),&lt;/li&gt;
&lt;li&gt;notka biograficzna, skupiajaca sie na cechach osobowosciowych, ktore predestynuja mnie do bycia odnoszacym sukcesy studentem (poparte konkretnymi przykladami),&lt;/li&gt;
&lt;li&gt;motywacja do studiowania kierunku Biostatystyka,&lt;/li&gt;
&lt;li&gt;&amp;ldquo;big-picture&amp;rdquo; tego, co chce osiagnac dzieki studiom PhD - na przestrzeni studiow i calego zycia,&lt;/li&gt;
&lt;li&gt;motywacja do studiowania kierunku Biostatystyka na tej konkretnej uczelni,&lt;/li&gt;
&lt;li&gt;zgrabne zakonczenie.&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Sugestie:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;W Personal Statement nie ma miejsca na &amp;ldquo;chcialem byc marynarzem chcialem miec tatuaze podrozowac&amp;rdquo;. Wszystko, co tam wrzucamy tytulem osiagniec i aktywnosci, powinno byc poparte/umotywowane konkretnymi wydarzeniami z zycia akademickiego/poza-akademickiego.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;W Personal Statement nie ma miejsca na bycie skromnym. Inni aplikanci nie beda skromni.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Warto zgooglowac, jakie elementy &lt;em&gt;powinny zawsze&lt;/em&gt; znalezc sie w tym dokumencie - czesto uczelnie zamieszczaja te infromacje na swoich podstronach dot. rekrutacji. Jednoczesnie sugeruje NIE szukac i NIE &amp;ldquo;inspirowac sie&amp;rdquo; gotowymi dokumentami instniejacymi w sieci; im bardziej &amp;ldquo;twoj&amp;rdquo; jest Personal Statement, tym lepiej.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Sugeruje szablon &lt;strong&gt;dac do sprawdzenia 3-5 znajomym&lt;/strong&gt; 🙏 pod katem tresci; wyszlifowana tresciowo wersje sugeruje dac do sprawdzenia pod wzgledem jezykowym dobremu tlumaczowi. Do sprawdzenia (i do tlumaczenia transkryptow) polecam Panie z &lt;a href=&#34;http://www.btcentrum.pl/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BT Centrum we Wroclawiu&lt;/a&gt; - absolutnie profesjonale i elastyczne, de facto &amp;ldquo;uratowaly&amp;rdquo; moje aplikacje (a mogly wysmiac mnie w progu za termin, w ktorym potrzebowalam miec gotowe tlumaczenia: &amp;ldquo;na jutro&amp;rdquo; 🐩).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;4-listy-rekomendacyjne&#34;&gt;4. Listy rekomendacyjne&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;Potrzebowalam &lt;strong&gt;3 listy rekomendacyjne&lt;/strong&gt;. Szczegolny przypadek: SOPHAS (wspomniany wyzej) &lt;em&gt;bardzo mocno&lt;/em&gt; sugerowal zadbanie o co najmniej jeden list pochodzacy od osoby spoza akademickiego srodowiska; w tym przypadku zalaczylam 4 listy rekomendacyjne.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Uwagi:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;W dobrym guscie jest zwrocenie sie z prosba o &lt;em&gt;rozwazenie mozliwosci wystosowania takiego listu&lt;/em&gt; na min. 1 miesiac przed data, na kiedy list ma byc gotowy. Warto w takim pierwszym mejlu zaznaczyc wyraznie, na kiedy list bedziemy potrzebowac.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Listy rekomendacyjne pisane przez profesorow w USA sa zawsze pelne &amp;ldquo;ochow&amp;rdquo; i &amp;ldquo;achow&amp;rdquo;. Choc ciezko mi wyobrazic sobie poproszenie explicite o &amp;ldquo;pelen pochwal list&amp;rdquo;, warto sprobowac zaznaczyc, ze np. ze wzgledu na wysoka konkurencyjnosc uczelni, potrzebujemy &amp;ldquo;silnego&amp;rdquo; listu rekomendacyjnego. (Z tego co rozumiem, wylistowanie aktywnosci aplikanta i jednozdaniowe wyrazenie przekonania, ze to dobry kandydat, to - generalnie rzecz biarac - tresc na slaby list.)&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;5-networking&#34;&gt;5. Networking&lt;/h1&gt;
&lt;p&gt;Ciezko mi podkreslic wystarczajaco mocno, jak &lt;strong&gt;istotnym elementem aplikacji jest networking&lt;/strong&gt;, tzn. nawiazanie kontaktu z profesorami na wybranych przez nas uczelniach.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Faktycznie, &lt;em&gt;dostalam&lt;/em&gt; sie na niektore uczelnie (ze swoich kategorii &amp;ldquo;safe&amp;rdquo; i &amp;ldquo;po srodku&amp;rdquo;), z ktorymi nie nawiazalam wczesniej zadnego bezposredniego kontaktu.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Twierdze, ze bardzo trudne byloby dostanie sie do &amp;ldquo;dream school&amp;rdquo; bez nawiazania wczesniej bezposredniego kontaktu. Tu, aplikacja bardzo potrzebuje &amp;ldquo;wsparcia od srodka&amp;rdquo; departamentu, do ktorego chcemy sie dostac.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Przyklady tego, &lt;strong&gt;jak mozna budowac potrzebny networking&lt;/strong&gt;:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Wyjechanie do USA na kilka miesiecy do tymczasowej pracy / na staz na wybrana uczelnie.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Pierwszym krokiem do zaproszenia na taki wyjazd moze byc krotki mejl do wybranego profesora, mowiacy: &amp;ldquo;Czesc, jestem bardzo zainteresowany twoja praca o X, sam od dawna czytam i pracuje nad rzeczami zwiazanymi z Y, czy znalazlbys 10 minut na rozmowe na Skajpie? Dostosuje sie do terminu, ktory zaproponujesz.&amp;rdquo;  &lt;br/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Wyjechanie do USA na konferencje / szkole wakacyjna / warsztaty, gdzie beda profesorowie z interesujacej nas uczelni.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Nawiazanie kontaktu telefonicznego / mejlowego z profesorami z interesujacej nas uczelni. (Por. wskazowka w podpunkcie wyzej).&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;h1 id=&#34;6-aplikowanie&#34;&gt;6. Aplikowanie&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Wypelnianie aplikacji jest czasochlonne&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Nalezy wziac margines na specjalne przypadki nawet w ramach wykonywania aplikacji w formularzu online. Przyklady: a) z ostatniej strony formularza aplikacyjnego: nie przechodzi platnosc polska karta platnicza; b) z ktorejkolwiek / wielu stron formularza aplikacyjnego: nie jest jasne, co oznacza to pole - musze wyslac mejla z pytaniem do dzialu rekrutacji tej uczelni 😎.  &lt;br/&gt;  &lt;br/&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;strong&gt;Aplikacje sa drogie&lt;/strong&gt;.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Niektore koszty sa jednorazowe i prawie niezalezne od liczby aplikacji (wyjazd na egzamin TOEFL / GRE). Aplikacja do kazdej kolejnej szkoly generuje  koszt (bezzwrotna oplata aplikacyjna: 30-130 USD, przeslanie wyniku GRE: 20 USD, przeslanie wyniku TOEFL: 20 USD).&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Mysle, ze calkowity koszt moich aplikacji (w tym: tlumacz przysiegly, dojazdy i noclegi na egzaminy w innych miastach Polski, wyslanie paczek UPS z dokumentami do kilku szkol w USA) to lacznie 7.000-10.000 PLN ☂️.&lt;/p&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
</description>
    </item>
    
  </channel>
</rss>
